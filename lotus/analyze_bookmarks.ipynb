{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bookmark Data Analysis Notebook\n",
    "\n",
    "This notebook demonstrates how to analyze the bookmark data collected by the deltaload tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "We'll load the bookmarks data from the JSONL file generated by deltaload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the JSONL data\n",
    "bookmarks_path = '/Users/imac/Desktop/ETL/deltaload/data-bookmark.jsonl'\n",
    "\n",
    "# Function to parse JSONL file\n",
    "def load_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error parsing line: {line[:100]}...\")\n",
    "    return data\n",
    "\n",
    "# Load the data\n",
    "bookmarks = load_jsonl(bookmarks_path)\n",
    "print(f\"Loaded {len(bookmarks)} bookmarks from {bookmarks_path}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(bookmarks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Now let's do some preprocessing on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert dates to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Parse metadata\n",
    "def parse_metadata(metadata_str):\n",
    "    try:\n",
    "        return json.loads(metadata_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return {}\n",
    "\n",
    "df['parsed_metadata'] = df['metadata'].apply(parse_metadata)\n",
    "\n",
    "# Extract domain from URL\n",
    "def extract_domain(url):\n",
    "    if not isinstance(url, str):\n",
    "        return 'unknown'\n",
    "    \n",
    "    match = re.search(r'https?://([\\w.-]+)/', url)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        match = re.search(r'https?://([\\w.-]+)', url)\n",
    "        return match.group(1) if match else 'unknown'\n",
    "\n",
    "df['domain'] = df['url'].apply(extract_domain)\n",
    "\n",
    "# Display data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "### Source Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Count by source\n",
    "source_counts = df['source'].value_counts()\n",
    "print(\"Bookmark Sources:\")\n",
    "print(source_counts)\n",
    "\n",
    "# Plot source distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=source_counts.index, y=source_counts.values)\n",
    "plt.title('Distribution of Bookmark Sources')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add count labels on top of bars\n",
    "for i, count in enumerate(source_counts.values):\n",
    "    ax.text(i, count + 10, f'{count:,}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group by date and source\n",
    "df['date'] = df['created_at'].dt.date\n",
    "date_source_counts = df.groupby(['date', 'source']).size().unstack().fillna(0)\n",
    "\n",
    "# Plot timeline\n",
    "plt.figure(figsize=(16, 8))\n",
    "date_source_counts.plot(kind='line', ax=plt.gca())\n",
    "plt.title('Bookmarks by Source Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Source')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Monthly aggregation\n",
    "df['year_month'] = df['created_at'].dt.to_period('M')\n",
    "monthly_counts = df.groupby(['year_month', 'source']).size().unstack().fillna(0)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "monthly_counts.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "plt.title('Monthly Bookmarks by Source')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(title='Source')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Top domains\n",
    "top_domains = df['domain'].value_counts().head(20)\n",
    "print(\"Top 20 Domains:\")\n",
    "print(top_domains)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=top_domains.values, y=top_domains.index)\n",
    "plt.title('Top 20 Domains in Bookmarks')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Domain')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(top_domains.values):\n",
    "    ax.text(count + 1, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Text length analysis\n",
    "df['content_length'] = df['content'].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='content_length', hue='source', bins=50, log_scale=(False, True))\n",
    "plt.title('Distribution of Content Length by Source')\n",
    "plt.xlabel('Content Length (characters)')\n",
    "plt.ylabel('Count (log scale)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average content length by source\n",
    "avg_length_by_source = df.groupby('source')['content_length'].mean().sort_values(ascending=False)\n",
    "print(\"Average Content Length by Source:\")\n",
    "print(avg_length_by_source)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "avg_length_by_source.plot(kind='bar')\n",
    "plt.title('Average Content Length by Source')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Average Length (characters)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitHub-specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter to GitHub data\n",
    "github_df = df[df['source'] == 'github'].copy()\n",
    "\n",
    "# Extract repository language and stars if available\n",
    "def extract_language(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        return metadata.get('language', 'Unknown')\n",
    "    return 'Unknown'\n",
    "\n",
    "def extract_stars(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        return metadata.get('stars', 0)\n",
    "    return 0\n",
    "\n",
    "github_df['language'] = github_df['parsed_metadata'].apply(extract_language)\n",
    "github_df['stars'] = github_df['parsed_metadata'].apply(extract_stars)\n",
    "\n",
    "# Top languages\n",
    "top_languages = github_df['language'].value_counts().head(15)\n",
    "print(\"Top 15 Languages in GitHub Repositories:\")\n",
    "print(top_languages)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=top_languages.values, y=top_languages.index)\n",
    "plt.title('Top 15 Languages in GitHub Repositories')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Language')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(top_languages.values):\n",
    "    ax.text(count + 1, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stars distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=github_df, x='stars', bins=50, log_scale=(False, True))\n",
    "plt.title('Distribution of Stars in GitHub Repositories')\n",
    "plt.xlabel('Stars Count (clipped at 95th percentile)')\n",
    "plt.ylabel('Repository Count (log scale)')\n",
    "plt.xlim(0, github_df['stars'].quantile(0.95))  # Clip at 95th percentile for better visibility\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter-specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter to Twitter data\n",
    "twitter_df = df[df['source'].isin(['twitter', 'twitter_like'])].copy()\n",
    "\n",
    "# Extract metrics like retweet_count, favorite_count if available\n",
    "def extract_retweet_count(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        return metadata.get('retweet_count', 0)\n",
    "    return 0\n",
    "\n",
    "def extract_favorite_count(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        return metadata.get('favorite_count', 0)\n",
    "    return 0\n",
    "\n",
    "twitter_df['retweet_count'] = twitter_df['parsed_metadata'].apply(extract_retweet_count)\n",
    "twitter_df['favorite_count'] = twitter_df['parsed_metadata'].apply(extract_favorite_count)\n",
    "\n",
    "# Engagement metrics\n",
    "print(\"Twitter Engagement Metrics:\")\n",
    "print(twitter_df[['retweet_count', 'favorite_count']].describe())\n",
    "\n",
    "# Plot engagement distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.histplot(data=twitter_df, x='retweet_count', bins=50, log_scale=(False, True))\n",
    "plt.title('Distribution of Retweet Counts')\n",
    "plt.xlabel('Retweet Count (clipped at 95th percentile)')\n",
    "plt.ylabel('Tweet Count (log scale)')\n",
    "plt.xlim(0, twitter_df['retweet_count'].quantile(0.95))  # Clip at 95th percentile for better visibility\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.histplot(data=twitter_df, x='favorite_count', bins=50, log_scale=(False, True))\n",
    "plt.title('Distribution of Favorite Counts')\n",
    "plt.xlabel('Favorite Count (clipped at 95th percentile)')\n",
    "plt.ylabel('Tweet Count (log scale)')\n",
    "plt.xlim(0, twitter_df['favorite_count'].quantile(0.95))  # Clip at 95th percentile for better visibility\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tweets over time\n",
    "twitter_df['hour'] = twitter_df['created_at'].dt.hour\n",
    "twitter_df['day_of_week'] = twitter_df['created_at'].dt.day_name()\n",
    "\n",
    "# Create a day of week order\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# Plot heatmap of tweet activity by hour and day\n",
    "plt.figure(figsize=(12, 8))\n",
    "tweet_heatmap = pd.crosstab(twitter_df['day_of_week'], twitter_df['hour'])\n",
    "# Reorder the index\n",
    "tweet_heatmap = tweet_heatmap.reindex(day_order)\n",
    "\n",
    "sns.heatmap(tweet_heatmap, cmap='viridis', annot=True, fmt='d')\n",
    "plt.title('Tweet Activity by Hour and Day of Week')\n",
    "plt.xlabel('Hour of Day (UTC)')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raindrop.io-specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filter to Raindrop data\n",
    "raindrop_df = df[df['source'] == 'raindrop'].copy()\n",
    "\n",
    "# Extract folder and tags\n",
    "def extract_folder(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        return metadata.get('folder', 'Unsorted')\n",
    "    return 'Unsorted'\n",
    "\n",
    "def extract_tags(metadata):\n",
    "    if isinstance(metadata, dict):\n",
    "        tags = metadata.get('tags', [])\n",
    "        if isinstance(tags, list):\n",
    "            return tags\n",
    "    return []\n",
    "\n",
    "raindrop_df['folder'] = raindrop_df['parsed_metadata'].apply(extract_folder)\n",
    "raindrop_df['tags'] = raindrop_df['parsed_metadata'].apply(extract_tags)\n",
    "\n",
    "# Top folders\n",
    "folder_counts = raindrop_df['folder'].value_counts().head(15)\n",
    "print(\"Top 15 Folders:\")\n",
    "print(folder_counts)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=folder_counts.values, y=folder_counts.index)\n",
    "plt.title('Top 15 Folders in Raindrop.io Bookmarks')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Folder')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(folder_counts.values):\n",
    "    ax.text(count + 1, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tags analysis (flatten the list of tags)\n",
    "all_tags = [tag for tags_list in raindrop_df['tags'] for tag in tags_list if tags_list]\n",
    "tag_counts = Counter(all_tags)\n",
    "top_tags = pd.Series(tag_counts).sort_values(ascending=False).head(20)\n",
    "\n",
    "print(\"\\nTop 20 Tags:\")\n",
    "print(top_tags)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = sns.barplot(x=top_tags.values, y=top_tags.index)\n",
    "plt.title('Top 20 Tags in Raindrop.io Bookmarks')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Tag')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(top_tags.values):\n",
    "    ax.text(count + 1, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Topic Modeling\n",
    "\n",
    "Let's try to identify common topics or themes in the bookmark content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract text content\n",
    "df['clean_content'] = df['content'].apply(lambda x: str(x) if pd.notna(x) else '')\n",
    "\n",
    "# Filter out very short content\n",
    "df_for_topics = df[df['clean_content'].str.len() > 10].copy()\n",
    "\n",
    "# Simple keyword extraction and frequency analysis\n",
    "def extract_keywords(text):\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "    \n",
    "    # Split into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out common stop words (a very basic list)\n",
    "    stop_words = set(['the', 'and', 'is', 'in', 'to', 'of', 'a', 'for', 'on', 'with', 'as', 'by', 'an', 'that', 'this', 'are', 'from'])\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Apply to all content\n",
    "all_keywords = []\n",
    "for content in df_for_topics['clean_content']:\n",
    "    all_keywords.extend(extract_keywords(content))\n",
    "\n",
    "# Count keywords\n",
    "keyword_counts = Counter(all_keywords)\n",
    "top_keywords = pd.Series(keyword_counts).sort_values(ascending=False).head(30)\n",
    "\n",
    "print(\"Top 30 Keywords:\")\n",
    "print(top_keywords)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ax = sns.barplot(x=top_keywords.values, y=top_keywords.index)\n",
    "plt.title('Top 30 Keywords in Bookmark Content')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Keyword')\n",
    "\n",
    "# Add count labels\n",
    "for i, count in enumerate(top_keywords.values):\n",
    "    ax.text(count + 10, i, f'{count:,}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations Based on Analysis\n",
    "\n",
    "Let's generate some simple recommendations based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find most popular repositories by stars\n",
    "top_repos = github_df.sort_values('stars', ascending=False).head(10)[['content', 'stars', 'url']]\n",
    "print(\"Top 10 GitHub Repositories by Stars:\")\n",
    "print(top_repos)\n",
    "\n",
    "# Find most engaging tweets\n",
    "twitter_df['engagement'] = twitter_df['retweet_count'] + twitter_df['favorite_count']\n",
    "top_tweets = twitter_df.sort_values('engagement', ascending=False).head(10)[['content', 'retweet_count', 'favorite_count', 'url']]\n",
    "print(\"\\nTop 10 Tweets by Engagement:\")\n",
    "print(top_tweets)\n",
    "\n",
    "# Find most popular domains\n",
    "print(\"\\nTop 10 Domains:\")\n",
    "print(df['domain'].value_counts().head(10))\n",
    "\n",
    "# Find most used programming languages\n",
    "if 'language' in github_df.columns:\n",
    "    print(\"\\nTop Programming Languages:\")\n",
    "    print(github_df['language'].value_counts().head(10))\n",
    "\n",
    "# Find most active weeks\n",
    "df['week'] = df['created_at'].dt.isocalendar().week\n",
    "df['year'] = df['created_at'].dt.isocalendar().year\n",
    "df['year_week'] = df['year'].astype(str) + '-W' + df['week'].astype(str).str.zfill(2)\n",
    "weekly_counts = df.groupby('year_week').size().sort_values(ascending=False).head(5)\n",
    "print(\"\\nMost Active Weeks:\")\n",
    "print(weekly_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}