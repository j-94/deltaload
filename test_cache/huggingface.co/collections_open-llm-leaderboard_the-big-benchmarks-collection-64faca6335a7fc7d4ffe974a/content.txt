Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)

*   * * *
    
    [🏆 #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    
    Note 📐 The 🤗 Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. 🤗 Submit a model for automated evaluation on the 🤗 GPU cluster on the “Submit” page!
    
*   * * *
    
    [🥇 #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    
    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.
    
*   * * *
    
    [🏆🤖 #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)
    
    Note 🏆 This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a model’s multitask accuracy on 57 tasks.
    
*   * * *
    
    [🏆🏋️ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
    
    Note The 🤗 LLM-Perf Leaderboard 🏋️ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:
    
*   * * *
    
    [📈 #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    
    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.
    
*   * * *
    
    [🏆 #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)
    
    Note The 🤗 Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (⬇️) and RTF (⬇️) - lower the better. Models are ranked based on their Average WER, from lowest to highest
    
*   * * *
    
    [📊 #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)
    
    Note The MT-Bench Browser (see Chatbot arena)
    
*   * * *
    
    [⚡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)
    
*   * * *
    
    [🚀 #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)
    
*   * * *
    
    [🚀 #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)
    
*   * * *
    
    [📉 #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
    
*   * * *
    
    [🏆 #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)
    
*   * * *
    
    [🏅 #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)