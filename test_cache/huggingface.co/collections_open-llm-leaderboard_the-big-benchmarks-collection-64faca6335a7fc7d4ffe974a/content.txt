Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)

*   * * *
    
    [ğŸ† #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    
    Note ğŸ“ The ğŸ¤— Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. ğŸ¤— Submit a model for automated evaluation on the ğŸ¤— GPU cluster on the â€œSubmitâ€ page!
    
*   * * *
    
    [ğŸ¥‡ #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    
    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.
    
*   * * *
    
    [ğŸ†ğŸ¤– #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)
    
    Note ğŸ† This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a modelâ€™s multitask accuracy on 57 tasks.
    
*   * * *
    
    [ğŸ†ğŸ‹ï¸ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
    
    Note The ğŸ¤— LLM-Perf Leaderboard ğŸ‹ï¸ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:
    
*   * * *
    
    [ğŸ“ˆ #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    
    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.
    
*   * * *
    
    [ğŸ† #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)
    
    Note The ğŸ¤— Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (â¬‡ï¸) and RTF (â¬‡ï¸) - lower the better. Models are ranked based on their Average WER, from lowest to highest
    
*   * * *
    
    [ğŸ“Š #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)
    
    Note The MT-Bench Browser (see Chatbot arena)
    
*   * * *
    
    [âš¡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)
    
*   * * *
    
    [ğŸš€ #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)
    
*   * * *
    
    [ğŸš€ #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)
    
*   * * *
    
    [ğŸ“‰ #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
    
*   * * *
    
    [ğŸ† #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)
    
*   * * *
    
    [ğŸ… #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)