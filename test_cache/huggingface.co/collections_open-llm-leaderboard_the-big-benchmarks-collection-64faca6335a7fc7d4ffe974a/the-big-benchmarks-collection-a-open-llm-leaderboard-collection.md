---
title: The Big Benchmarks Collection - a open-llm-leaderboard Collection
description: Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)
url: https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a
timestamp: 2025-01-20T15:49:39.782Z
domain: huggingface.co
path: collections_open-llm-leaderboard_the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a
---

# The Big Benchmarks Collection - a open-llm-leaderboard Collection


Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)


## Content

Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)

*   * * *
    
    [🏆 #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    
    Note 📐 The 🤗 Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. 🤗 Submit a model for automated evaluation on the 🤗 GPU cluster on the “Submit” page!
    
*   * * *
    
    [🥇 #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    
    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.
    
*   * * *
    
    [🏆🤖 #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)
    
    Note 🏆 This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a model’s multitask accuracy on 57 tasks.
    
*   * * *
    
    [🏆🏋️ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
    
    Note The 🤗 LLM-Perf Leaderboard 🏋️ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:
    
*   * * *
    
    [📈 #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    
    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.
    
*   * * *
    
    [🏆 #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)
    
    Note The 🤗 Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (⬇️) and RTF (⬇️) - lower the better. Models are ranked based on their Average WER, from lowest to highest
    
*   * * *
    
    [📊 #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)
    
    Note The MT-Bench Browser (see Chatbot arena)
    
*   * * *
    
    [⚡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)
    
*   * * *
    
    [🚀 #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)
    
*   * * *
    
    [🚀 #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)
    
*   * * *
    
    [📉 #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
    
*   * * *
    
    [🏆 #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)
    
*   * * *
    
    [🏅 #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)

## Metadata

```json
{
  "title": "The Big Benchmarks Collection - a open-llm-leaderboard Collection",
  "description": "Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)",
  "url": "https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a",
  "content": "Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)\n\n*   * * *\n    \n    [🏆 #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\n    \n    Note 📐 The 🤗 Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. 🤗 Submit a model for automated evaluation on the 🤗 GPU cluster on the “Submit” page!\n    \n*   * * *\n    \n    [🥇 #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n    \n    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.\n    \n*   * * *\n    \n    [🏆🤖 #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)\n    \n    Note 🏆 This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a model’s multitask accuracy on 57 tasks.\n    \n*   * * *\n    \n    [🏆🏋️ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n    \n    Note The 🤗 LLM-Perf Leaderboard 🏋️ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:\n    \n*   * * *\n    \n    [📈 #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)\n    \n    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.\n    \n*   * * *\n    \n    [🏆 #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n    \n    Note The 🤗 Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (⬇️) and RTF (⬇️) - lower the better. Models are ranked based on their Average WER, from lowest to highest\n    \n*   * * *\n    \n    [📊 #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)\n    \n    Note The MT-Bench Browser (see Chatbot arena)\n    \n*   * * *\n    \n    [⚡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)\n    \n*   * * *\n    \n    [🚀 #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)\n    \n*   * * *\n    \n    [🚀 #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)\n    \n*   * * *\n    \n    [📉 #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)\n    \n*   * * *\n    \n    [🏆 #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)\n    \n*   * * *\n    \n    [🏅 #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)",
  "usage": {
    "tokens": 908
  }
}
```
