---
title: The Big Benchmarks Collection - a open-llm-leaderboard Collection
description: Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)
url: https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a
timestamp: 2025-01-20T15:49:39.782Z
domain: huggingface.co
path: collections_open-llm-leaderboard_the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a
---

# The Big Benchmarks Collection - a open-llm-leaderboard Collection


Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)


## Content

Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)

*   * * *
    
    [ğŸ† #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)
    
    Note ğŸ“ The ğŸ¤— Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. ğŸ¤— Submit a model for automated evaluation on the ğŸ¤— GPU cluster on the â€œSubmitâ€ page!
    
*   * * *
    
    [ğŸ¥‡ #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    
    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.
    
*   * * *
    
    [ğŸ†ğŸ¤– #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)
    
    Note ğŸ† This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a modelâ€™s multitask accuracy on 57 tasks.
    
*   * * *
    
    [ğŸ†ğŸ‹ï¸ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
    
    Note The ğŸ¤— LLM-Perf Leaderboard ğŸ‹ï¸ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:
    
*   * * *
    
    [ğŸ“ˆ #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
    
    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.
    
*   * * *
    
    [ğŸ† #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)
    
    Note The ğŸ¤— Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (â¬‡ï¸) and RTF (â¬‡ï¸) - lower the better. Models are ranked based on their Average WER, from lowest to highest
    
*   * * *
    
    [ğŸ“Š #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)
    
    Note The MT-Bench Browser (see Chatbot arena)
    
*   * * *
    
    [âš¡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)
    
*   * * *
    
    [ğŸš€ #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)
    
*   * * *
    
    [ğŸš€ #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)
    
*   * * *
    
    [ğŸ“‰ #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)
    
*   * * *
    
    [ğŸ† #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)
    
*   * * *
    
    [ğŸ… #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)

## Metadata

```json
{
  "title": "The Big Benchmarks Collection - a open-llm-leaderboard Collection",
  "description": "Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)",
  "url": "https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a",
  "content": "Gathering benchmark spaces on the hub (beyond the Open LLM Leaderboard)\n\n*   * * *\n    \n    [ğŸ† #### Open LLM Leaderboard Track, rank and evaluate open LLMs and chatbots](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\n    \n    Note ğŸ“ The ğŸ¤— Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots. ğŸ¤— Submit a model for automated evaluation on the ğŸ¤— GPU cluster on the â€œSubmitâ€ page!\n    \n*   * * *\n    \n    [ğŸ¥‡ #### MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n    \n    Note Massive Text Embedding Benchmark (MTEB) Leaderboard.\n    \n*   * * *\n    \n    [ğŸ†ğŸ¤– #### Chatbot Arena Leaderboard](https://huggingface.co/spaces/lmarena-ai/chatbot-arena-leaderboard)\n    \n    Note ğŸ† This leaderboard is based on the following three benchmarks: Chatbot Arena - a crowdsourced, randomized battle platform. We use 70K+ user votes to compute Elo ratings. MT-Bench - a set of challenging multi-turn questions. We use GPT-4 to grade the model responses. MMLU (5-shot) - a test to measure a modelâ€™s multitask accuracy on 57 tasks.\n    \n*   * * *\n    \n    [ğŸ†ğŸ‹ï¸ #### LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)\n    \n    Note The ğŸ¤— LLM-Perf Leaderboard ğŸ‹ï¸ aims to benchmark the performance (latency, throughput & memory) of Large Language Models (LLMs) with different hardwares, backends and optimizations using Optimum-Benchmark and Optimum flavors. Anyone from the community can request a model or a hardware/backend/optimization configuration for automated benchmarking:\n    \n*   * * *\n    \n    [ğŸ“ˆ #### Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)\n    \n    Note Compare performance of base multilingual code generation models on HumanEval benchmark and MultiPL-E. We also measure throughput and provide information about the models. We only compare open pre-trained multilingual code models, that people can start from as base models for their trainings.\n    \n*   * * *\n    \n    [ğŸ† #### Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)\n    \n    Note The ğŸ¤— Open ASR Leaderboard ranks and evaluates speech recognition models on the Hugging Face Hub. We report the Average WER (â¬‡ï¸) and RTF (â¬‡ï¸) - lower the better. Models are ranked based on their Average WER, from lowest to highest\n    \n*   * * *\n    \n    [ğŸ“Š #### MT Bench](https://huggingface.co/spaces/lmsys/mt-bench)\n    \n    Note The MT-Bench Browser (see Chatbot arena)\n    \n*   * * *\n    \n    [âš¡ #### Toolbench Leaderboard](https://huggingface.co/spaces/qiantong-xu/toolbench-leaderboard)\n    \n*   * * *\n    \n    [ğŸš€ #### OpenCompass LLM Leaderboard](https://huggingface.co/spaces/opencompass/opencompass-llm-leaderboard)\n    \n*   * * *\n    \n    [ğŸš€ #### MMBench Leaderboard](https://huggingface.co/spaces/opencompass/MMBench)\n    \n*   * * *\n    \n    [ğŸ“‰ #### Open Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard)\n    \n*   * * *\n    \n    [ğŸ† #### Subquadratic LLM Leaderboard](https://huggingface.co/spaces/devingulliver/subquadratic-llm-leaderboard)\n    \n*   * * *\n    \n    [ğŸ… #### Open Persian LLM Leaderboard Open Persian LLM Leaderboard](https://huggingface.co/spaces/PartAI/open-persian-llm-leaderboard)",
  "usage": {
    "tokens": 908
  }
}
```
