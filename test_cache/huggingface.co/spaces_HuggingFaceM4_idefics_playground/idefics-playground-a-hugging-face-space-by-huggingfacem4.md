---
title: IDEFICS Playground - a Hugging Face Space by HuggingFaceM4
description: Discover amazing ML apps made by the community
url: https://huggingface.co/spaces/HuggingFaceM4/idefics_playground
timestamp: 2025-01-20T15:44:51.558Z
domain: huggingface.co
path: spaces_HuggingFaceM4_idefics_playground
---

# IDEFICS Playground - a Hugging Face Space by HuggingFaceM4


Discover amazing ML apps made by the community


## Content

Fetching metadata from the HF Docker repository...

runtime error
-------------

Exit code: 1. Reason: sr/local/lib/python3.10/site-packages/gradio\_client/utils.py", line 517, in synchronize\_async return fsspec.asyn.sync(fsspec.asyn.get\_loop(), func, \*args, \*\*kwargs) # type: ignore File "/usr/local/lib/python3.10/site-packages/fsspec/asyn.py", line 103, in sync raise return\_result File "/usr/local/lib/python3.10/site-packages/fsspec/asyn.py", line 56, in \_runner result\[0\] = await coro File "/usr/local/lib/python3.10/site-packages/gradio/helpers.py", line 277, in create await self.cache() File "/usr/local/lib/python3.10/site-packages/gradio/helpers.py", line 333, in cache prediction = await Context.root\_block.process\_api( File "/usr/local/lib/python3.10/site-packages/gradio/blocks.py", line 1432, in process\_api result = await self.call\_function( File "/usr/local/lib/python3.10/site-packages/gradio/blocks.py", line 1107, in call\_function prediction = await anyio.to\_thread.run\_sync( File "/usr/local/lib/python3.10/site-packages/anyio/to\_thread.py", line 56, in run\_sync return await get\_async\_backend().run\_sync\_in\_worker\_thread( File "/usr/local/lib/python3.10/site-packages/anyio/\_backends/\_asyncio.py", line 2441, in run\_sync\_in\_worker\_thread return await future File "/usr/local/lib/python3.10/site-packages/anyio/\_backends/\_asyncio.py", line 943, in run result = context.run(func, \*args) File "/usr/local/lib/python3.10/site-packages/gradio/utils.py", line 707, in wrapper response = f(\*args, \*\*kwargs) File "/home/user/app/app\_dialogue.py", line 632, in process\_example generated\_text = client.generate(prompt=query, \*\*generation\_args).generated\_text File "/usr/local/lib/python3.10/site-packages/text\_generation/client.py", line 284, in generate raise parse\_error(resp.status\_code, payload) text\_generation.errors.ShardNotReadyError: The model HuggingFaceM4/idefics-80b-instruct is too large to be loaded automatically (159GB \> 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Container logs:

## Metadata

```json
{
  "title": "IDEFICS Playground - a Hugging Face Space by HuggingFaceM4",
  "description": "Discover amazing ML apps made by the community",
  "url": "https://huggingface.co/spaces/HuggingFaceM4/idefics_playground",
  "content": "Fetching metadata from the HF Docker repository...\n\nruntime error\n-------------\n\nExit code: 1. Reason: sr/local/lib/python3.10/site-packages/gradio\\_client/utils.py\", line 517, in synchronize\\_async return fsspec.asyn.sync(fsspec.asyn.get\\_loop(), func, \\*args, \\*\\*kwargs) # type: ignore File \"/usr/local/lib/python3.10/site-packages/fsspec/asyn.py\", line 103, in sync raise return\\_result File \"/usr/local/lib/python3.10/site-packages/fsspec/asyn.py\", line 56, in \\_runner result\\[0\\] = await coro File \"/usr/local/lib/python3.10/site-packages/gradio/helpers.py\", line 277, in create await self.cache() File \"/usr/local/lib/python3.10/site-packages/gradio/helpers.py\", line 333, in cache prediction = await Context.root\\_block.process\\_api( File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1432, in process\\_api result = await self.call\\_function( File \"/usr/local/lib/python3.10/site-packages/gradio/blocks.py\", line 1107, in call\\_function prediction = await anyio.to\\_thread.run\\_sync( File \"/usr/local/lib/python3.10/site-packages/anyio/to\\_thread.py\", line 56, in run\\_sync return await get\\_async\\_backend().run\\_sync\\_in\\_worker\\_thread( File \"/usr/local/lib/python3.10/site-packages/anyio/\\_backends/\\_asyncio.py\", line 2441, in run\\_sync\\_in\\_worker\\_thread return await future File \"/usr/local/lib/python3.10/site-packages/anyio/\\_backends/\\_asyncio.py\", line 943, in run result = context.run(func, \\*args) File \"/usr/local/lib/python3.10/site-packages/gradio/utils.py\", line 707, in wrapper response = f(\\*args, \\*\\*kwargs) File \"/home/user/app/app\\_dialogue.py\", line 632, in process\\_example generated\\_text = client.generate(prompt=query, \\*\\*generation\\_args).generated\\_text File \"/usr/local/lib/python3.10/site-packages/text\\_generation/client.py\", line 284, in generate raise parse\\_error(resp.status\\_code, payload) text\\_generation.errors.ShardNotReadyError: The model HuggingFaceM4/idefics-80b-instruct is too large to be loaded automatically (159GB \\> 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nContainer logs:",
  "usage": {
    "tokens": 611
  }
}
```
