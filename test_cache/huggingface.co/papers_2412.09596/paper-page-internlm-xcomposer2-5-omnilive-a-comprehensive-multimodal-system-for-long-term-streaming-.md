---
title: Paper page - InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for
  Long-term Streaming Video and Audio Interactions
description: Join the discussion on this paper page
url: https://huggingface.co/papers/2412.09596
timestamp: 2025-01-20T16:19:08.490Z
domain: huggingface.co
path: papers_2412.09596
---

# Paper page - InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for
  Long-term Streaming Video and Audio Interactions


Join the discussion on this paper page


## Content

Paper page - InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions
===============

 [![Image 40: Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)

*   [Models](https://huggingface.co/models)
*   [Datasets](https://huggingface.co/datasets)
*   [Spaces](https://huggingface.co/spaces)
*   [Posts](https://huggingface.co/posts)
*   [Docs](https://huggingface.co/docs)
*   [Enterprise](https://huggingface.co/enterprise)
*   [Pricing](https://huggingface.co/pricing)

*   * * *
    
*   [Log In](https://huggingface.co/login)
*   [Sign Up](https://huggingface.co/join)

[Papers](https://huggingface.co/papers)

arxiv:2412.09596

InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions
======================================================================================================================

Published on Dec 12, 2024

· Submitted by [![Image 41](https://huggingface.co/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg) myownskyW7](https://huggingface.co/myownskyW7) on Dec 13, 2024

[#2 Paper of the day](https://huggingface.co/papers?date=2024-12-13)

 [Upvote 93](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)

*   [![Image 42](https://huggingface.co/avatars/8f256f3249bbc24d1cceb6f464eb06b5.svg)](https://huggingface.co/LLMbeginer "LLMbeginer")
*   [![Image 43](https://cdn-avatars.huggingface.co/v1/production/uploads/6447d332ab5c7251886d6fd1/bm5nwIp5CA_HosO8wXFvI.jpeg)](https://huggingface.co/zkniu "zkniu")
*   [![Image 44](https://huggingface.co/avatars/e6188562254f75a09b4048b800860016.svg)](https://huggingface.co/BeichenZhang "BeichenZhang")
*   [![Image 45](https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg)](https://huggingface.co/Zery "Zery")
*   [![Image 46](https://huggingface.co/avatars/013b79d79df719fa51bfd4bda5041695.svg)](https://huggingface.co/Willow123 "Willow123")
*   [![Image 47](https://huggingface.co/avatars/2ae2710753ce34a04937384bc6dddf70.svg)](https://huggingface.co/Songweii "Songweii")
*   [![Image 48](https://huggingface.co/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg)](https://huggingface.co/MichaelBarryUK "MichaelBarryUK")
*   [![Image 49](https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg)](https://huggingface.co/rookiexiong "rookiexiong")
*   +85

Authors:

Pan Zhang ,

Xiaoyi Dong ,

 ![Image 50](https://huggingface.co/avatars/b320c77dfad039d9f9c54127f610d44f.svg) [Yuhang Cao](https://huggingface.co/yhcao) ,

 ![Image 51](https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg) [Yuhang Zang](https://huggingface.co/yuhangzang) ,

Rui Qian ,

 ![Image 52](https://huggingface.co/avatars/456049dba67638d3cdb330cdf383f272.svg) [Xilin Wei](https://huggingface.co/Wiselnn) ,

 ![Image 53](https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png) [Lin Chen](https://huggingface.co/Lin-Chen) ,

 ![Image 54](https://huggingface.co/avatars/883f6ba38b993476115dfafcef9ce3c1.svg) [Yifei Li](https://huggingface.co/JoeLeelyf) ,

 ![Image 55](https://huggingface.co/avatars/f09ff031c278bc42bfd7a563853e142c.svg) [Junbo Niu](https://huggingface.co/Niujunbo2002) ,

 ![Image 56](https://huggingface.co/avatars/2f62f83f9c5c4cc9444571f067cd85b7.svg) [Shuangrui Ding](https://huggingface.co/Mar2Ding) ,

 ![Image 57](https://huggingface.co/avatars/a85635d886c7f157b6723dec5c01c030.svg) [Qipeng Guo](https://huggingface.co/QipengGuo) ,

 ![Image 58](https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png) [Haodong Duan](https://huggingface.co/KennyUTC) ,

Xin Chen ,

Han Lv ,

Zheng Nie ,

Min Zhang ,

Bin Wang ,

 ![Image 59](https://huggingface.co/avatars/18958b8406d1ce492b54c1c839f18c54.svg) [Wenwei Zhang](https://huggingface.co/ZwwWayne) ,

 ![Image 60](https://huggingface.co/avatars/60ba3e211c73b1c6c78460aff8699584.svg) [Xinyue Zhang](https://huggingface.co/XinyueZhang1997) ,

Jiaye Ge ,

Wei Li ,

 ![Image 61](https://huggingface.co/avatars/b07b264669fce4c75e063e6392ebd74b.svg) [Jingwen Li](https://huggingface.co/Jingwen)

+7 authors

Abstract
--------

Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.

[View arXiv page](https://arxiv.org/abs/2412.09596) [View PDF](https://arxiv.org/pdf/2412.09596) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)

*   

### Community

 ![Image 62](https://huggingface.co/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg) [myownskyW7](https://huggingface.co/myownskyW7)Paper author Paper submitter [Dec 13, 2024](https://huggingface.co/papers/2412.09596#675ba28f0f2c2a510ddb00d0)

Code: [https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive)  
Models: [https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b)

+

Reply

 ![Image 63](https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg) [librarian-bot](https://huggingface.co/librarian-bot)[Dec 14, 2024](https://huggingface.co/papers/2412.09596#675ce0d95382ed3e733002fd)

This is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.

The following papers were recommended by the Semantic Scholar API

*   [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://huggingface.co/papers/2412.09501) (2024)
*   [AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?](https://huggingface.co/papers/2412.02611) (2024)
*   [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://huggingface.co/papers/2410.19168) (2024)
*   [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://huggingface.co/papers/2410.12219) (2024)
*   [VideoSAVi: Self-Aligned Video Language Models without Human Supervision](https://huggingface.co/papers/2412.00624) (2024)
*   [Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model](https://huggingface.co/papers/2411.05903) (2024)
*   [Spider: Any-to-Many Multimodal LLM](https://huggingface.co/papers/2411.09439) (2024)

Please give a thumbs up to this comment if you found it helpful!

If you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space

You can directly ask Librarian Bot for paper recommendations by tagging it in a comment: ```

[@librarian-bot](https://huggingface.co/librarian-bot)
	 recommend
```

+

Reply

 ![Image 64](https://huggingface.co/avatars/fd080dbfb4fdd0a587281fcbd453beff.svg) [neltherion](https://huggingface.co/neltherion)[Dec 15, 2024](https://huggingface.co/papers/2412.09596#675e83845382ed3e73bb26c1)

Is there any way to extract embeddings from Videos and Images instead of chatting with them?

+

Reply

EditPreview

Upload images, audio, and videos by dragging in the text input, pasting, or clicking here.

Tap or paste here to upload images

Comment· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2412.09596) or [log in](https://huggingface.co/login?next=%2Fpapers%2F2412.09596) to comment

 [Upvote 93](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)

*   [![Image 65](https://huggingface.co/avatars/8f256f3249bbc24d1cceb6f464eb06b5.svg)](https://huggingface.co/LLMbeginer "LLMbeginer")
*   [![Image 66](https://cdn-avatars.huggingface.co/v1/production/uploads/6447d332ab5c7251886d6fd1/bm5nwIp5CA_HosO8wXFvI.jpeg)](https://huggingface.co/zkniu "zkniu")
*   [![Image 67](https://huggingface.co/avatars/e6188562254f75a09b4048b800860016.svg)](https://huggingface.co/BeichenZhang "BeichenZhang")
*   [![Image 68](https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg)](https://huggingface.co/Zery "Zery")
*   [![Image 69](https://huggingface.co/avatars/013b79d79df719fa51bfd4bda5041695.svg)](https://huggingface.co/Willow123 "Willow123")
*   [![Image 70](https://huggingface.co/avatars/2ae2710753ce34a04937384bc6dddf70.svg)](https://huggingface.co/Songweii "Songweii")
*   [![Image 71](https://huggingface.co/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg)](https://huggingface.co/MichaelBarryUK "MichaelBarryUK")
*   [![Image 72](https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg)](https://huggingface.co/rookiexiong "rookiexiong")
*   [![Image 73](https://huggingface.co/avatars/883f6ba38b993476115dfafcef9ce3c1.svg)](https://huggingface.co/JoeLeelyf "JoeLeelyf")
*   [![Image 74](https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg)](https://huggingface.co/Tommy930 "Tommy930")
*   [![Image 75](https://huggingface.co/avatars/f09ff031c278bc42bfd7a563853e142c.svg)](https://huggingface.co/Niujunbo2002 "Niujunbo2002")
*   [![Image 76](https://huggingface.co/avatars/ceaa73b79f448996187f07733d96b800.svg)](https://huggingface.co/yujieouo "yujieouo")
*   +81

Models citing this paper 1
--------------------------

[![Image 77](https://cdn-avatars.huggingface.co/v1/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg) #### internlm/internlm-xcomposer2d5-ol-7b Visual Question Answering • Updated 5 days ago • 34 • 47](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b)

Datasets citing this paper 0
----------------------------

No dataset linking this paper

Cite arxiv.org/abs/2412.09596 in a dataset README.md to link it from this page.

### Spaces citing this paper 0

No Space linking this paper

Cite arxiv.org/abs/2412.09596 in a Space README.md to link it from this page.

Collections including this paper 14
-----------------------------------

[#### VisionLM Collection 638 items • Updated about 9 hours ago • 40](https://huggingface.co/collections/zerozeyi/visionlm-65d1c4fb95c8c685860a081c)

[#### multimodal Collection 215 items • Updated about 14 hours ago • 8](https://huggingface.co/collections/zzfive/multimodal-6655461b80b530dba7caa00f)

[#### LLM Training Collection 41 items • Updated 9 days ago • 4](https://huggingface.co/collections/Stalin16/llm-training-673122244f195b205d97a449)

[#### LMMM Collection 8 items • Updated 30 days ago • 1](https://huggingface.co/collections/admarcosai/lmmm-657c395ff2dda5456b22524e)

[Browse 14 collections that include this paper](https://huggingface.co/collections?paper=2412.09596)

System theme

Company

[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/)[](https://huggingface.co/)

Website

[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)

## Metadata

```json
{
  "title": "Paper page - InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for\n  Long-term Streaming Video and Audio Interactions",
  "description": "Join the discussion on this paper page",
  "url": "https://huggingface.co/papers/2412.09596",
  "content": "Paper page - InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions\n===============\n\n [![Image 40: Hugging Face's logo](https://huggingface.co/front/assets/huggingface_logo-noborder.svg) Hugging Face](https://huggingface.co/)\n\n*   [Models](https://huggingface.co/models)\n*   [Datasets](https://huggingface.co/datasets)\n*   [Spaces](https://huggingface.co/spaces)\n*   [Posts](https://huggingface.co/posts)\n*   [Docs](https://huggingface.co/docs)\n*   [Enterprise](https://huggingface.co/enterprise)\n*   [Pricing](https://huggingface.co/pricing)\n\n*   * * *\n    \n*   [Log In](https://huggingface.co/login)\n*   [Sign Up](https://huggingface.co/join)\n\n[Papers](https://huggingface.co/papers)\n\narxiv:2412.09596\n\nInternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions\n======================================================================================================================\n\nPublished on Dec 12, 2024\n\n· Submitted by [![Image 41](https://huggingface.co/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg) myownskyW7](https://huggingface.co/myownskyW7) on Dec 13, 2024\n\n[#2 Paper of the day](https://huggingface.co/papers?date=2024-12-13)\n\n [Upvote 93](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)\n\n*   [![Image 42](https://huggingface.co/avatars/8f256f3249bbc24d1cceb6f464eb06b5.svg)](https://huggingface.co/LLMbeginer \"LLMbeginer\")\n*   [![Image 43](https://cdn-avatars.huggingface.co/v1/production/uploads/6447d332ab5c7251886d6fd1/bm5nwIp5CA_HosO8wXFvI.jpeg)](https://huggingface.co/zkniu \"zkniu\")\n*   [![Image 44](https://huggingface.co/avatars/e6188562254f75a09b4048b800860016.svg)](https://huggingface.co/BeichenZhang \"BeichenZhang\")\n*   [![Image 45](https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg)](https://huggingface.co/Zery \"Zery\")\n*   [![Image 46](https://huggingface.co/avatars/013b79d79df719fa51bfd4bda5041695.svg)](https://huggingface.co/Willow123 \"Willow123\")\n*   [![Image 47](https://huggingface.co/avatars/2ae2710753ce34a04937384bc6dddf70.svg)](https://huggingface.co/Songweii \"Songweii\")\n*   [![Image 48](https://huggingface.co/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg)](https://huggingface.co/MichaelBarryUK \"MichaelBarryUK\")\n*   [![Image 49](https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg)](https://huggingface.co/rookiexiong \"rookiexiong\")\n*   +85\n\nAuthors:\n\nPan Zhang ,\n\nXiaoyi Dong ,\n\n ![Image 50](https://huggingface.co/avatars/b320c77dfad039d9f9c54127f610d44f.svg) [Yuhang Cao](https://huggingface.co/yhcao) ,\n\n ![Image 51](https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/iUQm5FAomzqYi6fkqIn9F.jpeg) [Yuhang Zang](https://huggingface.co/yuhangzang) ,\n\nRui Qian ,\n\n ![Image 52](https://huggingface.co/avatars/456049dba67638d3cdb330cdf383f272.svg) [Xilin Wei](https://huggingface.co/Wiselnn) ,\n\n ![Image 53](https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png) [Lin Chen](https://huggingface.co/Lin-Chen) ,\n\n ![Image 54](https://huggingface.co/avatars/883f6ba38b993476115dfafcef9ce3c1.svg) [Yifei Li](https://huggingface.co/JoeLeelyf) ,\n\n ![Image 55](https://huggingface.co/avatars/f09ff031c278bc42bfd7a563853e142c.svg) [Junbo Niu](https://huggingface.co/Niujunbo2002) ,\n\n ![Image 56](https://huggingface.co/avatars/2f62f83f9c5c4cc9444571f067cd85b7.svg) [Shuangrui Ding](https://huggingface.co/Mar2Ding) ,\n\n ![Image 57](https://huggingface.co/avatars/a85635d886c7f157b6723dec5c01c030.svg) [Qipeng Guo](https://huggingface.co/QipengGuo) ,\n\n ![Image 58](https://cdn-avatars.huggingface.co/v1/production/uploads/1676546883247-noauth.png) [Haodong Duan](https://huggingface.co/KennyUTC) ,\n\nXin Chen ,\n\nHan Lv ,\n\nZheng Nie ,\n\nMin Zhang ,\n\nBin Wang ,\n\n ![Image 59](https://huggingface.co/avatars/18958b8406d1ce492b54c1c839f18c54.svg) [Wenwei Zhang](https://huggingface.co/ZwwWayne) ,\n\n ![Image 60](https://huggingface.co/avatars/60ba3e211c73b1c6c78460aff8699584.svg) [Xinyue Zhang](https://huggingface.co/XinyueZhang1997) ,\n\nJiaye Ge ,\n\nWei Li ,\n\n ![Image 61](https://huggingface.co/avatars/b07b264669fce4c75e063e6392ebd74b.svg) [Jingwen Li](https://huggingface.co/Jingwen)\n\n+7 authors\n\nAbstract\n--------\n\nCreating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time.\n\n[View arXiv page](https://arxiv.org/abs/2412.09596) [View PDF](https://arxiv.org/pdf/2412.09596) [Add to collection](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)\n\n*   \n\n### Community\n\n ![Image 62](https://huggingface.co/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg) [myownskyW7](https://huggingface.co/myownskyW7)Paper author Paper submitter [Dec 13, 2024](https://huggingface.co/papers/2412.09596#675ba28f0f2c2a510ddb00d0)\n\nCode: [https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive)  \nModels: [https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b)\n\n+\n\nReply\n\n ![Image 63](https://cdn-avatars.huggingface.co/v1/production/uploads/1674830754237-63d3e0e8ff1384ce6c5dd17d.jpeg) [librarian-bot](https://huggingface.co/librarian-bot)[Dec 14, 2024](https://huggingface.co/papers/2412.09596#675ce0d95382ed3e733002fd)\n\nThis is an automated message from the [Librarian Bot](https://huggingface.co/librarian-bots). I found the following papers similar to this paper.\n\nThe following papers were recommended by the Semantic Scholar API\n\n*   [Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition](https://huggingface.co/papers/2412.09501) (2024)\n*   [AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?](https://huggingface.co/papers/2412.02611) (2024)\n*   [MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark](https://huggingface.co/papers/2410.19168) (2024)\n*   [OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities](https://huggingface.co/papers/2410.12219) (2024)\n*   [VideoSAVi: Self-Aligned Video Language Models without Human Supervision](https://huggingface.co/papers/2412.00624) (2024)\n*   [Towards Multi-Modal Mastery: A 4.5B Parameter Truly Multi-Modal Small Language Model](https://huggingface.co/papers/2411.05903) (2024)\n*   [Spider: Any-to-Many Multimodal LLM](https://huggingface.co/papers/2411.09439) (2024)\n\nPlease give a thumbs up to this comment if you found it helpful!\n\nIf you want recommendations for any Paper on Hugging Face checkout [this](https://huggingface.co/spaces/librarian-bots/recommend_similar_papers) Space\n\nYou can directly ask Librarian Bot for paper recommendations by tagging it in a comment: ```\n\n[@librarian-bot](https://huggingface.co/librarian-bot)\n\t recommend\n```\n\n+\n\nReply\n\n ![Image 64](https://huggingface.co/avatars/fd080dbfb4fdd0a587281fcbd453beff.svg) [neltherion](https://huggingface.co/neltherion)[Dec 15, 2024](https://huggingface.co/papers/2412.09596#675e83845382ed3e73bb26c1)\n\nIs there any way to extract embeddings from Videos and Images instead of chatting with them?\n\n+\n\nReply\n\nEditPreview\n\nUpload images, audio, and videos by dragging in the text input, pasting, or clicking here.\n\nTap or paste here to upload images\n\nComment· [Sign up](https://huggingface.co/join?next=%2Fpapers%2F2412.09596) or [log in](https://huggingface.co/login?next=%2Fpapers%2F2412.09596) to comment\n\n [Upvote 93](https://huggingface.co/login?next=%2Fpapers%2F2412.09596)\n\n*   [![Image 65](https://huggingface.co/avatars/8f256f3249bbc24d1cceb6f464eb06b5.svg)](https://huggingface.co/LLMbeginer \"LLMbeginer\")\n*   [![Image 66](https://cdn-avatars.huggingface.co/v1/production/uploads/6447d332ab5c7251886d6fd1/bm5nwIp5CA_HosO8wXFvI.jpeg)](https://huggingface.co/zkniu \"zkniu\")\n*   [![Image 67](https://huggingface.co/avatars/e6188562254f75a09b4048b800860016.svg)](https://huggingface.co/BeichenZhang \"BeichenZhang\")\n*   [![Image 68](https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg)](https://huggingface.co/Zery \"Zery\")\n*   [![Image 69](https://huggingface.co/avatars/013b79d79df719fa51bfd4bda5041695.svg)](https://huggingface.co/Willow123 \"Willow123\")\n*   [![Image 70](https://huggingface.co/avatars/2ae2710753ce34a04937384bc6dddf70.svg)](https://huggingface.co/Songweii \"Songweii\")\n*   [![Image 71](https://huggingface.co/avatars/88bb4c4a67dc8958069e9014f5e73a0b.svg)](https://huggingface.co/MichaelBarryUK \"MichaelBarryUK\")\n*   [![Image 72](https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg)](https://huggingface.co/rookiexiong \"rookiexiong\")\n*   [![Image 73](https://huggingface.co/avatars/883f6ba38b993476115dfafcef9ce3c1.svg)](https://huggingface.co/JoeLeelyf \"JoeLeelyf\")\n*   [![Image 74](https://cdn-avatars.huggingface.co/v1/production/uploads/620783f24e28382272337ba4/zkUveQPNiDfYjgGhuFErj.jpeg)](https://huggingface.co/Tommy930 \"Tommy930\")\n*   [![Image 75](https://huggingface.co/avatars/f09ff031c278bc42bfd7a563853e142c.svg)](https://huggingface.co/Niujunbo2002 \"Niujunbo2002\")\n*   [![Image 76](https://huggingface.co/avatars/ceaa73b79f448996187f07733d96b800.svg)](https://huggingface.co/yujieouo \"yujieouo\")\n*   +81\n\nModels citing this paper 1\n--------------------------\n\n[![Image 77](https://cdn-avatars.huggingface.co/v1/production/uploads/6445306bc525660aa2099ecc/ipmEgm86UIby2q5q7NkKm.jpeg) #### internlm/internlm-xcomposer2d5-ol-7b Visual Question Answering • Updated 5 days ago • 34 • 47](https://huggingface.co/internlm/internlm-xcomposer2d5-ol-7b)\n\nDatasets citing this paper 0\n----------------------------\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2412.09596 in a dataset README.md to link it from this page.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2412.09596 in a Space README.md to link it from this page.\n\nCollections including this paper 14\n-----------------------------------\n\n[#### VisionLM Collection 638 items • Updated about 9 hours ago • 40](https://huggingface.co/collections/zerozeyi/visionlm-65d1c4fb95c8c685860a081c)\n\n[#### multimodal Collection 215 items • Updated about 14 hours ago • 8](https://huggingface.co/collections/zzfive/multimodal-6655461b80b530dba7caa00f)\n\n[#### LLM Training Collection 41 items • Updated 9 days ago • 4](https://huggingface.co/collections/Stalin16/llm-training-673122244f195b205d97a449)\n\n[#### LMMM Collection 8 items • Updated 30 days ago • 1](https://huggingface.co/collections/admarcosai/lmmm-657c395ff2dda5456b22524e)\n\n[Browse 14 collections that include this paper](https://huggingface.co/collections?paper=2412.09596)\n\nSystem theme\n\nCompany\n\n[TOS](https://huggingface.co/terms-of-service) [Privacy](https://huggingface.co/privacy) [About](https://huggingface.co/huggingface) [Jobs](https://apply.workable.com/huggingface/)[](https://huggingface.co/)\n\nWebsite\n\n[Models](https://huggingface.co/models) [Datasets](https://huggingface.co/datasets) [Spaces](https://huggingface.co/spaces) [Pricing](https://huggingface.co/pricing) [Docs](https://huggingface.co/docs)",
  "usage": {
    "tokens": 4125
  }
}
```
