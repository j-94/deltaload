2.16 Project: Building a Knowledge Base from Texts — Practical NLP with Python
===============        

Toggle navigation sidebar

Toggle in-page Table of Contents

 [![Image 9](https://www.nlplanet.org/images/logo.png) NLPlanet](https://www.nlplanet.org/)[Practical NLP with Python =========================](https://www.nlplanet.org/course-practical-nlp/)

*   [Welcome to the Course](https://www.nlplanet.org/course-practical-nlp/)

Refresher

*   [AI and ML Refresher](https://www.nlplanet.org/course-practical-nlp/00-ai-and-ml-refresher)

Intro to NLP

*   [1.1 A Brief History of NLP](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/01-what-is-nlp)
*   [1.2 What Tasks Can I Solve with NLP Today?](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/02-nlp-tasks)
*   [1.3 How It Started: Grammars and Why Human Language Is Hard for Computers](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/03-human-language-is-hard)
*   [1.4 Statistical Approaches and Text Classification with N-grams](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/04-n-grams)
*   [1.5 Stemming, Lemmatization, Stopwords, POS Tagging](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/05-tokenization-stemming-lemmatization)
*   [1.6 Project: Classify Medium Articles with Bag of Words](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/06-classify-articles)
*   [1.7 Project: Search Engine over Medium with Bag of Words](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/07-search-engine-bow)
*   [1.8 Project: Text Generation with N-Grams](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/08-text-generation-n-grams)
*   [1.9 Representing Texts as Vectors: TF-IDF](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/09-text-as-vectors-bow-tfidf)
*   [1.10 Project: Search Engine over Medium with TF-IDF](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/10-search-engine-tfidf)
*   [1.11 Project: Recommender System over Medium with TF-IDF](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/10a-recsys-tfidf)
*   [1.12 Representing Texts as Vectors: Word Embeddings](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/11-text-as-vectors-embeddings)
*   [1.13 Project: Search Engine over Medium with Embeddings](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/12-search-engine-embeddings)
*   [1.14 Project: Classify Medium Articles with Embeddings](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/13-classify-articles-embeddings)
*   [Chapter Quiz](https://www.nlplanet.org/course-practical-nlp/01-intro-to-nlp/14-chapter-quiz)

Hugging Face and Pre-trained Models

*   [2.1 First Steps with Hugging Face](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/01-first-steps-huggingface)
*   [2.2 Hugging Face Hub: Models and Model Cards](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/02-huggingface-hub-and-model-cards)
*   [2.3 Hugging Face Hub: Datasets](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/03-huggingface-hub-datasets)
*   [2.4 Hugging Face Spaces](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/04-huggingface-spaces)
*   [2.5 Hugging Face Pipeline for Quick Prototyping](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/05-huggingface-pipeline)
*   [2.6 Project: Finding the Tweets with Negative Sentiment about a Product](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/06-tweets-sentiment)
*   [2.7 Evaluating a Sentiment Analysis Model](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/07-evaluate-sentiment-analysis)
*   [2.8 Project: Detecting Emotions from Text](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/08-emotion-classification)
*   [2.9 Project: Language Detection](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/09-language-detection)
*   [2.10 Semantic Search on Big Data](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/10-semantic-search-big-data)
*   [2.11 Project: Multilingual Search and Recsys over Wikipedia](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/11-multilingual-search-recsys-wikipedia)
*   [2.12 Project: Clustering Newspaper Articles](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/12-clustering-articles)
*   [2.13 Project: Semantic Image Search over Unsplash](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/13-semantic-image-search)
*   [2.14 Project: Named Entity Recognition](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/14-named-entity-recognition)
*   [2.15 Knowledge Graphs](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/15-knowledge-graphs)
*   [2.16 Project: Building a Knowledge Base from Texts](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#)
*   [2.17 Question Answering](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/17-question-answering)
*   [2.18 Text Summarization](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/18-text-summarization)
*   [Chapter Quiz](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/19-chapter-quiz)

A Primer on Transformers

*   [3.1 Intro to Transformers and Why They Are So Used Today](https://www.nlplanet.org/course-practical-nlp/03-transformers-primer/01-what-are-transformers)

Powered by [Jupyter Book](https://jupyterbook.org/)

*    [![Image 10](https://www.nlplanet.org/course-practical-nlp/_static/images/logo_binder.svg)Binder](https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/02-practical-nlp-first-tasks/16-knowledge-graph-from-text.ipynb "Launch on Binder")

*   [.ipynb](https://www.nlplanet.org/course-practical-nlp/_sources/02-practical-nlp-first-tasks/16-knowledge-graph-from-text.ipynb "Download source file")
*   .pdf

Contents

*   [2.16 Project: Building a Knowledge Base from Texts](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#)
    *   [How to Build a Knowledge Graph](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-to-build-a-knowledge-graph)
    *   [How REBEL Works](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-rebel-works)
    *   [Implementing the Knowledge Graph Extraction Pipeline](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#implementing-the-knowledge-graph-extraction-pipeline)
        *   [Install and Import Libraries](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#install-and-import-libraries)
        *   [Load the Relation Extraction Model](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#load-the-relation-extraction-model)
        *   [From Short Text to KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-short-text-to-kb)
        *   [From Long Text to KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-long-text-to-kb)
        *   [Filter and Normalize Entities with Wikipedia](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#filter-and-normalize-entities-with-wikipedia)
        *   [Extract KB from Web Article](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#extract-kb-from-web-article)
        *   [Google News: Extract KB from Multiple Articles](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#google-news-extract-kb-from-multiple-articles)
        *   [Visualize KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#visualize-kb)
*   [Code Exercises](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#code-exercises)
*   [Quiz](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#quiz)
*   [Questions and Feedbacks](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#questions-and-feedbacks)

2.16 Project: Building a Knowledge Base from Texts
==================================================

Contents
--------

*   [2.16 Project: Building a Knowledge Base from Texts](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#)
    *   [How to Build a Knowledge Graph](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-to-build-a-knowledge-graph)
    *   [How REBEL Works](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-rebel-works)
    *   [Implementing the Knowledge Graph Extraction Pipeline](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#implementing-the-knowledge-graph-extraction-pipeline)
        *   [Install and Import Libraries](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#install-and-import-libraries)
        *   [Load the Relation Extraction Model](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#load-the-relation-extraction-model)
        *   [From Short Text to KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-short-text-to-kb)
        *   [From Long Text to KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-long-text-to-kb)
        *   [Filter and Normalize Entities with Wikipedia](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#filter-and-normalize-entities-with-wikipedia)
        *   [Extract KB from Web Article](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#extract-kb-from-web-article)
        *   [Google News: Extract KB from Multiple Articles](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#google-news-extract-kb-from-multiple-articles)
        *   [Visualize KB](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#visualize-kb)
*   [Code Exercises](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#code-exercises)
*   [Quiz](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#quiz)
*   [Questions and Feedbacks](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#questions-and-feedbacks)

2.16 Project: Building a Knowledge Base from Texts[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#project-building-a-knowledge-base-from-texts "Permalink to this headline")
========================================================================================================================================================================================================================================

In this article, we see how to implement a pipeline for extracting a Knowledge Base from texts or online articles. We’ll talk about [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition), [Relation Extraction](https://en.wikipedia.org/wiki/Relationship_extraction), [Entity Linking](https://it.wikipedia.org/wiki/Entity_linking), and other common steps done when building Knowledge Graphs.

You can try the [final demo on its Hugging Face Space](https://huggingface.co/spaces/fabiochiu/text-to-kb).

![Image 11: ../_images/build_kg_1.png](https://www.nlplanet.org/course-practical-nlp/_images/build_kg_1.png)Here is an example of a knowledge graph extracted from 20 news articles about “Google”. At the end of this lesson, you’ll be able to build knowledge graphs from any list of articles you like.

![Image 12: ../_images/build_kg_2.png](https://www.nlplanet.org/course-practical-nlp/_images/build_kg_2.png)Let’s zoom into it to read about its entities and relations.

![Image 13: ../_images/build_kg_3.png](https://www.nlplanet.org/course-practical-nlp/_images/build_kg_3.png)So, this is what we are going to do:

1.  Learn how to build knowledge graphs and how the REBEL model works.
    
2.  Implement a full pipeline that extracts relations from texts and builds a knowledge graph.
    
3.  Visualize the knowledge graph.
    

How to Build a Knowledge Graph[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-to-build-a-knowledge-graph "Permalink to this headline")
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

To build a knowledge graph from text, we typically need to perform two steps:

1.  Extract entities, a.k.a. [Named Entity Recognition (NER)](https://en.wikipedia.org/wiki/Named-entity_recognition), which are going to be the nodes of the knowledge graph.
    
2.  Extract relations between the entities, a.k.a. [Relation Classification (RC)](https://paperswithcode.com/task/relation-classification), which are going to be the edges of the knowledge graph.
    

These multiple-step pipelines often propagate errors or are limited to a small number of relation types. Recently, end-to-end approaches have been proposed to tackle both tasks simultaneously. This task is usually referred to as [Relation Extraction (RE)](https://paperswithcode.com/task/relation-extraction). In this article, we’ll use an end-to-end model called REBEL, from the paper [Relation Extraction By End-to-end Language generation](https://github.com/Babelscape/rebel/blob/main/docs/EMNLP_2021_REBEL__Camera_Ready_.pdf).

How REBEL Works[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#how-rebel-works "Permalink to this headline")
------------------------------------------------------------------------------------------------------------------------------------------------------------------------

REBEL is a text2text model trained by [BabelScape](https://babelscape.com/) by fine-tuning [BART](https://huggingface.co/docs/transformers/model_doc/bart) for translating a raw input sentence containing entities and implicit relations into a set of triplets that explicitly refer to those relations. It has been trained on more than 200 different relation types.

The authors created a custom dataset for REBEL pre-training, using entities and relations found in Wikipedia abstracts and [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page), and filtering them using a [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) [Natural Language Inference](https://paperswithcode.com/task/natural-language-inference) model (similar to [this model](https://huggingface.co/roberta-large-mnli)). Have a look at the paper to know more about the creation process of the dataset. The authors also [published their dataset on the Hugging Face Hub](https://huggingface.co/datasets/Babelscape/rebel-dataset).

The model performs quite well on an array of Relation Extraction and Relation Classification benchmarks.

You can find [REBEL in the Hugging Face Hub](https://huggingface.co/Babelscape/rebel-large).

Implementing the Knowledge Graph Extraction Pipeline[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#implementing-the-knowledge-graph-extraction-pipeline "Permalink to this headline")
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Here is what we are going to do, progressively tackling more complex scenarios:

1.  Load the Relation Extraction REBEL model.
    
2.  Extract a knowledge base from a short text.
    
3.  Extract a knowledge base from a long text.
    
4.  Filter and normalize entities.
    
5.  Extract a knowledge base from an article at a specific URL.
    
6.  Extract a knowledge base from multiple URLs.
    
7.  Visualize knowledge bases.
    

### Install and Import Libraries[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#install-and-import-libraries "Permalink to this headline")

First, let’s install the required libraries.

pip install transformers wikipedia newspaper3k GoogleNews pyvis

We need each library for the following reasons:

*   [transformers](https://github.com/huggingface/transformers): Load the REBEL mode.
    
*   [wikipedia](https://pypi.org/project/wikipedia/): Validate extracted entities by checking if they have a corresponding Wikipedia page.
    
*   [newspaper](https://github.com/codelucas/newspaper): Parse articles from URLs.
    
*   [GoogleNews](https://github.com/Iceloof/GoogleNews): Read Google News latest articles about a topic.
    
*   [pyvis](https://pyvis.readthedocs.io/en/latest/index.html): Graphs visualizations.
    

Let’s import all the necessary libraries and classes.

\# needed to load the REBEL model
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import math
import torch

\# wrapper for wikipedia API
import wikipedia

\# scraping of web articles
from newspaper import Article, ArticleException

\# google news scraping
from GoogleNews import GoogleNews

\# graph visualization
from pyvis.network import Network

\# show HTML in notebook
import IPython

### Load the Relation Extraction Model[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#load-the-relation-extraction-model "Permalink to this headline")

Thanks to the `transformers` library, we can load the pre-trained REBEL model and tokenizer with a few lines of code.

\# Load model and tokenizer
tokenizer \= AutoTokenizer.from\_pretrained("Babelscape/rebel-large")
model \= AutoModelForSeq2SeqLM.from\_pretrained("Babelscape/rebel-large")

### From Short Text to KB[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-short-text-to-kb "Permalink to this headline")

The next step is to write a function that is able to parse the strings generated by REBEL and transform them into relation triplets (e.g. the <Fabio, lives in, Italy\> triplet). This function must take into account additional new tokens (i.e. the `<triplet>` , `<subj>`, and `<obj>` tokens) used while training the model. Fortunately, the [REBEL model card](https://huggingface.co/Babelscape/rebel-large) provides us with a complete code example for this function, which we’ll use as-is.

\# from https://huggingface.co/Babelscape/rebel-large
def extract\_relations\_from\_model\_output(text):
    relations \= \[\]
    relation, subject, relation, object\_ \= '', '', '', ''
    text \= text.strip()
    current \= 'x'
    text\_replaced \= text.replace("<s\>", "").replace("<pad\>", "").replace("</s\>", "")
    for token in text\_replaced.split():
        if token \== "<triplet\>":
            current \= 't'
            if relation != '':
                relations.append({
                    'head': subject.strip(),
                    'type': relation.strip(),
                    'tail': object\_.strip()
                })
                relation \= ''
            subject \= ''
        elif token \== "<subj\>":
            current \= 's'
            if relation != '':
                relations.append({
                    'head': subject.strip(),
                    'type': relation.strip(),
                    'tail': object\_.strip()
                })
            object\_ \= ''
        elif token \== "<obj\>":
            current \= 'o'
            relation \= ''
        else:
            if current \== 't':
                subject += ' ' + token
            elif current \== 's':
                object\_ += ' ' + token
            elif current \== 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object\_ != '':
        relations.append({
            'head': subject.strip(),
            'type': relation.strip(),
            'tail': object\_.strip()
        })
    return relations

The function outputs a list of relations, where each relation is represented as a dictionary with the following keys:

*   `head`: The subject of the relation (e.g. “Fabio”).
    
*   `type`: The relation type (e.g. “lives in”).
    
*   `tail`: The object of the relation (e.g. “Italy”).
    

Next, let’s write the code for implementing a knowledge base class. Our KB class is made of a list of relations and has several methods to deal with adding new relations to the knowledge base or printing them. It implements a very simple logic at the moment.

\# knowledge base class
class KB():
    def \_\_init\_\_(self):
        self.relations \= \[\]

    def are\_relations\_equal(self, r1, r2):
        return all(r1\[attr\] \== r2\[attr\] for attr in \["head", "type", "tail"\])

    def exists\_relation(self, r1):
        return any(self.are\_relations\_equal(r1, r2) for r2 in self.relations)

    def add\_relation(self, r):
        if not self.exists\_relation(r):
            self.relations.append(r)

    def print(self):
        print("Relations:")
        for r in self.relations:
            print(f"  {r}")

Last, we define a `from_small_text_to_kb` function that returns a `KB` object with relations extracted from a short text. It does the following:

1.  Initialize an empty knowledge base KB object.
    
2.  Tokenize the input text.
    
3.  Use REBEL to generate relations from the text.
    
4.  Parse REBEL output and store relation triplets into the knowledge base object.
    
5.  Return the knowledge base object.
    

\# build a knowledge base from text
def from\_small\_text\_to\_kb(text, verbose\=False):
    kb \= KB()

    \# Tokenizer text
    model\_inputs \= tokenizer(text, max\_length\=512, padding\=True, truncation\=True,
                            return\_tensors\='pt')
    if verbose:
        print(f"Num tokens: {len(model\_inputs\['input\_ids'\]\[0\])}")

    \# Generate
    gen\_kwargs \= {
        "max\_length": 216,
        "length\_penalty": 0,
        "num\_beams": 3,
        "num\_return\_sequences": 3
    }
    generated\_tokens \= model.generate(
        \*\*model\_inputs,
        \*\*gen\_kwargs,
    )
    decoded\_preds \= tokenizer.batch\_decode(generated\_tokens, skip\_special\_tokens\=False)

    \# create kb
    for sentence\_pred in decoded\_preds:
        relations \= extract\_relations\_from\_model\_output(sentence\_pred)
        for r in relations:
            kb.add\_relation(r)

    return kb

Let’s try the function with some text about [Napoleon Bonaparte](https://en.wikipedia.org/wiki/Napoleon) from Wikipedia.

\# test the \`from\_small\_text\_to\_kb\` function

text \= "Napoleon Bonaparte (born Napoleone di Buonaparte; 15 August 1769 – 5 " \\
"May 1821), and later known by his regnal name Napoleon I, was a French military " \\
"and political leader who rose to prominence during the French Revolution and led " \\
"several successful campaigns during the Revolutionary Wars. He was the de facto " \\
"leader of the French Republic as First Consul from 1799 to 1804. As Napoleon I, " \\
"he was Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's " \\
"political and cultural legacy has endured, and he has been one of the most " \\
"celebrated and controversial leaders in world history."

kb \= from\_small\_text\_to\_kb(text, verbose\=True)
kb.print()

Num tokens: 133
Relations:
  {'head': 'Napoleon Bonaparte', 'type': 'date of birth', 'tail': '15 August 1769'}
  {'head': 'Napoleon Bonaparte', 'type': 'date of death', 'tail': '5 May 1821'}
  {'head': 'Napoleon Bonaparte', 'type': 'participant in', 'tail': 'French Revolution'}
  {'head': 'Napoleon Bonaparte', 'type': 'conflict', 'tail': 'Revolutionary Wars'}
  {'head': 'Revolutionary Wars', 'type': 'part of', 'tail': 'French Revolution'}
  {'head': 'French Revolution', 'type': 'participant', 'tail': 'Napoleon Bonaparte'}
  {'head': 'Revolutionary Wars', 'type': 'participant', 'tail': 'Napoleon Bonaparte'}

The model is able to extract several relations, such as Napoleon’s date of birth and date of death, and his participation in the French Revolution. Nice!

### From Long Text to KB[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#from-long-text-to-kb "Permalink to this headline")

Transformer models like REBEL have memory requirements that grow quadratically with the size of the inputs. This means that REBEL is able to work on common hardware at a reasonable speed with inputs of about 512 tokens, which correspond to about 380 English words. However, we may need to extract relations from documents long several thousands of words.

Moreover, from my experiments with the model, it seems to work better with shorter inputs. Intuitively, raw text relations are often expressed in single or contiguous sentences, therefore it may not be necessary to consider a high number of sentences at the same time to extract specific relations. Additionally, extracting a few relations is a simpler task than extracting many relations.

So, how do we put all this together?

For example, we can divide an input text long 1000 tokens into eight shorter overlapping spans long 128 tokens and extract relations from each span. While doing so, we also add some metadata to the extracted relations containing their span boundaries. With this info, we are able to see from which span of the text we extracted a specific relation which is now saved in our knowledge base.

Let’s modify the `KB` methods so that span boundaries are saved as well. The relation dictionary has now the keys:

*   `head` : The subject of the relation (e.g. “Fabio”).
    
*   `type` : The relation type (e.g. “lives in”).
    
*   `tail` : The object of the relation (e.g. “Italy”).
    
*   `meta` : A dictionary containing meta information about the relation. This dictionary has a `spans` key, whose value is the list of span boundaries (e.g. \[\[0, 128\], \[119, 247\]\]) where the relation has been found.
    

\# add \`merge\_relations\` to KB class
class KB():
    ...

    def merge\_relations(self, r1):
        r2 \= \[r for r in self.relations
              if self.are\_relations\_equal(r1, r)\]\[0\]
        spans\_to\_add \= \[span for span in r1\["meta"\]\["spans"\]
                        if span not in r2\["meta"\]\["spans"\]\]
        r2\["meta"\]\["spans"\] += spans\_to\_add

    def add\_relation(self, r):
        if not self.exists\_relation(r):
            self.relations.append(r)
        else:
            self.merge\_relations(r)

Next, we write the `from_text_to_kb` function, which is similar to the `from_small_text_to_kb` function but is able to manage longer texts by splitting them into spans. All the new code is about the spanning logic and the management of the spans into the relations.

\# extract relations for each span and put them together in a knowledge base
def from\_text\_to\_kb(text, span\_length\=128, verbose\=False):
    \# tokenize whole text
    inputs \= tokenizer(\[text\], return\_tensors\="pt")

    \# compute span boundaries
    num\_tokens \= len(inputs\["input\_ids"\]\[0\])
    if verbose:
        print(f"Input has {num\_tokens} tokens")
    num\_spans \= math.ceil(num\_tokens / span\_length)
    if verbose:
        print(f"Input has {num\_spans} spans")
    overlap \= math.ceil((num\_spans \* span\_length \- num\_tokens) / 
                        max(num\_spans \- 1, 1))
    spans\_boundaries \= \[\]
    start \= 0
    for i in range(num\_spans):
        spans\_boundaries.append(\[start + span\_length \* i,
                                 start + span\_length \* (i + 1)\])
        start \-= overlap
    if verbose:
        print(f"Span boundaries are {spans\_boundaries}")

    \# transform input with spans
    tensor\_ids \= \[inputs\["input\_ids"\]\[0\]\[boundary\[0\]:boundary\[1\]\]
                  for boundary in spans\_boundaries\]
    tensor\_masks \= \[inputs\["attention\_mask"\]\[0\]\[boundary\[0\]:boundary\[1\]\]
                    for boundary in spans\_boundaries\]
    inputs \= {
        "input\_ids": torch.stack(tensor\_ids),
        "attention\_mask": torch.stack(tensor\_masks)
    }

    \# generate relations
    num\_return\_sequences \= 3
    gen\_kwargs \= {
        "max\_length": 256,
        "length\_penalty": 0,
        "num\_beams": 3,
        "num\_return\_sequences": num\_return\_sequences
    }
    generated\_tokens \= model.generate(
        \*\*inputs,
        \*\*gen\_kwargs,
    )

    \# decode relations
    decoded\_preds \= tokenizer.batch\_decode(generated\_tokens,
                                           skip\_special\_tokens\=False)

    \# create kb
    kb \= KB()
    i \= 0
    for sentence\_pred in decoded\_preds:
        current\_span\_index \= i // num\_return\_sequences
        relations \= extract\_relations\_from\_model\_output(sentence\_pred)
        for relation in relations:
            relation\["meta"\] \= {
                "spans": \[spans\_boundaries\[current\_span\_index\]\]
            }
            kb.add\_relation(relation)
        i += 1

    return kb

Let’s try it with a longer text of 726 tokens about Napoleon. We are currently splitting the text into spans long 128 tokens.

text \= """
Napoleon Bonaparte (born Napoleone di Buonaparte; 15 August 1769 – 5 May 1821), and later known by his regnal name Napoleon I, was a French military and political leader who rose to prominence during the French Revolution and led several successful campaigns during the Revolutionary Wars. He was the de facto leader of the French Republic as First Consul from 1799 to 1804. As Napoleon I, he was Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's political and cultural legacy has endured, and he has been one of the most celebrated and controversial leaders in world history. Napoleon was born on the island of Corsica not long after its annexation by the Kingdom of France.\[5\] He supported the French Revolution in 1789 while serving in the French army, and tried to spread its ideals to his native Corsica. He rose rapidly in the Army after he saved the governing French Directory by firing on royalist insurgents. In 1796, he began a military campaign against the Austrians and their Italian allies, scoring decisive victories and becoming a national hero. Two years later, he led a military expedition to Egypt that served as a springboard to political power. He engineered a coup in November 1799 and became First Consul of the Republic. Differences with the British meant that the French faced the War of the Third Coalition by 1805. Napoleon shattered this coalition with victories in the Ulm Campaign, and at the Battle of Austerlitz, which led to the dissolving of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him because Prussia became worried about growing French influence on the continent. Napoleon knocked out Prussia at the battles of Jena and Auerstedt, marched the Grande Armée into Eastern Europe, annihilating the Russians in June 1807 at Friedland, and forcing the defeated nations of the Fourth Coalition to accept the Treaties of Tilsit. Two years later, the Austrians challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram. Hoping to extend the Continental System, his embargo against Britain, Napoleon invaded the Iberian Peninsula and declared his brother Joseph King of Spain in 1808. The Spanish and the Portuguese revolted in the Peninsular War, culminating in defeat for Napoleon's marshals. Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic retreat of Napoleon's Grande Armée. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France. A chaotic military campaign resulted in a large coalition army defeating Napoleon at the Battle of Leipzig in October 1813. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba, between Corsica and Italy. In France, the Bourbons were restored to power. However, Napoleon escaped Elba in February 1815 and took control of France.\[6\]\[7\] The Allies responded by forming a Seventh Coalition, which defeated Napoleon at the Battle of Waterloo in June 1815. The British exiled him to the remote island of Saint Helena in the Atlantic, where he died in 1821 at the age of 51. Napoleon had an extensive impact on the modern world, bringing liberal reforms to the many countries he conquered, especially the Low Countries, Switzerland, and parts of modern Italy and Germany. He implemented liberal policies in France and Western Europe.
"""

kb \= from\_text\_to\_kb(text, verbose\=True)
kb.print()

Input has 726 tokens
Input has 6 spans
Span boundaries are \[\[0, 128\], \[119, 247\], \[238, 366\], \[357, 485\], \[476, 604\], \[595, 723\]\]
Relations:
  {'head': 'Napoleon Bonaparte', 'type': 'date of birth',
   'tail': '15 August 1769', 'meta': {'spans': \[\[0, 128\]\]}}
  ...
  {'head': 'Napoleon', 'type': 'place of birth',
   'tail': 'Corsica', 'meta': {'spans': \[\[119, 247\]\]}}
  ...
  {'head': 'Fourth Coalition', 'type': 'start time',
   'tail': '1806', 'meta': {'spans': \[\[238, 366\]\]}}
  ...

The text has been split into six spans, from which 23 relations have been extracted! Note that we also know from which text span each relation comes.

### Filter and Normalize Entities with Wikipedia[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#filter-and-normalize-entities-with-wikipedia "Permalink to this headline")

If you look closely at the extracted relations, you can see that there’s a relation with the entity “Napoleon Bonaparte” and a relation with the entity “Napoleon”. How can we tell our knowledge base that the two entities should be treated as the same?

One way to do this is to use the `wikipedia` library to check if “Napoleon Bonaparte” and “Napoleon” have the same Wikipedia page. If so, they are normalized to the title of the Wikipedia page. If an extracted entity doesn’t have a corresponding Wikipedia page, we ignore it at the moment. This step is commonly called [Entity Linking](https://en.wikipedia.org/wiki/Entity_linking).

Note that this approach relies on Wikipedia to be constantly updated by people with relevant entities. Therefore, it won’t work if you want to extract entities different from the ones already present in Wikipedia. Moreover, note that we are ignoring “date” (e.g. the `15 August 1769` in `<Napoleon, date of birth, 15 August 1769>`) entities for simplicity.

Let’s modify our `KB` code:

*   The `KB` now stores an `entities` dictionary with the entities of the stored relations. The keys are the entity identifiers (i.e. the title of the corresponding Wikipedia page), and the value is a dictionary containing the Wikipedia page `url` and its `summary`.
    
*   When adding a new relation, we now check its entities with the `wikipedia` library.
    

\# filter and normalize entities before adding them to the KB
class KB():
    def \_\_init\_\_(self):
        self.entities \= {}
        self.relations \= \[\]

    ...

    def get\_wikipedia\_data(self, candidate\_entity):
        try:
            page \= wikipedia.page(candidate\_entity, auto\_suggest\=False)
            entity\_data \= {
                "title": page.title,
                "url": page.url,
                "summary": page.summary
            }
            return entity\_data
        except:
            return None

    def add\_entity(self, e):
        self.entities\[e\["title"\]\] \= {k:v for k,v in e.items() if k != "title"}

    def add\_relation(self, r):
        \# check on wikipedia
        candidate\_entities \= \[r\["head"\], r\["tail"\]\]
        entities \= \[self.get\_wikipedia\_data(ent) for ent in candidate\_entities\]

        \# if one entity does not exist, stop
        if any(ent is None for ent in entities):
            return

        \# manage new entities
        for e in entities:
            self.add\_entity(e)

        \# rename relation entities with their wikipedia titles
        r\["head"\] \= entities\[0\]\["title"\]
        r\["tail"\] \= entities\[1\]\["title"\]

        \# manage new relation
        if not self.exists\_relation(r):
            self.relations.append(r)
        else:
            self.merge\_relations(r)

    def print(self):
        print("Entities:")
        for e in self.entities.items():
            print(f"  {e}")
        print("Relations:")
        for r in self.relations:
            print(f"  {r}")

Let’s extract relations and entities from the same text about Napoleon:

text \= """
Napoleon Bonaparte (born Napoleone di Buonaparte; 15 August 1769 – 5 May 1821), and later known by his regnal name Napoleon I, was a French military and political leader who rose to prominence during the French Revolution and led several successful campaigns during the Revolutionary Wars. He was the de facto leader of the French Republic as First Consul from 1799 to 1804. As Napoleon I, he was Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's political and cultural legacy has endured, and he has been one of the most celebrated and controversial leaders in world history. Napoleon was born on the island of Corsica not long after its annexation by the Kingdom of France.\[5\] He supported the French Revolution in 1789 while serving in the French army, and tried to spread its ideals to his native Corsica. He rose rapidly in the Army after he saved the governing French Directory by firing on royalist insurgents. In 1796, he began a military campaign against the Austrians and their Italian allies, scoring decisive victories and becoming a national hero. Two years later, he led a military expedition to Egypt that served as a springboard to political power. He engineered a coup in November 1799 and became First Consul of the Republic. Differences with the British meant that the French faced the War of the Third Coalition by 1805. Napoleon shattered this coalition with victories in the Ulm Campaign, and at the Battle of Austerlitz, which led to the dissolving of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him because Prussia became worried about growing French influence on the continent. Napoleon knocked out Prussia at the battles of Jena and Auerstedt, marched the Grande Armée into Eastern Europe, annihilating the Russians in June 1807 at Friedland, and forcing the defeated nations of the Fourth Coalition to accept the Treaties of Tilsit. Two years later, the Austrians challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram. Hoping to extend the Continental System, his embargo against Britain, Napoleon invaded the Iberian Peninsula and declared his brother Joseph King of Spain in 1808. The Spanish and the Portuguese revolted in the Peninsular War, culminating in defeat for Napoleon's marshals. Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic retreat of Napoleon's Grande Armée. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France. A chaotic military campaign resulted in a large coalition army defeating Napoleon at the Battle of Leipzig in October 1813. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba, between Corsica and Italy. In France, the Bourbons were restored to power. However, Napoleon escaped Elba in February 1815 and took control of France.\[6\]\[7\] The Allies responded by forming a Seventh Coalition, which defeated Napoleon at the Battle of Waterloo in June 1815. The British exiled him to the remote island of Saint Helena in the Atlantic, where he died in 1821 at the age of 51. Napoleon had an extensive impact on the modern world, bringing liberal reforms to the many countries he conquered, especially the Low Countries, Switzerland, and parts of modern Italy and Germany. He implemented liberal policies in France and Western Europe.
"""

kb \= from\_text\_to\_kb(text)
kb.print()

Entities:
 ('Napoleon', {'url': 'https://en.wikipedia.org/wiki/Napoleon',
  'summary': "Napoleon Bonaparte (born Napoleone di Buonaparte; 15 August ..."})
 ('French Revolution', {'url': 'https://en.wikipedia.org/wiki/French\_Revolution',
  'summary': 'The French Revolution (French: Révolution française..."})
 ...
Relations:
 {'head': 'Napoleon', 'type': 'participant in', 'tail': 'French Revolution',
  'meta': {'spans': \[\[0, 128\], \[119, 247\]\]}}
 {'head': 'French Revolution', 'type': 'participant', 'tail': 'Napoleon',
  'meta': {'spans': \[\[0, 128\]\]}}
 ...

All the extracted entities are linked to Wikipedia pages and normalized with their titles. “Napoleon Bonaparte” and “Napoleon” are now both referred to with “Napoleon”!

### Extract KB from Web Article[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#extract-kb-from-web-article "Permalink to this headline")

Let’s go another step further. We want our knowledge base to manage the addition of relations and entities from articles from around the web, and to keep track of where each relation comes from.

To do this, we need to modify our `KB` class so that:

*   Along with `relations` and `entities`, `sources` (i.e. articles from around the web) are stored as well. Each article has its URL as key and a dictionary with keys `article_title` and `article_publish_date` as value. We’ll see later how to extract these two features.
    
*   When we add a new relation to our knowledge base, the relation `meta` field is now a dictionary with article URLs as keys, and another dictionary containing the `spans` as value. In this way, the knowledge base keeps track of all the articles from which a specific relation has been extracted. This information can be an indicator of the quality of an extracted relation.
    

\# keep track of where relations have been extracted
class KB():
    def \_\_init\_\_(self):
        self.entities \= {} \# { entity\_title: {...} }
        self.relations \= \[\] \# \[ head: entity\_title, type: ..., tail: entity\_title,
          \# meta: { article\_url: { spans: \[...\] } } \]
        self.sources \= {} \# { article\_url: {...} }

    ...

    def merge\_relations(self, r2):
        r1 \= \[r for r in self.relations
              if self.are\_relations\_equal(r2, r)\]\[0\]

        \# if different article
        article\_url \= list(r2\["meta"\].keys())\[0\]
        if article\_url not in r1\["meta"\]:
            r1\["meta"\]\[article\_url\] \= r2\["meta"\]\[article\_url\]

        \# if existing article
        else:
            spans\_to\_add \= \[span for span in r2\["meta"\]\[article\_url\]\["spans"\]
                            if span not in r1\["meta"\]\[article\_url\]\["spans"\]\]
            r1\["meta"\]\[article\_url\]\["spans"\] += spans\_to\_add

    ...

    def add\_relation(self, r, article\_title, article\_publish\_date):
        \# check on wikipedia
        candidate\_entities \= \[r\["head"\], r\["tail"\]\]
        entities \= \[self.get\_wikipedia\_data(ent) for ent in candidate\_entities\]

        \# if one entity does not exist, stop
        if any(ent is None for ent in entities):
            return

        \# manage new entities
        for e in entities:
            self.add\_entity(e)

        \# rename relation entities with their wikipedia titles
        r\["head"\] \= entities\[0\]\["title"\]
        r\["tail"\] \= entities\[1\]\["title"\]

        \# add source if not in kb
        article\_url \= list(r\["meta"\].keys())\[0\]
        if article\_url not in self.sources:
            self.sources\[article\_url\] \= {
                "article\_title": article\_title,
                "article\_publish\_date": article\_publish\_date
            }

        \# manage new relation
        if not self.exists\_relation(r):
            self.relations.append(r)
        else:
            self.merge\_relations(r)

    def print(self):
        print("Entities:")
        for e in self.entities.items():
            print(f"  {e}")
        print("Relations:")
        for r in self.relations:
            print(f"  {r}")
        print("Sources:")
        for s in self.sources.items():
            print(f"  {s}")

Next, we modify the `from_text_to_kb` function so that it prepares the relation `meta` field taking into account article URLs as well.

\# extract text from url, extract relations and populate the KB
def from\_text\_to\_kb(text, article\_url, span\_length\=128, article\_title\=None,
                    article\_publish\_date\=None, verbose\=False):
    ...
    \# create kb
    kb \= KB()
    i \= 0
    for sentence\_pred in decoded\_preds:
        current\_span\_index \= i // num\_return\_sequences
        relations \= extract\_relations\_from\_model\_output(sentence\_pred)
        for relation in relations:
            relation\["meta"\] \= {
                article\_url: {
                    "spans": \[spans\_boundaries\[current\_span\_index\]\]
                }
            }
            kb.add\_relation(relation, article\_title, article\_publish\_date)
        i += 1

    return kb

Last, we use the `newspaper` library to download and parse articles from URLs and define a `from_url_to_kb` function. The library automatically extracts the article text, title, and publish date (if present).

\# parse an article with newspaper3k
def get\_article(url):
    article \= Article(url)
    article.download()
    article.parse()
    return article

\# extract the article from the url (along with metadata), extract relations and populate a KB
def from\_url\_to\_kb(url):
    article \= get\_article(url)
    config \= {
        "article\_title": article.title,
        "article\_publish\_date": article.publish\_date
    }
    kb \= from\_text\_to\_kb(article.text, article.url, \*\*config)
    return kb

Let’s try to extract a knowledge base from the article [Microstrategy chief: ‘Bitcoin is going to go into the millions’](https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html).

\# test the \`from\_url\_to\_kb\` function
url \= "https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html"
kb \= from\_url\_to\_kb(url)
kb.print()

Entities:
  ('MicroStrategy', {'url': 'https://en.wikipedia.org/wiki/MicroStrategy',
    'summary': "MicroStrategy Incorporated is an American company that ..."})
  ('Michael J. Saylor', {'url': 'https://en.wikipedia.org/wiki/Michael\_J.\_Saylor',
    'summary': 'Michael J. Saylor (born February 4, 1965) is an American ..."})
  ...
Relations:
  {'head': 'MicroStrategy', 'type': 'founded by', 'tail': 'Michael J. Saylor',
   'meta': {'https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html': 
     {'spans': \[\[0, 128\]\]}}}
  {'head': 'Michael J. Saylor', 'type': 'employer', 'tail': 'MicroStrategy',
   'meta': {'https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html':
     {'spans': \[\[0, 128\]\]}}}
  ...
Sources:
  ('https://finance.yahoo.com/news/microstrategy-bitcoin-millions-142143795.html',
    {'article\_title': "Microstrategy chief: 'Bitcoin is going to go into the millions'",
     'article\_publish\_date': None})

The KB is showing a lot of information!

*   From the `entities` list, we see that [Microstrategy](https://en.wikipedia.org/wiki/MicroStrategy) is an American company.
    
*   From the `relations` list, we see that [Michael J. Saylor](https://en.wikipedia.org/wiki/Michael_J._Saylor) is a founder of the Microstrategy company, and where we extracted such relation (i.e. the article URL and the text span).
    
*   From the `sources` list, we see the title and publish date of the aforementioned article.
    

### Google News: Extract KB from Multiple Articles[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#google-news-extract-kb-from-multiple-articles "Permalink to this headline")

We are almost done! Consider this last scenario: creating a knowledge base from multiple articles. We can deal with it by extracting a separate knowledge base from each article and then merging all the knowledge bases together. Let’s add a `merge_with_kb` method to our KB class.

class KB():
    ...

    def merge\_with\_kb(self, kb2):
        for r in kb2.relations:
            article\_url \= list(r\["meta"\].keys())\[0\]
            source\_data \= kb2.sources\[article\_url\]
            self.add\_relation(r, source\_data\["article\_title"\],
                              source\_data\["article\_publish\_date"\])

    ...

Then, we use the `GoogleNews` library to get the URLs of recent news articles about a specific topic. Once we have multiple URLs, we feed them to the `from_urls_to_kb` function, which extracts a knowledge base from each article and then merges them together.

\# get news links from google news
def get\_news\_links(query, lang\="en", region\="US", pages\=1, max\_links\=100000):
    googlenews \= GoogleNews(lang\=lang, region\=region)
    googlenews.search(query)
    all\_urls \= \[\]
    for page in range(pages):
        googlenews.get\_page(page)
        all\_urls += googlenews.get\_links()
    return list(set(all\_urls))\[:max\_links\]

\# build a KB from multiple news links
def from\_urls\_to\_kb(urls, verbose\=False):
    kb \= KB()
    if verbose:
        print(f"{len(urls)} links to visit")
    for url in urls:
        if verbose:
            print(f"Visiting {url}...")
        try:
            kb\_url \= from\_url\_to\_kb(url)
            kb.merge\_with\_kb(kb\_url)
        except ArticleException:
            if verbose:
                print(f"  Couldn't download article at url {url}")
    return kb

Let’s try extracting a knowledge base from three articles from Google News about “Google”.

\# test the \`from\_urls\_to\_kb\` function
news\_links \= get\_news\_links("Google", pages\=1, max\_links\=3)
kb \= from\_urls\_to\_kb(news\_links, verbose\=True)
kb.print()

3 links to visit
Visiting https://www.hindustantimes.com/india-news/google-doodle-celebrates-india-s-gama-pehlwan-the-undefeated-wrestling-champion-101653180853982.html...
Visiting https://tech.hindustantimes.com/tech/news/google-doodle-today-celebrates-gama-pehlwan-s-144th-birth-anniversary-know-who-he-is-71653191916538.html...
Visiting https://www.moneycontrol.com/news/trends/current-affairs-trends/google-doodle-celebrates-gama-pehlwan-the-amritsar-born-wrestling-champ-who-inspired-bruce-lee-8552171.html...
Entities:
  ('Google', {'url': 'https://en.wikipedia.org/wiki/Google',
    'summary': 'Google LLC is an American ...'})
  ...
Relations:
  {'head': 'Google', 'type': 'owner of', 'tail': 'Google Doodle',
    'meta': {'https://tech.hindustantimes.com/tech/news/google-doodle-today-celebrates-gama-pehlwan-s-144th-birth-anniversary-know-who-he-is-71653191916538.html':
      {'spans': \[\[0, 128\]\]}}}
  ...
Sources:
  ('https://www.hindustantimes.com/india-news/google-doodle-celebrates-india-s-gama-pehlwan-the-undefeated-wrestling-champion-101653180853982.html',
    {'article\_title': "Google Doodle celebrates India's Gama Pehlwan, the undefeated wrestling champion",
    'article\_publish\_date': datetime.datetime(2022, 5, 22, 6, 59, 56, tzinfo=tzoffset(None, 19800))})
  ('https://tech.hindustantimes.com/tech/news/google-doodle-today-celebrates-gama-pehlwan-s-144th-birth-anniversary-know-who-he-is-71653191916538.html',
    {'article\_title': "Google Doodle today celebrates Gama Pehlwan's 144th birth anniversary; know who he is",
    'article\_publish\_date': datetime.datetime(2022, 5, 22, 9, 32, 38, tzinfo=tzoffset(None, 19800))})
  ('https://www.moneycontrol.com/news/trends/current-affairs-trends/google-doodle-celebrates-gama-pehlwan-the-amritsar-born-wrestling-champ-who-inspired-bruce-lee-8552171.html',
    {'article\_title': 'Google Doodle celebrates Gama Pehlwan, the Amritsar-born wrestling champ who inspired Bruce Lee',
    'article\_publish\_date': None})

The knowledge bases are getting bigger! We got 10 entities, 10 relations, and 3 sources. Note that we know from which article each relation comes.

### Visualize KB[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#visualize-kb "Permalink to this headline")

Congratulations if you’ve read this far, we’re done with the scenarios! Let’s visualize the output of our work by plotting the knowledge bases. As our knowledge bases are graphs, we can use the `pyvis` library, which allows the creation of interactive network visualizations.

We define a `save_network_html` function that:

*   Initialize an empty directed `pyvis` network.
    
*   Add the knowledge base entities as nodes.
    
*   Add the knowledge base relations as edges.
    
*   Save the network in an HTML file.
    

\# from KB to HTML visualization
def save\_network\_html(kb, filename\="network.html"):
    \# create network
    net \= Network(directed\=True, width\="auto", height\="700px", bgcolor\="#eeeeee")

    \# nodes
    color\_entity \= "#00FF00"
    for e in kb.entities:
        net.add\_node(e, shape\="circle", color\=color\_entity)

    \# edges
    for r in kb.relations:
        net.add\_edge(r\["head"\], r\["tail"\],
                    title\=r\["type"\], label\=r\["type"\])

    \# save network
    net.repulsion(
        node\_distance\=200,
        central\_gravity\=0.2,
        spring\_length\=200,
        spring\_strength\=0.05,
        damping\=0.09
    )
    net.set\_edge\_smooth('dynamic')
    net.show(filename)

Let’s try the `save_network_html` function with a knowledge base built from 20 news articles about “Google”.

\# extract KB from news about Google and visualize it
news\_links \= get\_news\_links("Google", pages\=5, max\_links\=20)
kb \= from\_urls\_to\_kb(news\_links, verbose\=True)
filename \= "network\_3\_google.html"
save\_network\_html(kb, filename\=filename)

Remember that, even though they are not visualized, the knowledge graph saves information about the provenience of each relation (e.g. from which articles it has been extracted and other metadata), along with Wikipedia data about each entity. Visualizing knowledge graphs is useful for debugging purposes, but their main benefits come when used for inference.

This is the resulting graph:

Code Exercises[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#code-exercises "Permalink to this headline")
======================================================================================================================================================================

Go to Notebook

Quiz[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#quiz "Permalink to this headline")
==================================================================================================================================================

To build a knowledge graph from text, we typically need to perform two steps:

1.  Extract entities and extract relations between these entities.
    
2.  Identify topics and classify them into categories.
    
3.  Tokenize the text and perform lemmatization.
    
4.  Generate a list of keywords and map them to relevant concepts.
    

Answer

The correct answer is **1**.

True or False. Relation Extraction involves doing both Named Entity Recognition and Relation Classification in an end-to-end approach.

Answer

The correct answer is **True**.

What data has the REBEL relation extraction model been trained on?

1.  Unstructured text from webpages and books.
    
2.  A corpus of tagged dialogue transcripts.
    
3.  Entities and relations found in Wikipedia abstracts and Wikidata.
    

Answer

The correct answer is **3**.

What’s the job of the Entity Linking task?

1.  To link entities in a text with an existing knowledge base.
    
2.  To classify entities in a text.
    
3.  To assign a probability to each candidate entity.
    

Answer

The correct answer is **1**.

What’s the name of a Python library for plotting graphs?

1.  `seaborn`
    
2.  `matplotlib`
    
3.  `pyvis`
    
4.  `ggplot`
    

Answer

The correct answer is **3**.

Questions and Feedbacks[#](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/16-knowledge-graph-from-text#questions-and-feedbacks "Permalink to this headline")
========================================================================================================================================================================================

Have questions about this lesson? Would you like to exchange ideas? Or would you like to point out something that needs to be corrected? Join the [NLPlanet Discord server](https://discord.gg/zfC862H2dJ) and interact with the community! There’s a specific channel for this course called **practical-nlp-nlplanet**.

 

[previous 2.15 Knowledge Graphs](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/15-knowledge-graphs "previous page")[next 2.17 Question Answering](https://www.nlplanet.org/course-practical-nlp/02-practical-nlp-first-tasks/17-question-answering "next page")

By Fabio Chiusano  
© Copyright 2022.