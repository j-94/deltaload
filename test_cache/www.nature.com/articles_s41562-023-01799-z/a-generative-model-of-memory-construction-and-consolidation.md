---
title: A generative model of memory construction and consolidation
description: Episodic memories are (re)constructed, share neural substrates with imagination, combine unique features with schema-based predictions and show schema-based distortions that increase with consolidation. Here we present a computational model in which hippocampal replay (from an autoassociative network) trains generative models (variational autoencoders) to (re)create sensory experiences from latent variable representations in entorhinal, medial prefrontal and anterolateral temporal cortices via the hippocampal formation. Simulations show effects of memory age and hippocampal lesions in agreement with previous models, but also provide mechanisms for semantic memory, imagination, episodic future thinking, relational inference and schema-based distortions including boundary extension. The model explains how unique sensory and predictable conceptual elements of memories are stored and reconstructed by efficiently combining both hippocampal and neocortical systems, optimizing the use of limited hippocampal storage for new and unusual information. Overall, we believe hippocampal replay training generative models provides a comprehensive account of memory construction, imagination and consolidation. Spens and Burgess develop a computational model that shows how the hippocampus encodes episodic memories and replays them to train generative models of the world. Conceptual and sensory representations of experience can then be recombined for imagination and memory.
url: https://www.nature.com/articles/s41562-023-01799-z#Sec13
timestamp: 2025-01-20T15:56:50.145Z
domain: www.nature.com
path: articles_s41562-023-01799-z
---

# A generative model of memory construction and consolidation


Episodic memories are (re)constructed, share neural substrates with imagination, combine unique features with schema-based predictions and show schema-based distortions that increase with consolidation. Here we present a computational model in which hippocampal replay (from an autoassociative network) trains generative models (variational autoencoders) to (re)create sensory experiences from latent variable representations in entorhinal, medial prefrontal and anterolateral temporal cortices via the hippocampal formation. Simulations show effects of memory age and hippocampal lesions in agreement with previous models, but also provide mechanisms for semantic memory, imagination, episodic future thinking, relational inference and schema-based distortions including boundary extension. The model explains how unique sensory and predictable conceptual elements of memories are stored and reconstructed by efficiently combining both hippocampal and neocortical systems, optimizing the use of limited hippocampal storage for new and unusual information. Overall, we believe hippocampal replay training generative models provides a comprehensive account of memory construction, imagination and consolidation. Spens and Burgess develop a computational model that shows how the hippocampus encodes episodic memories and replays them to train generative models of the world. Conceptual and sensory representations of experience can then be recombined for imagination and memory.


## Content

Main
----

Episodic memory concerns autobiographical experiences in their spatiotemporal context, whereas semantic memory concerns factual knowledge[1](https://www.nature.com/articles/s41562-023-01799-z#ref-CR1 "Tulving, E. How many memory systems are there? Am. Psychol. 40, 385–398 (1985)."). The former is thought to rapidly capture multimodal experience via long-term potentiation in the hippocampus, enabling the latter to learn statistical regularities over multiple experiences in the neocortex[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 "Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970)."),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 "Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971)."),[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 "McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995)."),[5](https://www.nature.com/articles/s41562-023-01799-z#ref-CR5 "Teyler, T. J. & DiScenna, P. The hippocampal memory indexing theory. Behav. Neurosci. 100, 147–154 (1986)."). Crucially, episodic memory is thought to be constructive; recall is the (re)construction of a past experience, rather than the retrieval of a copy[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 "Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932)."),[7](https://www.nature.com/articles/s41562-023-01799-z#ref-CR7 "Schacter, D. L. Constructive memory: past and future. Dialogues Clin. Neurosci. 14, 7–18 (2012)."). But the mechanisms behind episodic (re)construction and its link to semantic memory are not well understood.

Old memories can be preserved after hippocampal damage despite amnesia for recent ones[8](https://www.nature.com/articles/s41562-023-01799-z#ref-CR8 "Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. J. Neurol. Neurosurg. Psychiatry 20, 11–21 (1957)."), suggesting that memories initially encoded in the hippocampus end up being stored in neocortical areas, an idea known as ‘systems consolidation’[9](https://www.nature.com/articles/s41562-023-01799-z#ref-CR9 "Squire, L. R. & Alvarez, P. Retrograde amnesia and memory consolidation: a neurobiological perspective. Curr. Opin. Neurobiol. 5, 169–177 (1995)."). The standard model of systems consolidation involves transfer of information from the hippocampus to the neocortex[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 "Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970)."),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 "Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971)."),[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 "McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995)."),[10](https://www.nature.com/articles/s41562-023-01799-z#ref-CR10 "Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. Proc. Natl Acad. Sci. USA 91, 7041–7045 (1994)."), whereas other views suggest that episodic and semantic information from the same events can exist in parallel[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 "Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997)."). Hippocampal ‘replay’ of patterns of neural activity during rest[12](https://www.nature.com/articles/s41562-023-01799-z#ref-CR12 "Wilson, M. A. & McNaughton, B. L. Reactivation of hippocampal ensemble memories during sleep. Science 265, 676–679 (1994)."),[13](https://www.nature.com/articles/s41562-023-01799-z#ref-CR13 "Diba, K. & Buzsáki, G. Forward and reverse hippocampal place-cell sequences during ripples. Nat. Neurosci. 10, 1241–1242 (2007).") is thought to play a role in consolidation[14](https://www.nature.com/articles/s41562-023-01799-z#ref-CR14 "Girardeau, G., Benchenane, K., Wiener, S. I., Buzsáki, G. & Zugaro, M. B. Selective suppression of hippocampal ripples impairs spatial memory. Nat. Neurosci. 12, 1222–1223 (2009)."),[15](https://www.nature.com/articles/s41562-023-01799-z#ref-CR15 "Ego-Stengel, V. & Wilson, M. A. Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat. Hippocampus 20, 1–10 (2010)."). However, consolidation does not just change which brain regions support memory traces; it also converts them into a more abstract representation, a process sometimes referred to as semanticization[16](https://www.nature.com/articles/s41562-023-01799-z#ref-CR16 "Winocur, G. & Moscovitch, M. Memory transformation and systems consolidation. J. Int. Neuropsychol. Soc. 17, 766–780 (2011)."),[17](https://www.nature.com/articles/s41562-023-01799-z#ref-CR17 "Norman, Y., Raccah, O., Liu, S., Parvizi, J. & Malach, R. Hippocampal ripples and their coordinated dialogue with the default mode network during recent and remote recollection. Neuron 109, 2767–2780 (2021).").

Generative models capture the probability distributions underlying data, enabling the generation of realistic new items by sampling from these distributions. Here we propose that consolidated memory takes the form of a generative network, trained to capture the statistical structure of stored events by learning to reproduce them (see also refs. [18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 "Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000)."),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 "Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002).")). As consolidation proceeds, the generative network supports both the recall of ‘facts’ (semantic memory) and the reconstruction of experience from these ‘facts’ (episodic memory), in conjunction with additional information from the hippocampus that becomes less necessary as training progresses.

This builds on existing models of spatial cognition in which recall and imagination of scenes involve the same neural circuits[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 "Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000)."),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 "Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007)."),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018)."), and is supported by evidence from neuropsychology that damage to the hippocampal formation (HF) leads to deficits in imagination[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 "Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007)."), episodic future thinking[24](https://www.nature.com/articles/s41562-023-01799-z#ref-CR24 "Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. Curr. Opin. Behav. Sci. 17, 41–50 (2017)."), dreaming[25](https://www.nature.com/articles/s41562-023-01799-z#ref-CR25 "Spanó, G. et al. Dreaming with hippocampal damage. Elife 9, e56211 (2020).") and daydreaming[26](https://www.nature.com/articles/s41562-023-01799-z#ref-CR26 "McCormick, C., Rosenthal, C. R., Miller, T. D. & Maguire, E. A. Mind-wandering in people with hippocampal damage. J. Neurosci. 38, 2745–2754 (2018)."), as well as by neuroimaging evidence that recall and imagination involve similar neural processes[27](https://www.nature.com/articles/s41562-023-01799-z#ref-CR27 "Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. Neuropsychologia 45, 1363–1377 (2007)."),[28](https://www.nature.com/articles/s41562-023-01799-z#ref-CR28 "Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. Trends Cogn. Sci. 11, 299–306 (2007).").

We model consolidation as the training of a generative model by an initial autoassociative encoding of memory through ‘teacher–student learning’[29](https://www.nature.com/articles/s41562-023-01799-z#ref-CR29 "Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at 
https://arxiv.org/abs/1503.02531
(2015).") during hippocampal replay (see also ref. [30](https://www.nature.com/articles/s41562-023-01799-z#ref-CR30 "Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. Nat. Neurosci. 26, 1438–1448 (2023).")). Recall after consolidation has occurred is a generative process mediated by schemas representing common structure across events, as are other forms of scene construction or imagination. Our model builds on: (1) research into the relationship between generative models and consolidation[18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 "Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000)."),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 "Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002)."), (2) the use of variational autoencoders to model the hippocampal formation[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 "Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020)."),[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 "van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020).") and (3) the view that abstract allocentric latent variables are learned from egocentric sensory representations in spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).").

More generally, we build on the idea that the memory system learns schemas which encode ‘priors’ for the reconstruction of input patterns[34](https://www.nature.com/articles/s41562-023-01799-z#ref-CR34 "Hemmer, P. & Steyvers, M. A Bayesian account of reconstructive memory. Top. Cogn. Sci. 1, 189–202 (2009)."),[35](https://www.nature.com/articles/s41562-023-01799-z#ref-CR35 "Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. Neural Comput. 34, 1841–1870 (2022)."). Unpredictable aspects of experience need to be stored in detail for further learning, while fully predicted aspects do not, consistent with the idea that memory helps to predict the future[36](https://www.nature.com/articles/s41562-023-01799-z#ref-CR36 "Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. Nat. Rev. Neurosci. 8, 657–661 (2007)."),[37](https://www.nature.com/articles/s41562-023-01799-z#ref-CR37 "Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. Trends Cogn. Sci. 24, 542–556 (2020)."),[38](https://www.nature.com/articles/s41562-023-01799-z#ref-CR38 "Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. Learn. Mem. 28, 422–434 (2021)."),[39](https://www.nature.com/articles/s41562-023-01799-z#ref-CR39 "Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. J. Neurosci. 42, 9053–9068 (2022)."). We suggest that familiar components are encoded in the autoassociative network as concepts (relying on the generative network for reconstruction), while novel components are encoded in greater sensory detail. This is efficient in terms of memory storage[40](https://www.nature.com/articles/s41562-023-01799-z#ref-CR40 "Barlow, H. B. et al. in Sensory Communication (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013)."),[41](https://www.nature.com/articles/s41562-023-01799-z#ref-CR41 "Barlow, H. B. Unsupervised learning. Neural Comput. 1, 295–311 (1989)."),[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 "Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).") and reflects the fact that consolidation can be a gradual transition, during which the autoassociative network supports aspects of memory not yet captured by the generative network. In other words, the generative network can reconstruct predictable aspects of an event from the outset on the basis of existing schemas, but as consolidation progresses, the network updates its schemas to reconstruct the event more accurately until the formerly unpredictable details stored in HF are no longer required.

Our model draws together existing ideas in machine learning to suggest an explanation for the following key features of memory, only subsets of which are captured by previous models:

1.  1.The initial encoding of memory requires only a single exposure to the event and depends on the HF, while the consolidated form of memory is acquired more gradually[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 "Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970)."),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 "Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971)."),[10](https://www.nature.com/articles/s41562-023-01799-z#ref-CR10 "Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. Proc. Natl Acad. Sci. USA 91, 7041–7045 (1994)."), as in the complementary learning systems (CLS) model[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 "McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).").
    
2.  2.The semantic content of memories becomes independent of the HF over time[43](https://www.nature.com/articles/s41562-023-01799-z#ref-CR43 "Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380 (1997)."),[44](https://www.nature.com/articles/s41562-023-01799-z#ref-CR44 "Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. Neuron 38, 127–133 (2003)."),[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 "Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015)."), consistent with CLS.
    
3.  3.Vivid, detailed episodic memory remains dependent on HF[46](https://www.nature.com/articles/s41562-023-01799-z#ref-CR46 "McKenzie, S. & Eichenbaum, H. Consolidation and reconsolidation: two lives of memories? Neuron 71, 224–233 (2011)."), consistent with multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 "Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).") (but not with CLS).
    
4.  4.Similar neural circuits are involved in recall, imagination and episodic future thinking[27](https://www.nature.com/articles/s41562-023-01799-z#ref-CR27 "Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. Neuropsychologia 45, 1363–1377 (2007)."),[28](https://www.nature.com/articles/s41562-023-01799-z#ref-CR28 "Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. Trends Cogn. Sci. 11, 299–306 (2007)."), suggesting a common mechanism for event generation, as modelled in spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).").
    
5.  5.Consolidation extracts statistical regularities from episodic memories to inform behaviour[47](https://www.nature.com/articles/s41562-023-01799-z#ref-CR47 "Durrant, S. J., Taylor, C., Cairney, S. & Lewis, P. A. Sleep-dependent consolidation of statistical learning. Neuropsychologia 49, 1322–1331 (2011)."),[48](https://www.nature.com/articles/s41562-023-01799-z#ref-CR48 "Richards, B. A. et al. Patterns across multiple memories are identified over time. Nat. Neurosci. 17, 981–986 (2014)."), and supports relational inference and generalization[49](https://www.nature.com/articles/s41562-023-01799-z#ref-CR49 "Ellenbogen, J. M., Hu, P. T., Payne, J. D., Titone, D. & Walker, M. P. Human relational memory requires time and sleep. Proc. Natl Acad. Sci. USA 104, 7723–7728 (2007)."). The Tolman–Eichenbaum machine (TEM)[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).") simulates this in the domain of multiple tasks with common transition structures (see also ref. [50](https://www.nature.com/articles/s41562-023-01799-z#ref-CR50 "Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016).")), while ref. [51](https://www.nature.com/articles/s41562-023-01799-z#ref-CR51 "Schapiro, A. C., Turk-Browne, N. B., Botvinick, M. M. & Norman, K. A. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. Phil. Trans. R. Soc. B 372, 20160049 (2017).") models how both individual examples and statistical regularities could be learned within HF.
    
6.  6.Post-consolidation episodic memories are more prone to schema-based distortions in which semantic or contextual knowledge influences recall[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 "Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932)."),[52](https://www.nature.com/articles/s41562-023-01799-z#ref-CR52 "Payne, J. D. et al. The role of sleep in false memory formation. Neurobiol. Learn. Mem. 92, 327–334 (2009)."), consistent with the behaviour of generative models[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 "Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).").
    
7.  7.Neural representations in the entorhinal cortex (EC) such as grid cells[53](https://www.nature.com/articles/s41562-023-01799-z#ref-CR53 "Hafting, T., Fyhn, M., Molden, S., Moser, M. B. & Moser, E. I. Microstructure of a spatial map in the entorhinal cortex. Nature 436, 801–806 (2005).") are thought to encode latent structures underlying experiences[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 "Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016)."), and other regions of the association cortex, such as the medial prefrontal cortex (mPFC), may compress stimuli to a minimal representation[55](https://www.nature.com/articles/s41562-023-01799-z#ref-CR55 "Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. Nat. Commun. 11, 46 (2020).").
    
8.  8.Novelty is thought to promote encoding within HF[56](https://www.nature.com/articles/s41562-023-01799-z#ref-CR56 "Hasselmo, M. E., Wyble, B. P. & Wallenstein, G. V. Encoding and retrieval of episodic memories: role of cholinergic and GABAergic modulation in the hippocampus. Hippocampus 6, 693–708 (1996)."), while more predictable events consistent with existing schemas are consolidated more rapidly[57](https://www.nature.com/articles/s41562-023-01799-z#ref-CR57 "Tse, D. et al. Schemas and memory consolidation. Science 316, 76–82 (2007)."). Activity in the hippocampus can reflect prediction error or mismatch novelty[58](https://www.nature.com/articles/s41562-023-01799-z#ref-CR58 "Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. PLoS Biol. 4, e424 (2006)."),[59](https://www.nature.com/articles/s41562-023-01799-z#ref-CR59 "Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. Learn. Mem. 18, 523–528 (2011)."), and novelty is thought to affect the degree of compression of representations in memory[60](https://www.nature.com/articles/s41562-023-01799-z#ref-CR60 "Hedayati, S., O’Donnell, R. E. & Wyble, B. A model of working memory for latent representations. Nat. Hum. Behav. 6, 709–719 (2022).") to make efficient use of limited HF capacity[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 "Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).").
    
9.  9.Memory traces in the hippocampus appear to involve a mixture of sensory and conceptual features, with the latter encoded by concept cells[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 "Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012)."), potentially bound together by episode-specific neurons[62](https://www.nature.com/articles/s41562-023-01799-z#ref-CR62 "Kolibius, L. D. et al. Hippocampal neurons code individual episodic memories in humans. Nat. Hum. Behav. 7, 1968–1979 (2023)."). Few models explore how this could happen.
    

### Consolidation as the training of a generative model

Our model simulates how the initial representation of memories can be used to train a generative network, which learns to reconstruct memories by capturing the statistical structure of experienced events (or ‘schemas’). First, the hippocampus rapidly encodes an event; then, generative networks gradually take over after being trained on replayed representations from the hippocampus. This makes the memory more abstracted, more supportive of generalization and relational inference, but also more prone to gist-based distortion. The generative networks can be used to reconstruct (for memory) or construct (for imagination) sensory experience, or to support semantic memory and relational inference directly from their latent variable representations (see Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).

**Fig. 1: Architecture of the basic model.**

[![Image 26: figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig1_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/1)

**a**, First, the hippocampus rapidly encodes an event, modelled as one-shot memorization in an autoassociative network (an MHN). Then, generative networks are trained on replayed representations from the autoassociative network, learning to reconstruct memories by capturing the statistical structure of experienced events. **b**, A more detailed schematic of the generative network to indicate the multiple layers of, and overlap between, the encoder and decoder (where layers closer to the sensory neocortex overlap more). The generation of a sensory experience, for example, visual imagery, requires the decoder to the sensory neocortex via HF. **c**, Random noise inputs to the MHN (top row) reactivate its memories (bottom row) after 10,000 items from the Shapes3D dataset are encoded, with five examples shown. **d**, The generative model (a variational autoencoder) can recall images (bottom row) from a partial input (top row), following training on 10,000 replayed memories sampled from the MHN. **e**, Episodic memory after consolidation: a partial input is mapped to latent variables whose return projections to the sensory neocortex via HF then decode these back into a sensory experience. **f**, Imagination: latent variables are decoded into an experience via HF and return projections to the neocortex. **g**, Semantic memory: a partial input is mapped to latent variables, which capture the ‘key facts’ of the scene. The bottom rows of **e**–**g** illustrate these functions in a model that has encoded the Shapes3D dataset into latent variables (_v_1, _v_2, _v_3, …, _v__n_). Diagrams were created using [BioRender.com](http://biorender.com/).

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/1)

Before consolidation, the hippocampal autoassociative network encodes the memory. A modern Hopfield network (MHN)[63](https://www.nature.com/articles/s41562-023-01799-z#ref-CR63 "Ramsauer, H. et al. Hopfield networks is all you need. in International Conference on Learning Representations (2021).") is used, which can be interpreted such that the feature units activated by an event are bound together by a memory unit[64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 "Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).") (see [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14) and [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1)). Teacher–student learning[29](https://www.nature.com/articles/s41562-023-01799-z#ref-CR29 "Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at 
https://arxiv.org/abs/1503.02531
(2015).") allows transfer of memories from one neural network to another during consolidation[30](https://www.nature.com/articles/s41562-023-01799-z#ref-CR30 "Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. Nat. Neurosci. 26, 1438–1448 (2023)."). Accordingly, we use outputs from the autoassociative network to train the generative network: random inputs to the hippocampus result in the reactivation of memories, and this reactivation results in consolidation. After consolidation, generative networks encode the information contained in memories. Reliance on the generative networks increases over time as they learn to reconstruct a particular event.

Specifically, the generative networks are implemented as variational autoencoders (VAEs), which are autoencoders with special properties such that the most compressed layer represents a set of latent variables, which can be sampled from to generate realistic new examples corresponding to the training dataset[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 "Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at 
https://arxiv.org/abs/1312.6114
(2013)."),[66](https://www.nature.com/articles/s41562-023-01799-z#ref-CR66 "Kingma, D. P. & Welling, M. An introduction to variational autoencoders. Found. Trends Mach. Learn. 12, 307–392 (2019)."). Latent variables can be thought of as hidden factors behind the observed data, and directions in the latent space can correspond to meaningful transformations (see [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14)). The VAE’s encoder ‘encodes’ sensory experience as latent variables, while its decoder ‘decodes’ latent variables back to sensory experience. In psychological terms, after training on a class of stimuli, VAEs can reconstruct such stimuli from a partial input according to the schema for that class, and generate novel stimuli consistent with the schema. (Our use of VAEs is illustrative, and we would expect a range of other generative latent variable models, such as predictive coding networks[67](https://www.nature.com/articles/s41562-023-01799-z#ref-CR67 "Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. Neural Comput. 7, 889–904 (1995)."),[68](https://www.nature.com/articles/s41562-023-01799-z#ref-CR68 "Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)."),[69](https://www.nature.com/articles/s41562-023-01799-z#ref-CR69 "Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)."), to show similar behaviour.) See [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14) and [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for further details.

Generative networks capture the probability distributions underlying events, or ‘schemas’. In other words, here ‘schemas’ are rules or priors (expected probability distributions) for reconstructing a certain type of stimulus (for example, the schema for an office predicts the presence of co-occurring objects such as desks and chairs, facilitating episode generation), whereas concepts represent categories but not necessarily how to reconstruct them. However, schemas and concepts are closely related, and their meanings can overlap, with conflicting definitions in the psychology literature[70](https://www.nature.com/articles/s41562-023-01799-z#ref-CR70 "Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. Trends Cogn. Sci. 21, 618–631 (2017)."),[71](https://www.nature.com/articles/s41562-023-01799-z#ref-CR71 "Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. Neuropsychologia 53, 104–114 (2014).").

During perception, the generative model provides an ongoing estimate of novelty from its reconstruction error (also known as ‘prediction error’, the difference between input and output representations). Aspects of an event that are consistent with previous experience (that is, with low reconstruction error) do not need to be encoded in detail in the autoassociative ‘teacher’ network[36](https://www.nature.com/articles/s41562-023-01799-z#ref-CR36 "Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. Nat. Rev. Neurosci. 8, 657–661 (2007)."),[37](https://www.nature.com/articles/s41562-023-01799-z#ref-CR37 "Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. Trends Cogn. Sci. 24, 542–556 (2020)."),[38](https://www.nature.com/articles/s41562-023-01799-z#ref-CR38 "Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. Learn. Mem. 28, 422–434 (2021)."),[39](https://www.nature.com/articles/s41562-023-01799-z#ref-CR39 "Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. J. Neurosci. 42, 9053–9068 (2022)."). Once the generative network’s reconstruction error is sufficiently low, the hippocampal trace is unnecessary, freeing up capacity for new encodings. However, we have not simulated decay, deletion or capacity constraints in the autoassociative memory part of the model.

### Combining conceptual and sensory features in episodic memory

Consolidation is often considered in terms of fine-grained sensory representations updating coarse-grained conceptual representations, for example, the sight of a particular dog updating the concept of a dog. Modelling hippocampal representations as sensory-like is a reasonable simplification, which we make in simulations of the ‘basic’ model in Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1). However, memories probably bind together representations along a spectrum from coarse-grained and conceptual to fine-grained and sensory. For example, the hippocampal encoding of a day at the beach is likely to bind together coarse-grained concepts such as ‘beach’ and ‘sea’ along with sensory representations such as the melody of an unfamiliar song or the sight of a particular sandcastle, consistent with the evidence for concept cells in the hippocampus[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 "Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012)."). (This also fits with the observation that ambiguous images ‘flip’ between interpretations in perception but are stable when held in memory[72](https://www.nature.com/articles/s41562-023-01799-z#ref-CR72 "Chambers, D. & Reisberg, D. Can mental images be ambiguous? J. Exp. Psychol. Hum. Percept. Perform. 11, 317–328 (1985)."), reflecting how the conceptual content of memories constrains recall.)

Furthermore, encoding every sensory detail in the hippocampus would be inefficient (elements already predicted by conceptual representations being redundant); an efficient system should take advantage of shared structure across memories to encode only what is necessary[40](https://www.nature.com/articles/s41562-023-01799-z#ref-CR40 "Barlow, H. B. et al. in Sensory Communication (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013)."),[41](https://www.nature.com/articles/s41562-023-01799-z#ref-CR41 "Barlow, H. B. Unsupervised learning. Neural Comput. 1, 295–311 (1989)."). Accordingly, we suggest that predictable elements are encoded as conceptual features linked to the generative latent variable representation, while unpredictable elements are encoded in a more detailed and veridical form as sensory features.

Suppose someone sees an unfamiliar animal in the forest (Fig. [2b](https://www.nature.com/articles/s41562-023-01799-z#Fig2)). Much of the event might be consistent with an existing forest schema, but the unfamiliar animal would be novel. In the extended model (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2) and section ‘Combining conceptual and unpredictable sensory features’), the reconstruction error per element of the experience is calculated by the generative model during perception, and elements with high reconstruction error are encoded in the autoassociative network as sensory features, along with conceptual features linked to the generative model’s latent variable representation. In other words, each pattern is split into a predictable component (approximating the generative network’s prediction for the pattern), plus an unpredictable component (elements with high prediction error). This produces a sparser vector than storing every element in detail, increasing the capacity of the network[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 "Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).").

**Fig. 2: Architecture of the extended model.**

[![Image 27: figure 2](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig2_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/2)

**a**, Each scene is initially encoded as a combination of predictable conceptual features related to the latent variables of the generative network and unpredictable sensory features that were poorly predicted by the generative network. An MHN (in red) encodes both sensory and conceptual features (with connections to the sensory neocortex and latent variables in EC, respectively), binding them together via memory units. Memories may eventually be learned by the generative model (in blue), but consolidation can be a prolonged process, during which time the generative network provides schemas for reconstruction and the autoassociative network supports new or detailed information not yet captured by these schemas. Multiple generative networks can be trained concurrently, with different networks optimized for different tasks. This includes networks with latent variables in EC, mPFC and alTL, each with its own semantic projections. However, in all cases, return projections to the sensory neocortex are via HF. **b**, An illustration of encoding in the extended model. **c**, Encoding ‘scenes’ from the Shapes3D dataset, with each ‘scene’ decomposed into unpredicted sensory features (top) and conceptual features linked to the generative network’s latent variables (bottom). Novel features (white squares overlaid on the image with varying opacity) are added to each ‘scene’. **d**, Recalling ‘scenes’ (with novel features) from the Shapes3D dataset. First, the input is decomposed; then, the MHN performs pattern completion on both sensory and conceptual features. The conceptual features (which in these simulations are simply the generative network’s latent variables) are then decoded into a schema-based prediction, onto which any stored sensory features are overwritten. Diagrams were created using [BioRender.com](http://biorender.com/).

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/2)

### Neural substrates of the model

Which brain regions do the components of this model represent? The autoassociative network involves the hippocampus binding together the constituents of a memory in the neocortex, whereas the generative network involves neocortical inputs projecting to latent variable representations in the higher association cortex, which then project back to the neocortex via the HF. The entorhinal (EC), medial prefrontal cortex (mPFC) and anterolateral temporal lobe (alTL) are all prime candidates for the site of latent variable representations.

First, the EC is the main route between the hippocampus and the neocortex, and is where grid cells, which are thought to be a latent variable representation of spatial or relational structure[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 "Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016)."), are most often observed[73](https://www.nature.com/articles/s41562-023-01799-z#ref-CR73 "Moser, E. I., Kropff, E. & Moser, M. B. Place cells, grid cells, and the brain’s spatial representation system. Annu. Rev. of Neurosci. 31, 69–89 (2008)."). Second, mPFC and its connections to HF play a crucial role in episodic memory processing[70](https://www.nature.com/articles/s41562-023-01799-z#ref-CR70 "Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. Trends Cogn. Sci. 21, 618–631 (2017)."),[74](https://www.nature.com/articles/s41562-023-01799-z#ref-CR74 "Takashima, A. et al. Declarative memory consolidation in humans: a prospective functional magnetic resonance imaging study. Proc. Natl Acad. Sci. USA 103, 756–761 (2006)."),[75](https://www.nature.com/articles/s41562-023-01799-z#ref-CR75 "Gais, S. et al. Sleep transforms the cerebral trace of declarative memories. Proc. Natl Acad. Sci. USA 104, 18778–18783 (2007)."),[76](https://www.nature.com/articles/s41562-023-01799-z#ref-CR76 "Frankland, P. W. & Bontempi, B. The organization of recent and remote memories. Nat. Rev. Neurosci. 6, 119–130 (2005)."),[77](https://www.nature.com/articles/s41562-023-01799-z#ref-CR77 "van Kesteren, M. T. R., Fernández, G., Norris, D. G. & Hermans, E. J. Persistent schema-dependent hippocampal-neocortical connectivity during memory encoding and postencoding rest in humans. Proc. Natl Acad. Sci. USA 107, 7550–7555 (2010)."),[78](https://www.nature.com/articles/s41562-023-01799-z#ref-CR78 "Benchenane, K. et al. Coherent theta oscillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning. Neuron 66, 921–936 (2010)."), are thought to encode schemas[57](https://www.nature.com/articles/s41562-023-01799-z#ref-CR57 "Tse, D. et al. Schemas and memory consolidation. Science 316, 76–82 (2007)."),[71](https://www.nature.com/articles/s41562-023-01799-z#ref-CR71 "Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. Neuropsychologia 53, 104–114 (2014)."), are implicated in transitive inference[79](https://www.nature.com/articles/s41562-023-01799-z#ref-CR79 "Koscik, T. R. & Tranel, D. The human ventromedial prefrontal cortex is critical for transitive inference. J. Cogn. Neurosci. 24, 1191–1204 (2012).") and the integration of memories[80](https://www.nature.com/articles/s41562-023-01799-z#ref-CR80 "Spalding, K. N. et al. Ventromedial prefrontal cortex is necessary for normal associative inference and memory integration. J. Neurosci. 38, 3767–3775 (2018)."), and perform dimensionality reduction by compressing irrelevant features[55](https://www.nature.com/articles/s41562-023-01799-z#ref-CR55 "Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. Nat. Commun. 11, 46 (2020)."). Third, the anterior and lateral temporal cortices associated with semantic memory[81](https://www.nature.com/articles/s41562-023-01799-z#ref-CR81 "Chan, D. et al. Patterns of temporal lobe atrophy in semantic dementia and Alzheimer’s disease. Ann. Neurol. 49, 433–442 (2001).") and retrograde amnesia[82](https://www.nature.com/articles/s41562-023-01799-z#ref-CR82 "Bright, P. et al. Retrograde amnesia in patients with hippocampal, medial temporal, temporal lobe, or frontal pathology. Learn. Mem. 13, 545–557 (2006).") probably contain latent variable representations capturing semantic structure. This might correspond to the ‘anterior temporal network’ associated with semantic dementia[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 "Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012)."), while the first network (between sensory and entorhinal cortices) might correspond to the ‘posterior medial network’[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 "Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012)."), and to the network mapping between visual scenes and allocentric spatial representations[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 "Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000)."),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 "Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007)."),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).").

Which regions constitute the generative network’s decoder? The decoder converts latent variable representations in the higher association cortex back to sensory neocortical representations via HF. Patients with damage to the hippocampus proper but not the EC can generate simple scenes (or fragments thereof), but an intact hippocampus is required for more coherent imagery of complex ones[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 "Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007)."). We hypothesize that conceptual units in the hippocampus proper help to generate complex, conceptually coherent scenes (perhaps through a recurrent ‘clean up’ mechanism), but that an intact EC and its return pathway to the sensory neocortex (the ventral visual stream for images) can still decode representations to some extent in their absence.

Multiple generative networks can be trained concurrently from a single autoassociative network through consolidation, with different networks optimized for different tasks. In other words, multiple networks could update their parameters to minimize prediction error on the basis of the same replayed memories. This could consist of a primary VAE with latent variables in the EC, plus additional parallel pathways from the higher sensory cortex to the EC via latent variables in the mPFC or the alTL. (Computationally, the shared connections could be fixed as the alternative pathways are trained.) Note that in all cases, return projections to the sensory neocortex via HF are required to decode latent variables into sensory experiences.

Results
-------

### Modelling encoding and recall

Each new event is encoded as an autoassociative trace in the hippocampus, modelled as an MHN. Two properties of this network are particularly important: memorization occurs with only one exposure, and random inputs to the network retrieve stored memories sampled from the whole set of memories (modelling replay).

We model recall as (re)constructing a scene from a partial input. First, we simulate encoding and replay in the autoassociative network. The network memorizes a set of scenes, representing events, as described above. When the network is given a partial input, it retrieves the closest stored memory. Even when the network is given random noise, it retrieves stored memories (see Fig. [1c](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). Second, we simulate recall in the generative network trained on reactivated memories from the autoassociative network, which is able to reconstruct the original image when presented with a partial version of an item from the training data (Fig. [1d](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).

In the basic model (Fig. [1a](https://www.nature.com/articles/s41562-023-01799-z#Fig1)), the prediction error could be calculated for each event so that only the unpredictable events are stored in the hippocampus, as the predictable ones can already be retrieved by the generative network (however, this is not simulated explicitly). In the extended model (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2) and section ‘Combining conceptual and unpredictable sensory features’), prediction error is calculated for each element of an event, determining which sensory details are stored.

### Modelling semantic memory

Existing semantic memory survives when the hippocampus is lesioned[43](https://www.nature.com/articles/s41562-023-01799-z#ref-CR43 "Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380 (1997)."),[44](https://www.nature.com/articles/s41562-023-01799-z#ref-CR44 "Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. Neuron 38, 127–133 (2003)."),[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 "Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015)."), and hippocampal amnesics can describe remote memories more successfully than recent ones[8](https://www.nature.com/articles/s41562-023-01799-z#ref-CR8 "Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. J. Neurol. Neurosurg. Psychiatry 20, 11–21 (1957)."),[84](https://www.nature.com/articles/s41562-023-01799-z#ref-CR84 "Spiers, H. J., Maguire, E. A. & Burgess, N. Hippocampal amnesia. Neurocase 7, 357–382 (2001)."), even if they might not recall them ‘episodically’[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 "Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997)."). This temporal gradient indicates that the semantic component of memories becomes HF-independent. In the model, EC lesions impair all truly episodic recollection since the return projections from the HF are required for the generation of sensory experiences. Here we describe how remote memories could be retrieved ‘in semantic form’ despite lesions including the hippocampus and the EC.

The latent variable representation of an event in the generative network encodes the key facts about the event and can drive semantic memory directly without decoding the representation back into a sensory experience (Fig. [1g](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). The output route via HF is necessary for turning latent variable representations in mPFC or alTL into a sensory experience, but the latent variables themselves could support semantic retrieval. Thus, when the HF (including the EC) is removed, the model can still support retrieval of semantic information (see section ‘Modelling brain damage’ for details). To show this, we trained models to predict attributes of each image from its latent vector. Figure [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows that semantic ‘decoding accuracy’ increases as training progresses, reflecting the learning of semantic structure as a by-product of learning to reconstruct the sensory input patterns (_r_s(48) = 0.997, _P_ < 0.001, 95% confidence interval (CI) = 0.987, 1.000). While semantic memory is much more complex than simple classification, richer ‘semantic’ outputs such as verbal descriptions can also be decoded from latent variable representations of images[85](https://www.nature.com/articles/s41562-023-01799-z#ref-CR85 "Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in Proc. IEEE Conference on Computer Vision and Pattern Recognition 3156–3164 (2015)."),[86](https://www.nature.com/articles/s41562-023-01799-z#ref-CR86 "Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at 
https://arxiv.org/abs/2111.09734
(2021).").

**Fig. 3: Learning, relational inference and imagination in the generative model.**

[![Image 28: figure 3](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig3_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/3)

**a**, Reconstruction error (red) and decoding accuracy (blue) improve during training of the generative model. Decoding accuracy refers to the performance of a support vector classifier trained to output the central object’s shape from the latent variables, using 200 examples at the end of each epoch of generative model training. An epoch is one presentation of the training set of 10,000 samples from the hippocampus. **b**, Relational inference as vector arithmetic in the latent space. The three items on the right of each equation are items from the training data. Their latent variable representations are combined as vectors according to the equation, giving the latent variable representation from which the first item is generated. The pair in brackets describes a relation which is applied to the second item to produce the first. In the top row, the object shape changes from a cylinder to a sphere. In the second, the object shape changes from a cylinder to a cube, and the object colour from red to blue. In the third and fourth, more complex transitions change the object colour and shape, wall colour and angle. **c**, Imagining new items via interpolation in latent space. Each row shows points along a line in the latent space between two items from the training data, decoded into images by the generative network’s decoder. **d**, Imagining new items from a category. Samples from each of the shape categories of the support vector classifier in **a** are shown.

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/3)

### Imagination, episodic future thinking and relational inference

Here we model the generation of events that have not been experienced from the generative network’s latent variables. Events can be generated either by external specification of latent variables (imagination) or by transforming the latent variable representations of specific events (relational inference). The former is simulated by sampling from categories in the latent space then decoding the results (Fig. [3d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). The latter is simulated by interpolating between the latent representations of events (Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) or by doing vector arithmetic in the latent space (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). This illustrates that the model has learnt some conceptual structure to the data, supporting reasoning tasks of the form ‘what is to _A_ as _B_ is to _C_?’, and provides a model for the flexible recombination of memories thought to underlie episodic future thinking[24](https://www.nature.com/articles/s41562-023-01799-z#ref-CR24 "Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. Curr. Opin. Behav. Sci. 17, 41–50 (2017).").

### Modelling schema-based distortions

The schema-based distortions observed in human episodic memory increase over time[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 "Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932).") and with sleep[52](https://www.nature.com/articles/s41562-023-01799-z#ref-CR52 "Payne, J. D. et al. The role of sleep in false memory formation. Neurobiol. Learn. Mem. 92, 327–334 (2009)."), suggesting an association with consolidation. Recall by the generative network distorts memories towards prototypical representations. Figure [4a–d](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows that handwritten digits from the MNIST dataset[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 "LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).") ‘recalled’ by a VAE become more prototypical (MNIST is used for this because each image has a single category). Recalled pairs from the same class become more similar, that is, intra-class variation decreases (paired samples _t_\-test _t_(7,839) = 60.523, _P_ < 0.001, Cohen’s _d_ = −0.684, 95% CI = 0.021, 0.022). The pixel space of MNIST digits before and after recall and the latent space of their encodings also show this effect. In summary, recall with a generative network distorts stimuli towards more prototypical representations even when no class information is given during training. As reliance on the generative model increases, so does the level of distortion.

**Fig. 4: Generative network shows schema-based distortions.**

[![Image 29: figure 4](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig4_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/4)

**a**, MNIST digits (top) and the VAE’s output for each (bottom). Recalled pairs from the same class become more similar. A total of 10,000 items from the MNIST dataset were encoded in the MHN, and 10,000 replayed samples were used to train the VAE. **b**, The variation within each MNIST class is smaller for the recalled items than for the original inputs. For each of the 10 classes, the variance per pixel is calculated across 500 images, and the 784 pixel variances are then plotted for each class before and after recall. In each boxplot, the box gives the interquartile range, its central line gives the median, and its whiskers extend to the 10th and 90th percentiles of the data. **c**,**d**, The pixel spaces of MNIST digits (bottom row) and the latent space of their encodings (top row) show more compact clusters for the generative network’s outputs (**d**) than for its inputs (**c**). Pixel and latent spaces are shown projected into 2D with UMAP[146](https://www.nature.com/articles/s41562-023-01799-z#ref-CR146 "McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at 
https://arxiv.org/abs/1802.03426
(2018).") and colour-coded by class. **e**, Examples of boundary extension and contraction. Top row: the noisy input images (from a held-out test set), with an atypically ‘zoomed out’ or ‘zoomed in’ view (by 80% and 120% on the left and right, respectively) for three original images. Bottom row: the predicted images for each input image, which are distorted towards the ‘typical view’ in each case. **f**, Adapted figure from ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 "Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. 
https://doi.org/10.1167/jov.21.9.2124
(2021)."), showing the distribution of boundary extension vs contraction as a function of the viewpoint of an image. Specifically, the values are the average of ‘closer’ vs ‘further’ judgements, assigned −1 and 1, respectively, of an identical stimulus image in comparison with the remembered image (with 900 trials per position). Error bars give the standard error of the mean. Example stimuli are shown at the bottom. **g**, In our model, the VAE increases the estimated size of the central object in atypically ‘zoomed out’ views compared with the training data, and decreases it in atypically ‘zoomed in’ views, as in ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 "Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. 
https://doi.org/10.1167/jov.21.9.2124
(2021)."). Two hundred images are used at each ‘zoom level’. See **b** for a description of boxplot elements.

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/4)

Boundary extension and contraction exemplify this phenomenon. Boundary extension is the tendency to remember a wider field of view than was observed[88](https://www.nature.com/articles/s41562-023-01799-z#ref-CR88 "Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. J. Exp. Psychol. Learn. Mem. Cogn. 15, 179–187 (1989)."), while boundary contraction is the opposite[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020)."). Unusually close-up views appear to cause boundary extension, and unusually far away ones boundary contraction[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020)."), although this is debated[90](https://www.nature.com/articles/s41562-023-01799-z#ref-CR90 "Intraub, H. Searching for boundary extension. Curr. Biol. 30, R1463–R1464 (2020)."),[91](https://www.nature.com/articles/s41562-023-01799-z#ref-CR91 "Bainbridge, W. A. & Baker, C. I. Reply to Intraub. Curr. Biol. 30, R1465–R1466 (2020)."). We modelled this by giving the generative network a range of new scenes that were artificially ‘zoomed in’ or ‘zoomed out’ compared with those in its training set; its reconstructions are distorted towards the ‘typical view’ (Fig. [4e](https://www.nature.com/articles/s41562-023-01799-z#Fig4)), as in human data. Figure [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows the change in the object size in memory quantitatively, mirroring the findings in ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 "Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. 
https://doi.org/10.1167/jov.21.9.2124
(2021).") (Fig. [4f](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). (Note that the measure of boundary extension vs contraction used by ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 "Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. 
https://doi.org/10.1167/jov.21.9.2124
(2021).") is produced by averaging ‘closer’ vs ‘further’ judgements of an identical stimulus image in comparison with the remembered image, rather than the drawing-based measure we use, but the two measures are significantly correlated[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).").)

### Combining conceptual and unpredictable sensory features

In the extended model, memories stored in the hippocampal autoassociative network combine conceptual features (derived from the generative network’s latent variables) and unpredictable sensory features (those with a high reconstruction error during encoding) (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2)). In these simulations, the conceptual features are simply a one-to-one copy of latent variable representations. (Since latent variable representations are not stable as the generative network learns, concepts derived from latent variables seem more likely to be stored than the latent variables themselves, so this is a simplification; see section ‘Extended model’ for further details.)

Figure [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5) shows the stages of recall in the extended model after encoding with a lower or higher prediction error threshold. After decomposing the input into its predictable (conceptual) and unpredictable (sensory) features, the autoassociative network performs pattern completion on the combined representation. The prototypical (that is, predicted) image corresponding to the retrieved conceptual features must then be obtained by decoding the associated latent variable representation into an experience via the return projections to the sensory neocortex. Next, the predictable and unpredictable elements are recombined, simply by overwriting the prototypical prediction with any unpredictable elements, via the connections from the sensory features to the sensory neocortex. The extended model is therefore able to exploit the generative network to reconstruct the predictable aspects of the event from its latent variables, storing only those sensory details that were poorly predicted in the autoassociative network. Equally, as the generative network improves, sensory features stored in the hippocampus may no longer differ significantly from the initial schematic reconstruction in the sensory neocortex, signalling that the hippocampal representation is no longer needed.

**Fig. 5: Retrieval dependence on reconstruction error threshold and replay in the extended model.**

[![Image 30: figure 5](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig5_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/5)

**a**, The stages of recall are shown from left to right (see Fig. [2d](https://www.nature.com/articles/s41562-023-01799-z#Fig2)), where each row represents an example scene. Each scene consists of a standard Shapes3D image with the addition of novel features (several white squares overlaid on the image with varying opacity). **b**, Repeating this process with a higher error threshold for encoding (with the same events and partial inputs) means fewer poorly predicted sensory features are stored in the autoassociative MHN, leading to more prototypical recall with increased reconstruction error. **c**, Average reconstruction error and number of sensory features (that is, pixels) stored in the autoassociative MHN against the error threshold for encoding. One hundred images are tested and error bars give the s.e.m. **d**, Replay in the extended model. The autoassociative network retrieves memories when random noise is given as input, as shown for three example inputs (upper row). As above, the square images show the poorly predicted sensory features and the rectangles below these display the latent variable representations (lower row).

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/5)

### Schema-based distortions in the extended model

The schema-based distortions shown in the basic model result from the generative network and increase with dependence on it, but memory distortions can also have a rapid onset[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995)."). In the extended model, even immediate recall involves a combination of conceptual and sensory features, and the presence of conceptual features induces distortions before consolidation of that specific memory.

In general, recall is biased towards the ‘mean’ of the class soon after encoding due to the influence of the conceptual representations (Fig. [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5)). This is more pronounced when the error threshold for encoding is high, as there is more reliance on the ‘prototypical’ representations, resulting in the recall of fewer novel features. At a lower error threshold, more sensory detail is encoded, that is, the dimension of the memory trace is higher (_r_s(3) = −1, _P_ < 0.001). This results in a lower reconstruction error (_r_s(3) = 1, _P_ < 0.001), indicating lower distortion but at the expense of efficiency.

External context further distorts memory. Reference [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 "Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).") asked participants to reproduce ambiguous sketches. A context was established by telling the participants that they would see images from a certain category. After a delay, drawings from memory were distorted to look more like members of the context category. Figure [6b](https://www.nature.com/articles/s41562-023-01799-z#Fig6) shows the result of encoding the same ambiguous image with two different externally provided concepts (a cube in the top row, a sphere in the bottom row), represented by the latent variables for each concept, as opposed to the latent variables predicted by the image itself as in Fig. [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5). During recall, the encoded concept is retrieved in the autoassociative network, determining the prototypical scene reconstructed by the generative network. This biases recall towards the class provided as context, mirroring Fig. [6a](https://www.nature.com/articles/s41562-023-01799-z#Fig6).

**Fig. 6: Schema-based distortions: effects of conceptual context in the extended model.**

[![Image 31: figure 6](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig6_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/6)

**a**, Adapted figure from ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 "Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).") showing that recall of an ambiguous item (stimulus figure, centre) depends on its context at encoding (word from list 1, left; or list 2, right), as shown by drawing from memory (reproduced figure, far left and far right). **b**, Memory distortions in the extended model, when the original scene (containing an ambiguous blurred shape) is encoded with a given concept (cube, top; sphere, bottom), represented by the latent variables for that class. Then, a partial input is processed by the generative network to produce predicted conceptual features and the sensory features not predicted by the prototype for that concept (in this case, a white square) for input to the autoassociative MHN. However, pattern completion in the MHN reproduces the originally encoded sensory and conceptual features (cube, top; sphere, bottom), and these are recombined to produce the final output, which is distorted towards the encoded conceptual context.

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/6)

We also simulate the Deese–Roediger–McDermott (DRM) task[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).") in the extended model to demonstrate its applicability to non-image stimuli. In the DRM task, participants are shown lists of words that are semantically related to ‘lure words’ not present in the list; there is a robust finding that false recognition and recall of the lure words occur[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995)."). In the extended model, gist-based semantic intrusions arise as a consequence of learning the co-occurrence statistics of words. First, the VAE is trained to reconstruct the sets of words in simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 "Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016).") converted to vectors of word counts, representing background knowledge. The system then encodes the experimental lists as the combination of an ‘id\_n’ term capturing unique spatiotemporal context, and the VAE’s latent representation of each word list (respectively analogous to the stimulus-unique pixels and the VAE’s latent representation of each image in Fig. [5](https://www.nature.com/articles/s41562-023-01799-z#Fig5)). As in the human data, lure words are often but not always recalled when the system is presented with ‘id\_n’ (Fig. [7a](https://www.nature.com/articles/s41562-023-01799-z#Fig7)), since the latent variable representations that generate the words in the list also tend to generate the lure word. The system also forgets some words and produces additional semantic intrusions. In addition, the chance of recalling the lure word is higher for longer lists, as in human data from ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 "Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997)."), as more related words provide a stronger ‘prior’ for the lure (Fig. [7b](https://www.nature.com/articles/s41562-023-01799-z#Fig7)) (_r_s(10) = 0.998, _P_ < 0.001, 95% CI = 0.982, 1.000).

**Fig. 7: Modelling the DRM task.**

[![Image 32: figure 7](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig7_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/7)

**a**, First, the VAE is trained to reconstruct simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 "Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016).") converted to vectors of word counts, representing background knowledge. The system then encodes the lists as the combination of an ‘id\_n’ term capturing unique spatiotemporal context, and the VAE’s latent variable representation of the word list. In each plot, recalled stimuli when the system is presented with ‘id\_n’ are shown, with output scores treated as probabilities so that words with a score \>0.5 (above dashed lines) are recalled. Words from the stimulus list are shown in blue, and lures in red. See Fig. 1 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for results for the remaining DRM lists. **b**, The chance of recalling the lure word is higher when longer lists are encoded (blue). Each measurement is averaged across 400 trials (20 random subsets of each of the 20 DRM lists), and error bars give the s.e.m. This qualitatively resembles human data from ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 "Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997).") (grey).

[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/7)

### Modelling brain damage

Recent episodic memory is impaired following damage to the HF, whereas semantic memory, including the semantic content of remote episodes, appears relatively spared. In the model, the semantic form of a consolidated memory survives damage to the HF due to latent variable representations in the mPFC or the alTL (even if those in the EC are lesioned); Fig. [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3) demonstrates how semantic recall performance improves with the age of a memory, reflecting the temporal gradient of retrograde amnesia (see section ‘Modelling semantic memory’). However, these semantic ‘facts’ cannot be used to generate an experience ‘episodically’ without the generative network’s decoder, in agreement with multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 "Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).").

The extent of retrograde amnesia can vary greatly depending in part on which regions of the HF are damaged[98](https://www.nature.com/articles/s41562-023-01799-z#ref-CR98 "Cipolotti, L. et al. Long-term retrograde amnesia… the crucial role of the hippocampus. Neuropsychologia 39, 151–172 (2001)."),[99](https://www.nature.com/articles/s41562-023-01799-z#ref-CR99 "Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. J. Neurosci. 6, 2950–2967 (1986)."). The dissociation of retrograde and anterograde amnesia in some cases suggests that the circuits for encoding memories and the circuits for recalling them via the HF only overlap partially[99](https://www.nature.com/articles/s41562-023-01799-z#ref-CR99 "Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. J. Neurosci. 6, 2950–2967 (1986)."). For example, if the autoassociative network is damaged but not the generative network’s decoder, the generative network can still perform reconstruction of fully consolidated memories. This could explain varying reports of the gradient of retrograde amnesia when assessing episodic recollection (as opposed to semantic memory), if the generative network’s decoder is intact in patients showing spared episodic recollection of early memories[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 "Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015)."). Note that the location of damage within the generative network’s decoder also affects the resulting deficit in our model. In particular, patients with damage restricted to the hippocampus proper can (re)construct simple scenes but not more complex ones[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 "Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007).").

Our model also shows the characteristic anterograde amnesia after hippocampal damage, as the hippocampus is required to initially bind features together and support off-line training of the generative model. Anterograde semantic learning would also be impaired by hippocampal damage (as the generative network is trained by hippocampal replay). While hippocampal replay need not be the only mechanism for schema acquisition, it would probably be much slower without the benefit of replay. However, semantic learning over short timescales may be relatively unimpaired, as it is less dependent on extracting regularities from long-term memory[100](https://www.nature.com/articles/s41562-023-01799-z#ref-CR100 "Knowlton, B. J., Squire, L. R. & Gluck, M. A. Probabilistic classification learning in amnesia. Learn. Mem. 1, 106–120 (1994).").

In semantic dementia, semantic memory is impaired, and remote episodic memory is impaired more than recent episodic memory[101](https://www.nature.com/articles/s41562-023-01799-z#ref-CR101 "Hodges, J. R. & Graham, K. S. Episodic memory: insights from semantic dementia. Phil. Trans. R. Soc. Lond. B 356, 1423–1434 (2001)."). This would be consistent with lesions to the generative network, as recent memories can rely more on the hippocampal autoassociative network. However, the exact effects would depend on the distribution of damage across the various potential generative networks in the EC, mPFC and alTL. Of these, the alTL network is associated with semantic dementia, and the posterior medial network (corresponding to the generative network between the sensory areas and the EC) with Alzheimer’s disease[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 "Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012).").

Finally, neuropsychological evidence suggests a distinction between familiarity and recollection, and furthermore a partial dissociation between different tests of familiarity; patients with selective hippocampal damage can exhibit recognition memory deficits in a simple ‘yes/no’ task with similar foils, but not in a ‘forced choice’ variant involving choosing the more familiar stimulus from a set[102](https://www.nature.com/articles/s41562-023-01799-z#ref-CR102 "Migo, E., Montaldi, D., Norman, K. A., Quamme, J. & Mayes, A. The contribution of familiarity to recognition memory is a function of test format when using similar foils. Q. J.Exp. Psychol. 62, 1198–1215 (2009)."). This is consistent with the idea that lower prediction error in the neocortical generative network indicates familiarity, but retrieval of unique details from the hippocampus is required for more definitive recognition memory.

Discussion
----------

We have proposed a model of systems consolidation as the training of a generative neural network, which learns to support episodic memory, and also imagination, semantic memory and inference. This occurs through teacher–student learning. The hippocampal ‘teacher’ rapidly encodes an event, which may combine unpredictable sensory elements (with connections to and from the sensory cortex) and predictable conceptual elements (with connections to and from latent variable representations in the generative network). After exposure to replayed representations from the ‘teacher’, the generative ‘student’ network supports reconstruction of events by forming a schematic representation in the sensory neocortex from latent variables via the HF, with unpredictable sensory elements added from the hippocampus.

In contrast to the relatively veridical initial encoding, the generative model learns to capture the probability distributions underlying experiences, or ‘schemas’. This enables not just efficient recall, reconstructing memories without the need to store them individually, but also imagination (by sampling from the latent variable distributions) and inference (by using the learned statistics of experience to predict the values of unseen variables). In addition, semantic memory (that is, factual knowledge) develops as a by-product of learning to predict sensory experience. As the generative model becomes more accurate, the need to store and retrieve unpredicted details in the hippocampus decreases (producing a gradient of retrograde amnesia in cases of hippocampal damage). However, the generative network necessarily introduces distortion compared to the initial memory system. Multiple generative networks can be trained in parallel, and we expect this to include networks with latent variables in the EC, mPFC and alTL.

We now compare the model’s performance to the list of key findings from the introduction:

1.  1.Gradual consolidation follows one-shot encoding: A memory is encoded in the hippocampal ‘teacher’ network after a single exposure, and transferred to the generative ‘student’ network after being replayed repeatedly (Fig. [1c,d](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).
    
2.  2.Semantic memory becomes hippocampus-independent: The latent variable representations learned by the generative networks constitute the ‘key facts’ of an episode, supporting semantic memory (Fig. [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).
    
3.  3.Episodic memory remains hippocampus-dependent: Return projections to the sensory neocortex via the HF are required to decode the latent variable representations into a sensory experience (Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). (EC is required for even simple (re)construction, while the hippocampus proper helps to generate complex conceptually coherent scenes and retrieves unpredictable details that are not yet consolidated into the generative network; see section ‘Neural substrates of the model’.)
    
4.  4.Shared substrate for episode generation: Generative models are a common mechanism for episode generation. Familiar scenes can be reconstructed and new ones can be generated by sampling or transforming existing latent variable representations (Fig. [3b–d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)), providing a model for imagination, scene construction and episodic future thinking.
    
5.  5.Consolidation promotes inference and generalization: Relational inference corresponds to vector arithmetic applied to the generative network’s latent variables (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).
    
6.  6.Episodic memories are distorted: We show how memory distortions arise from the generative network (Figs. [4](https://www.nature.com/articles/s41562-023-01799-z#Fig4), [6](https://www.nature.com/articles/s41562-023-01799-z#Fig6) and [7](https://www.nature.com/articles/s41562-023-01799-z#Fig7)). This extends the model of ref. [32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 "Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).") to relate memory distortion to consolidation.
    
7.  7.Association cortex encodes latent structure: Latent variable representations in the EC, mPFC, and alTL provide schemas for episodic recollection and imagination (via HF) and for semantic retrieval and inference.
    
8.  8.Prediction error affects memory processing: The generative network is constantly calculating the reconstruction error of experiences[58](https://www.nature.com/articles/s41562-023-01799-z#ref-CR58 "Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. PLoS Biol. 4, e424 (2006)."),[59](https://www.nature.com/articles/s41562-023-01799-z#ref-CR59 "Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. Learn. Mem. 18, 523–528 (2011)."). Events that are consistent with the existing generative model require less encoding in the autoassociative hippocampal network (Fig. [5](https://www.nature.com/articles/s41562-023-01799-z#Fig5)).
    
9.  9.Episodic memories include conceptual features: When an experience combines a mixture of familiar and unfamiliar elements, both concepts and poorly predicted sensory elements are stored in the hippocampus via association to a specific memory unit.
    

Our model can be seen as an update to the complementary learning systems (CLS)[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 "McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).") framework to better account for points 3 to 9 above, reconciling the development of semantic representations in the neocortex (as in CLS) with the continued dependence on the hippocampal formation for episodic recall (as in multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 "Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).")). Furthermore, it provides a unified view of: (1) episode generation, (2) how episodic memories change over time and exhibit distortions and (3) how semantic and episodic information are combined in memory. We build on previous work exploring the role of generative networks in consolidation[18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 "Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000)."),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 "Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002)."), as models of the hippocampal formation[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 "Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020)."),[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 "van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020)."), as priors for episodic memory[35](https://www.nature.com/articles/s41562-023-01799-z#ref-CR35 "Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. Neural Comput. 34, 1841–1870 (2022).") and as models of spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).").

A key aspect of the model is that multiple generative networks can be trained concurrently from a single autoassociative network (Fig. [2a](https://www.nature.com/articles/s41562-023-01799-z#Fig2)) and may be optimized for different tasks. Thus, the latent representations in the mPFC and the alTL may be more closely linked to value or language than those in the EC[103](https://www.nature.com/articles/s41562-023-01799-z#ref-CR103 "Moscovitch, M. & Melo, B. Strategic retrieval and the frontal lobes: evidence from confabulation and amnesia. Neuropsychologia 35, 1017–1034 (1997)."),[104](https://www.nature.com/articles/s41562-023-01799-z#ref-CR104 "Lin, W. J., Horner, A. J. & Burgess, N. Ventromedial prefrontal cortex, adding value to autobiographical memories. Sci. Rep. 6, 28630 (2016)."). These differences may arise from differences in network structure (for example, the degree of compression) or from additional training objectives that shape their representations[105](https://www.nature.com/articles/s41562-023-01799-z#ref-CR105 "Gluck, M. A. & Myers, C. E. Hippocampal mediation of stimulus representation: a computational theory. Hippocampus 3, 491–516 (1993).") (for example, the generative network with latent variables in the mPFC might be trained to predict task-relevant value in addition to the EC representations). We expect the generative networks to overlap closer to their sensory inputs/outputs, where general-purpose features are more useful, and diverge as the representations become more abstract (or task-specific if there are additional training objectives)[106](https://www.nature.com/articles/s41562-023-01799-z#ref-CR106 "Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. & Lipson, H. Understanding neural networks through deep visualization. Preprint at 
https://arxiv.org/abs/1506.06579
(2015)."). This may involve a primary VAE with latent variables in the EC, with additional pathways from the higher sensory cortex to the EC routed via latent variables in the mPFC or the alTL.

Our model raises some fundamental questions: Does true episodic memory require event-unique detail, and does this require the hippocampus? Or can prototypical predictions qualify as memory rather than imagination? In the model, event-unique details are initially provided by the hippocampus but can also be provided by the generative network. For example, if you know that someone attended your 8th birthday party and gave you a particular gift, these personal semantic facts need not be hippocampal-dependent but could generate a scene with the right event-specific details, which would seem like episodic memory. The increasingly sophisticated generation of images from text using generative models[107](https://www.nature.com/articles/s41562-023-01799-z#ref-CR107 "Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with CLIP latents. Preprint at 
https://arxiv.org/abs/2204.06125
(2022).") suggests that episode construction from semantic facts is computationally plausible.

Episodic memories are defined by their unique spatiotemporal context[1](https://www.nature.com/articles/s41562-023-01799-z#ref-CR1 "Tulving, E. How many memory systems are there? Am. Psychol. 40, 385–398 (1985)."). In the model, spatial and temporal context correspond to conceptual features captured by place[108](https://www.nature.com/articles/s41562-023-01799-z#ref-CR108 "O’Keefe, J. & Dostrovsky, J. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. Brain Res. 34, 171–175 (1971)."),[109](https://www.nature.com/articles/s41562-023-01799-z#ref-CR109 "Ekstrom, A. D. et al. Human hippocampal theta activity during virtual navigation. Hippocampus 15, 881–889 (2005).") or time[110](https://www.nature.com/articles/s41562-023-01799-z#ref-CR110 "Eichenbaum, H. Time cells in the hippocampus: a new dimension for mapping memories. Nat. Rev. Neurosci. 15, 732–744 (2014)."),[111](https://www.nature.com/articles/s41562-023-01799-z#ref-CR111 "Umbach, G. et al. Time cells in the human hippocampus and entorhinal cortex support episodic memory. Proc. Natl Acad. Sci. USA 117, 28463–28474 (2020).") cells in the hippocampus and might be linked to latent variable representations formed in the EC, such as grid cells in the medial EC, which form an efficient basis for locations in real[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[112](https://www.nature.com/articles/s41562-023-01799-z#ref-CR112 "Dordek, Y., Soudry, D., Meir, R. & Derdikman, D. Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis. Elife 5, e10094 (2016)."),[113](https://www.nature.com/articles/s41562-023-01799-z#ref-CR113 "Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippocampus as a predictive map. Nat. Neurosci. 20, 1643–1653 (2017).") or cognitive spaces[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 "Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016)."), or temporal context representations in the lateral EC[114](https://www.nature.com/articles/s41562-023-01799-z#ref-CR114 "Tsao, A. et al. Integrating time from experience in the lateral entorhinal cortex. Nature 561, 57–62 (2018)."),[115](https://www.nature.com/articles/s41562-023-01799-z#ref-CR115 "Bright, I. M. et al. A temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex. Proc. Natl Acad. Sci. USA 117, 20274–20283 (2020)."). Events with specific spatial and temporal context can be generated from these latent variable representations, as has been modelled in detail for space[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 "Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000)."),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 "Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007)."),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).").

More generally, this work builds on the spatial cognition literature, in which place and head direction cells act as latent variables in a generative model[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 "Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000)."),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 "Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007)."),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018)."), allowing the generation of a scene from a specific viewpoint. References [20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 "Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000)."),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 "Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007)."),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 "Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).") explore how egocentric sensory representations could be transformed into allocentric latent variables before storage in the medial temporal lobe and conversely, how egocentric representations could be reconstructed from allocentric ones to support imagery. The latent representations learned through consolidation in our model correspond loosely to the allocentric representations, and the sensory representations produced by HF to the egocentric ones; only egocentric and sensory representations are directly experienced, whereas allocentric and semantic representations are useful abstractions that can also be exploited for efficient hippocampal encoding.

Our model simplifies the true nature of mnemonic processing in several ways. First, the interaction of sensory and conceptual features in the hippocampus and latent variables in the EC during retrieval could be more complex, with each type of representation contributing to pattern completion of the other as in interactions between items and contextual representations in the Temporal Context Model[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 "Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002)."), and might iterate over retrievals from both hippocampal and generative networks[50](https://www.nature.com/articles/s41562-023-01799-z#ref-CR50 "Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016)."). Second, our model distinguishes between ‘sensory’ and ‘conceptual’ representations in the hippocampus, respectively linked to the sensory neocortex at the input/output of the generative network and to the latent variable layer in the middle. In reality, a gradient of levels of representation in the hippocampus is more likely, from detailed sensory representations to coarse-grained conceptual ones, respectively linked to lower or higher neocortical areas[117](https://www.nature.com/articles/s41562-023-01799-z#ref-CR117 "Moscovitch, M., Cabeza, R., Winocur, G. & Nadel, L. Episodic memory and beyond: the hippocampus and neocortex in transformation. Annu. Review Psychol. 67, 105–134 (2016)."), and might map onto the observed functional gradients along the longitudinal axis of the hippocampus[118](https://www.nature.com/articles/s41562-023-01799-z#ref-CR118 "Strange, B. A., Witter, M. P., Lein, E. S. & Moser, E. I. Functional organization of the hippocampal longitudinal axis. Nat. Rev. Neurosci. 15, 655–669 (2014)."). Third, our generative network uses back-propagation of the prediction error between output and input patterns to learn. Generative networks with more plausible (if less efficient) learning rules exist[67](https://www.nature.com/articles/s41562-023-01799-z#ref-CR67 "Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. Neural Comput. 7, 889–904 (1995)."),[68](https://www.nature.com/articles/s41562-023-01799-z#ref-CR68 "Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999)."),[69](https://www.nature.com/articles/s41562-023-01799-z#ref-CR69 "Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010)."), which have the advantage of producing a prediction error signal at each layer (between top–down prediction and bottom–up recognition), potentially allowing learning of concepts and exceptions at all levels of description. Fourth, considering consolidation as a continual lifelong process rather than during encoding of a single dataset introduces new complexities; these include the instability of latent representations and the prevention of catastrophic forgetting of already consolidated memories as new memories are assimilated into the generative network. The model could be extended to address this, for example, by using replay from the generative network as well as from the hippocampal network, which could reduce catastrophic forgetting and stabilize latent variable representations in both networks[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 "van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020)."),[119](https://www.nature.com/articles/s41562-023-01799-z#ref-CR119 "Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. Nat. Neurosci. 7, 286–294 (2004)."),[120](https://www.nature.com/articles/s41562-023-01799-z#ref-CR120 "van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at 
https://arxiv.org/abs/1809.10635
(2018)."), building on previous research on sleep and learning[121](https://www.nature.com/articles/s41562-023-01799-z#ref-CR121 "Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. Proc. Natl Acad. Sci. USA 119, e2123432119 (2022)."). Fifth, we model semantic memory as prediction of categorical information for an ‘event’, but future work should model more complex semantic knowledge, for example, by decoding language from latent representations of multimodal stimuli[85](https://www.nature.com/articles/s41562-023-01799-z#ref-CR85 "Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in Proc. IEEE Conference on Computer Vision and Pattern Recognition 3156–3164 (2015)."),[86](https://www.nature.com/articles/s41562-023-01799-z#ref-CR86 "Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at 
https://arxiv.org/abs/2111.09734
(2021)."). In particular, the relationship between semantic memory for specific ‘events’ and the broader ‘web’ of general knowledge should be considered.

Episodic memories contain important sequential structure not modelled by our encoding and reconstruction of simple scenes. Future work could expand the model’s scope to sequential information as follows. A range of stimuli could be represented as sequences of arbitrary symbols (including language, spatial trajectories and transitions on a graph). A heteroassociative variant of an MHN, which is better suited to sequential data, could be used to store such stimuli. Specifically, the interpretation of an MHN that we use[64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 "Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).") can capture sequential information if the projections from feature units to memory units correspond to the current state, but the projections from memory units back to feature units correspond to the next state so that one state retrieves the next[122](https://www.nature.com/articles/s41562-023-01799-z#ref-CR122 "Millidge, B., Salvatori, T., Song, Y., Lukasiewicz, T. & Bogacz, R. Universal Hopfield networks: a general framework for single-shot associative memory models. in International Conference on Machine Learning 15561–15583 (PMLR, 2022)."),[123](https://www.nature.com/articles/s41562-023-01799-z#ref-CR123 "Chaudhry, H. T., Zavatone-Veth, J. A., Krotov, D. & Pehlevan, C. Long sequence Hopfield memory. Preprint at 
https://arxiv.org/abs/2306.04532
(2023)."),[124](https://www.nature.com/articles/s41562-023-01799-z#ref-CR124 "Tang, M., Barron, H. & Bogacz, R. Sequential memory with temporal predictive coding. Adv. Neural Inf. Process. Syst. 27 (2023)."). With certain modifications based on previous work involving the role of temporal context in memory[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 "Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002)."),[125](https://www.nature.com/articles/s41562-023-01799-z#ref-CR125 "Burgess, N. & Hitch, G. J. Memory for serial order: a network model of the phonological loop and its timing. Psychol. Rev. 106, 551–581 (1999)."), asymmetric MHNs can store sequences with complex repetitions and temporal correlations, such as language. We could then implement the student model as a sequential generative network trained to predict the next input during sequential replay (for example, GPT-2 (ref. [126](https://www.nature.com/articles/s41562-023-01799-z#ref-CR126 "Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog 
https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
(2019)."))). Such networks capture relational structure, developing grid-like latent representations in spatial tasks[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 "Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020)."), and learn the gist of narratives. The sequential model could also be applied to phenomena such as event segmentation[127](https://www.nature.com/articles/s41562-023-01799-z#ref-CR127 "Bird, C. M. How do we remember events? Curr. Opin. Behav. Sci. 32, 120–125 (2020).") and memory distortions in narratives[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 "Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932)."). (Note that for more complex sequential data such as videos, pattern completion of both the current stimulus and the next stimulus would be required, potentially needing a combination of autoassociative and heteroassociative connectivity in the hippocampal network.)

Our model makes testable predictions. First, if participants learn stimuli generated from known latent variables, it predicts that these specific latent variable representations should develop in the association cortex over time (and that this representation would support, for example, vector arithmetic and interpolation). This could be tested by representational similarity analysis, which should reveal a more conceptual similarity structure developing in the association cortex through consolidation, as opposed to a similarity structure reflecting the sensory stimuli in earlier sensory cortices. If the stimuli also contained slight variation, that is, they were not entirely described by the latent variables, the development of a latent variable representation should be correlated with gist-based distortions in memory and anti-correlated with hippocampal processing of unpredictable elements.

Second, the model makes multiple predictions about the effects of brain damage. Just as boundary extension is reduced in patients with damage to the HF[128](https://www.nature.com/articles/s41562-023-01799-z#ref-CR128 "Mullally, S. L., Intraub, H. & Maguire, E. A. Attenuated boundary extension produces a paradoxical memory advantage in amnesic patients. Curr. Biol. 22, 261–268 (2012).") or the vmPFC[129](https://www.nature.com/articles/s41562-023-01799-z#ref-CR129 "De Luca, F. et al. Boundary extension is attenuated in patients with ventromedial prefrontal cortex damage. Cortex 108, 1–12 (2018)."), we predict that other biases towards the ‘canonical view’ would be attenuated in such patients; for example, healthy controls would distort images with an atypical viewing angle towards a more typical angle in memory, but this would be reduced in, for example, hippocampal patients. Similarly, ambiguous images such as the duck/rabbit drawing ‘flip’ between interpretations in perception but are stable when held in imagery[72](https://www.nature.com/articles/s41562-023-01799-z#ref-CR72 "Chambers, D. & Reisberg, D. Can mental images be ambiguous? J. Exp. Psychol. Hum. Percept. Perform. 11, 317–328 (1985)."), presumably due to maintained hippocampal conceptual representations. We predict that this conceptual stability in imagery would also be reduced in such patients. This could also extend to non-scene stimuli: if the ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 "Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).") task were tested with both healthy controls and patients with damage to the generative decoder, we would predict reduced contextual distortion in the latter. Furthermore, patients with an inaccurate generative model, for example, due to semantic dementia, might rely more on sensory features to compensate. (Note that the pattern of deficits would depend on both the nature of the priors encoded in the generative network and the error threshold for encoding. In some cases, damage to the generative network could produce atypical ‘priors’ rather than suppressing them. Thus, if the generative network is inaccurate but the error threshold for encoding is high, atypical distortions will be observed rather than a reduction in conceptual distortions.)

Third, the model suggests that the error threshold for encoding could vary depending on the importance of the stimuli or the amount of attentional resource available. For example, emotional salience could lower this threshold, with traumatic memories being encoded in greater sensory detail and with less contextual coherence[130](https://www.nature.com/articles/s41562-023-01799-z#ref-CR130 "Van Der Kolk, B. A., Burbridge, J. A. & Suzuki, J. The psychobiology of traumatic memory. Clinical implications of neuroimaging studies. Ann. N. Y. Acad. Sci. 821, 99–113 (1997)."),[131](https://www.nature.com/articles/s41562-023-01799-z#ref-CR131 "Bisby, J. A., Burgess, N. & Brewin, C. R. Reduced memory coherence for negative events and its relationship to posttraumatic stress disorder. Curr. Dir. Psychol. Sci. 29, 267–272 (2020)."). Equally, conditions such as autism spectrum disorder, which are potentially attributable to hypo-priors[132](https://www.nature.com/articles/s41562-023-01799-z#ref-CR132 "Pellicano, E. & Burr, D. When the world becomes ‘too real’: a Bayesian explanation of autistic perception. Trends Cogn. Sci. 16, 504–510 (2012)."), might be associated with a lower prediction error threshold for veridical storage (and thus reduced conceptual influence on memory and increased sensory detail). In addition, reality monitoring deficits would change the perceived prediction error relative to reality, leading to atypical memory storage (for example, a reduced ability to compensate for prediction errors by storing sensory details).

Fourth, biological intelligence excels at generalizing from only a small number of examples. The model predicts that learning to generalize rapidly benefits from having a generative model that can create new examples, for example, by inferring variants (as in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) (see also ref. [133](https://www.nature.com/articles/s41562-023-01799-z#ref-CR133 "Barry, D. N. & Love, B. C. A neural network account of memory replay and knowledge consolidation. Cereb. Cortex. 33, 83–95 (2022).")). Finally, the model suggests a link between latent spaces and cognitive maps[134](https://www.nature.com/articles/s41562-023-01799-z#ref-CR134 "Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron 100, 490–509 (2018)."). For example, one might predict that the position of a memory in latent space is reflected in place and grid cell firing, as observed for other conceptual representations[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 "Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016)."),[134](https://www.nature.com/articles/s41562-023-01799-z#ref-CR134 "Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron 100, 490–509 (2018)."),[135](https://www.nature.com/articles/s41562-023-01799-z#ref-CR135 "Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. Nature 595, 80–84 (2021).").

In summary, our proposed model takes inspiration from recent advances in machine learning to capture many of the intriguing phenomena associated with episodic memory, its (re)constructive nature, its relationship to schemas, and consolidation, as well as aspects of imagination, inference and semantic memory.

Methods
-------

### Data

In the simulations, images represent events (except for the DRM[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).") task stimuli). The Shapes3D dataset[136](https://www.nature.com/articles/s41562-023-01799-z#ref-CR136 "Burgess, C. & Kim, H. 3D Shapes dataset. GitHub 
https://github.com/google-deepmind/3d-shapes
(2018).") was used throughout, except for the use of MNIST[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 "LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).") to explore certain distortions. Note that one MHN was used per dataset, and one generative model was trained per dataset from the corresponding MHN’s outputs.

### Basic model

In our model, the hippocampus rapidly encodes an event, modelled as one-shot memorization in an autoassociative network (an MHN). Then, generative networks are trained on replayed representations from the autoassociative network, learning to reconstruct memories by capturing the statistical structure of experienced events.

The generative networks used are variational autoencoders, a type of autoencoder with special properties such that randomly sampling values for the latent variables in the model’s ‘bottleneck’ layer generates valid stimuli[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 "Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at 
https://arxiv.org/abs/1312.6114
(2013)."). Figure 3 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1), adapted from ref. [137](https://www.nature.com/articles/s41562-023-01799-z#ref-CR137 "Hou, X., Shen, L., Sun, K. & Qiu, G. Deep feature consistent variational autoencoder. in 2017 IEEE Winter Conference on Applications of Computer Vision (WACV) 1133–1141 (IEEE, 2017)."), shows how directions in the latent space can correspond to meaningful transformations. While most diagrams show the VAE’s input and output layers in the sensory neocortex as separated (in line with conventions for visualizing neural networks), it is important to note that the input and output layers are in fact the same, as shown in Fig. [1b](https://www.nature.com/articles/s41562-023-01799-z#Fig1). There may be considerable overlap between the encoder and decoder, especially closer to the sensory neocortex, but we did not model this explicitly. The autoassociative model is an MHN, with the property that even random input values will retrieve one of the stored patterns via pattern completion. Specifically, we considered the biological interpretation of the MHN as feature units and memory units suggested by ref. [64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 "Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).") (see [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details).

We modelled consolidation as teacher–student learning, where the autoassociative network is the ‘teacher’ and the generative network is the ‘student’ trained on replayed representations from the ‘teacher’. We gave random noise (consisting of uniformly sampled values in each channel for each pixel) as an input to the MHN, then used the outputs of the network to train the VAE. (These outputs represent the high-level sensory representations activated by hippocampal pattern completion, via return projections to the sensory cortex.) The noise input to the autoassociative network could potentially represent random activation during sleep[138](https://www.nature.com/articles/s41562-023-01799-z#ref-CR138 "Stella, F., Baracskay, P., O’Neill, J. & Csicsvari, J. Hippocampal reactivation of random trajectories resembling Brownian diffusion. Neuron 102, 450–461 (2019)."),[139](https://www.nature.com/articles/s41562-023-01799-z#ref-CR139 "González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E. & Bazhenov, M. Can sleep protect memories from catastrophic forgetting? Elife 9, e51005 (2020)."),[140](https://www.nature.com/articles/s41562-023-01799-z#ref-CR140 "Pezzulo, G., Zorzi, M. & Corbetta, M. The secret life of predictive brains: what’s spontaneous activity for? Trends Cogn. Sci. 25, 730–743 (2021)."). Attributes such as reward salience might also influence which memories are replayed but were not modelled here[141](https://www.nature.com/articles/s41562-023-01799-z#ref-CR141 "Igata, H., Ikegaya, Y. & Sasaki, T. Prioritized experience replays on a hippocampal predictive map for learning. Proc. Natl Acad. Sci. USA 118, e2011266118 (2021).").

During the encoding state in our simulations, images were stored in a continuous MHN with high inverse temperature, _β_, set to 20 (higher values of _β_ produce attractor states corresponding to individual memories, while lower values of _β_ make metastable states more likely). Reference [63](https://www.nature.com/articles/s41562-023-01799-z#ref-CR63 "Ramsauer, H. et al. Hopfield networks is all you need. in International Conference on Learning Representations (2021).") provides an excellent Python implementation of MHNs that we used in our code. During the ‘rest’ state, random noise was given as an input _N_ times to the MHN, retrieving _N_ attractor states from the network. (The distribution of retrieved attractor states was not tested but was approximately random, and very few spurious attractors were observed with sufficiently high inverse temperature.) In the main simulations, 10,000 items from the Shapes3D dataset were encoded in the MHN, and 10,000 replayed states were used to train the VAE (that is, _N_ is 10,000). (Rather than replaying new samples from the MHN at each epoch of the VAE’s training, a single set of samples was used for efficiency and simplicity.)

A VAE was then trained on the ‘replayed’ images from the MHN, using the Keras API for TensorFlow[142](https://www.nature.com/articles/s41562-023-01799-z#ref-CR142 "Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) 265–283 (USENIX Assoc., 2016)."). The loss function (that is, the error minimized through training) is the sum of two terms, the reconstruction error and the Kullback–Leibler divergence[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 "Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at 
https://arxiv.org/abs/1312.6114
(2013)."); the former encourages accurate reconstruction, while the latter (which measures the divergence between the latent variables and a Gaussian distribution) encourages a latent space one can sample from. Specifically, the reconstruction loss in our model is a mean absolute error loss. (Note that the terms reconstruction error and prediction error are used interchangeably throughout.)

The stochastic gradient descent method used was the AMSGrad variant of the Adam optimizer with early stopping enabled, for a maximum of 50 epochs (where an epoch is a complete pass through the training set). A latent variable vector length of 20, learning rate of 0.001 and Kullback–Leibler weighting of 1 were used in the main results. The variational autoencoders were not optimized for performance, as their purpose is illustrative (more data and hyperparameter tuning would be likely to improve reconstruction accuracy). Architectural choices within the VAE were not principled but were based on successful architectures for similar stimuli in the literature. See [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details of the VAE’s architecture. The VAEs were trained using gradient descent and back-propagation as usual; while this method is biologically implausible due to its non-local nature, more plausible learning algorithms might be feasible[143](https://www.nature.com/articles/s41562-023-01799-z#ref-CR143 "Whittington, J. C. R. & Bogacz, R. Theories of error back-propagation in the brain. Trends Cogn. Sci. 23, 235–250 (2019).").

While this was not modelled explicitly, once the generative network’s reconstruction error is sufficiently low, the hippocampal trace is unnecessary. As a result, it could be ‘marked for deletion’ or overwritten in some way, freeing up capacity for new encodings. However, we did not simulate decay, deletion or capacity constraints in the autoassociative memory part of the model. In these simulations, the main cause of forgetting would be interference from new memories in the generative model.

Note that throughout the simulations, the input to recall was a noisy version of the encoded stimulus image. Specifically, noise was added by replacing a random fraction (0.1 unless stated otherwise) of values in the image array with zero.

While we used only one modality at a time (imagery for the majority of simulations, text for the DRM task), our model is compatible with the multimodal nature of experience, as multimodal inputs to VAEs are possible, which result in a multimodal latent space[144](https://www.nature.com/articles/s41562-023-01799-z#ref-CR144 "Khattar, D., Goud, J. S., Gupta, M. & Varma, V. Mvae: multimodal variational autoencoder for fake news detection. in The World Wide Web Conference 2915–2921 (ACM, 2019)."). This could reflect the multimodal nature of concept cells in the hippocampus[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 "Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012).").

### Modelling semantic memory

We modelled semantic memory as the ability to decode latent variables into semantic information without the need to reconstruct the event episodically.

Decoding accuracy was measured by training a support vector machine to classify the central object’s shape from the network’s latent variables, using 200 examples at the end of each epoch and measuring classification accuracy on a held-out test set. (Notably, there was good performance with only a small amount of training data when decoding the latent variables, compared with decoding alternative representations such as the sensory input or intermediate layer activations, that is, few-shot learning is possible by making use of compressed ‘semantic’ representations. See Fig. 2 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1).)

### Modelling imagination and inference

In the generative network, new items can either be generated from externally specified (or randomly sampled) latent variables (imagination), or by transforming the latent variable representations of specific events (relational inference). The former was simulated by sampling from categories in the latent space, then decoding the results (Fig. [3d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). The latter was simulated by interpolating between the latent representations of events (Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) or by doing vector arithmetic in the latent space (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).

Examples of the four different object shapes were generated by Monte Carlo sampling for simplicity, that is, samples from the latent space were classified by the semantic decoding classifier, and examples that activate each category are displayed. (Note that there are many alternative ways to do this, for example, by extracting the decision boundaries from the classifier and sampling within the region corresponding to each class.) Generating imagined scenes from more naturalistic inputs, for example, natural language descriptions, would require a much more sophisticated text to the latent space model, but recent machine learning advances suggest that this is possible[145](https://www.nature.com/articles/s41562-023-01799-z#ref-CR145 "Ramesh, A. et al. Zero-shot text-to-image generation. in International Conference on Machine Learning 8821–8831 (PMLR, 2021).").

To demonstrate interpolation, each row of Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows items generated from latent variables along a line in the latent space between two real items from the training data. To demonstrate vector arithmetic, each equation in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows ‘result = vector_A_ + (vector_B_ − vector_C_)’ (reflecting relational inference problems of the form ‘what is to _A_ as _B_ is to _C_?’), where the result is produced by taking the relation between vector_B_ and vector_C_, applying that to vector_A_ and decoding the result. In other words, the three items on the right of each equation in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3) are real items from the training data. Their latent variable representations are combined as vectors according to the equation shown, giving the latent variable representation from which the first item is generated. Thus, the pair in brackets describes a relation that is applied to the first item on the right to produce the new item on the left of the equation.

### Modelling schema-based distortions

Items recalled by the generative network become more prototypical, a form of schema-based distortion. This can be shown simply in the basic model, using the MNIST digits dataset[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 "LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).") to exemplify ten clearly defined classes of items (Fig. [4](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). To show this quantitatively, we calculated the intra-class variation, measured as the mean variance per pixel, within each MNIST class before and after recall, for 5,000 images from the test set. As expected, the intra-class variation was smaller for the recalled items than for the original inputs. (See [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details of the model architecture.)

To visualize this, we projected the pixel and latent spaces before and after recall (of 2,000 images from the MNIST test set) into two dimensions (2D) with uniform manifold approximation and projection (UMAP)[146](https://www.nature.com/articles/s41562-023-01799-z#ref-CR146 "McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at 
https://arxiv.org/abs/1802.03426
(2018)."), a dimensionality reduction method, and colour-coded them by class (Fig. [4c,d](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). The pixel space of MNIST digits (bottom row) and the latent space of their encodings (top row) showed more compact clusters for the generative network’s outputs (Fig. [4d](https://www.nature.com/articles/s41562-023-01799-z#Fig4)) than for its inputs (Fig. [4c](https://www.nature.com/articles/s41562-023-01799-z#Fig4)).

### Modelling boundary extension and contraction

Boundary extension is the tendency to remember a wider field of view than was observed for certain stimuli[88](https://www.nature.com/articles/s41562-023-01799-z#ref-CR88 "Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. J. Exp. Psychol. Learn. Mem. Cogn. 15, 179–187 (1989)."), while boundary contraction is the tendency to remember a narrower one[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020)."). Whether boundaries are extended or contracted seems to depend on the perceived distance of the central object, with unusually close-up (that is, ‘object-oriented’) views causing boundary extension, and unusually far away (that is, ‘scene-oriented’) views causing boundary contraction[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).").

We tested boundary extension and contraction in the basic model by giving it a range of artificially ‘zoomed in’ or ‘zoomed out’ images, adapted from Shapes3D scenes not seen during training, and observing the outputs. The ‘zoomed in’ view was produced by removing _n_ pixels from the margin. The ‘zoomed out’ view was produced by extrapolating the pixels at the margin outwards by _n_ additional pixels. (In both cases, the new images were then resized to the standard size.) The zoom level is the ratio of the central object size in the output image to the size in the original image, given as a percentage; for example, an image with a zoom level of 80% or a ratio of 0.8 was produced by adding a margin so that the object size is 80% of the original size. As the Shapes3D images are of width and height 64, the number of pixels to add or remove was calculated as ‘margin = (32/ratio) − 32’.

In Fig. [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4), the change in object size between the noisy input and output was estimated as follows: first the image was converted to a few colours by _k_\-means clustering of pixels. Then, the colour of the central object was determined by finding the predominant colour in a particular central region of the image. A 1D array of pixels corresponding to a vertical line at the horizontal midpoint of the image was processed to identify the fraction of pixels of the central object colour. This enabled us to calculate the change in object size, which we plotted against the degree of ‘zoom’. (For this object size estimation approach to work, we filtered the Shapes3D dataset to images where the object colour was different from both the wall and floor colour, and additionally to cubes to minimize shadow.)

Note that the measure of boundary extension vs contraction displayed in Fig. [4f](https://www.nature.com/articles/s41562-023-01799-z#Fig4), reproduced from ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 "Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. 
https://doi.org/10.1167/jov.21.9.2124
(2021)."), was not based on the degree of distortion, but was produced by averaging ‘closer’ vs ‘further’ judgements of an identical stimulus image in comparison to the remembered image. This differs from our measure in Fig. [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4), which instead corresponds to the drawing-based measure in ref. [89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020)."); however, these measures have been shown to be correlated[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 "Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).").

Figure [4e](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows a few examples of boundary extension and contraction. In the left- and right-hand images of each set, the margin _n_ is chosen such that the central object is 80% and 120% of the original size, respectively.

### Extended model

The extended model was designed to capture the fact that memory traces in the hippocampus bind together a mixture of sensory and conceptual elements, with the latter encoded by concept cells[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 "Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012)."), and the fact that schemas shape the reconstruction of memories even before consolidation, as shown by the rapid onset of schema-based distortions[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).").

In the extended model, each scene was initially encoded as the combination of a predictable and an unpredictable component. The predictable component consisted of concepts captured by the latent variables of the generative network, and the unpredictable component consisted of parts of the stimuli that were poorly predicted by the generative network. Thus, the MHN model has both conceptual and sensory feature units, which store the predictable and unpredictable aspects of memory, respectively. While memories may eventually become fully dependent on the generative model, consolidation can be a prolonged process during which the generative network provides schemas for reconstruction and the autoassociative network supports new or detailed information not yet captured by schemas. (The VAE trained in the basic model simulations was used in the extended model simulations described below.)

How did encoding work in our simulations? For a new image, the prediction error of each pixel was calculated by the VAE (simply the magnitude of the difference between the VAE’s input and output). Those pixels with a reconstruction error above the threshold constituted the unpredictable component, while the VAE’s latent variables constituted the predictable component, and these components were combined into a single vector and encoded in the MHN. Note that when the threshold is zero, the reconstruction is guaranteed to be perfect, but as the threshold increases, the reconstruction decreases in accuracy.

How did recall work before full consolidation? After decomposing the input into its predictable (conceptual) and unpredictable (sensory) components, as described above, the autoassociative network could retrieve a memory. The image corresponding to the conceptual component was then obtained by decoding the stored latent variables. Next, the predictable and unpredictable elements were recombined, simply by overwriting the initial schematic reconstruction in the sensory neocortex with any stored (that is, non-zero) sensory features in the hippocampus. Figure [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5) shows this process. The lower the error threshold for encoding sensory details, the more information was stored in the autoassociative network, reducing the reconstruction error of recall (see also section ‘Modelling schema-based distortions’).

How did replay work? When the autoassociative network was given random noise, both the unpredictable elements and the corresponding latent variables were retrieved. In Fig. [5d](https://www.nature.com/articles/s41562-023-01799-z#Fig5), the square images show the unpredictable elements of MNIST images and the rectangles below these display the vector of latent variables. (As the generative model improves, the presence of hippocampal sensory features that no longer differ from the initial reconstruction indicates that the hippocampal representation is no longer needed, but this was not simulated explicitly.)

We note that the latent variable representation is not stable as the generative network learns. If some latent variables are stored in the autoassociative network while the VAE continues to change, the quality of the VAE’s reconstruction will gradually worsen; this is also a feature of previous models[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 "Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021)."). Some degree of degradation may reflect forgetting, but consolidation can be a prolonged process and hippocampal representations can persist in this time. Therefore, we think that concepts derived from latent variables are more likely to be stored than the latent variables themselves, promoting the stability of hippocampal representations. (For example, in humans, language provides a set of relatively persistent concepts, stabilized by the need to communicate.) Projections from the latent variables can classify attributes with only a small amount of training data (see section ‘Modelling semantic memory’); we suggest that there could be a two-way mapping between latent variables and concepts, which supports categorization of incoming experience as well as semantic memory. However, for simplicity, the conceptual features were simply a one-to-one copy of latent variable representations in these simulations. It may also be possible to stabilize the latent variable representations by reducing catastrophic forgetting in the generative network, for example, by using generative as well as hippocampal replay[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 "van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020)."),[119](https://www.nature.com/articles/s41562-023-01799-z#ref-CR119 "Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. Nat. Neurosci. 7, 286–294 (2004)."),[120](https://www.nature.com/articles/s41562-023-01799-z#ref-CR120 "van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at 
https://arxiv.org/abs/1809.10635
(2018)."), with the generative network trained on its own self-generated representations in addition to new memories. This builds on previous research suggesting that certain stages of sleep are optimized to preserve remote memories, while others consolidate new ones[121](https://www.nature.com/articles/s41562-023-01799-z#ref-CR121 "Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. Proc. Natl Acad. Sci. USA 119, e2123432119 (2022)."). This could reduce interference of new learning with remote memories in the generative network, as well as make hippocampal representations in the extended model more stable.

### Modelling schema-based distortions in the extended model

#### Carmichael experiment

We demonstrated the contextual modulation of memory (as in ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 "Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).")) in the extended model by manipulating the conceptual component of an ‘event’. To model an external conceptual context being encoded, the original image was stored in the autoassociative network along with activation of a given concept (a cube or a sphere), represented as the latent variables for that class. While in most simulations the latent variables stored in the MHN were simply the output of the VAE’s encoder, here an external context activated the conceptual representation, consistent with activity in the EC, mPFC or alTL driven by extrinsic factors.

During recall, a noisy input was processed by the generative network to produce a predicted conceptual feature and the sensory features not predicted by the prototype for that concept, for input to the autoassociative MHN. Pattern completion in the MHN produced the originally encoded sensory and conceptual features, and these were recombined to produce the final output.

#### DRM experiment

The DRM task is a classic way to measure gist-based memory distortion[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 "Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959)."),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 "Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995)."). Here we demonstrated the rapid onset of semantic intrusions in the extended model, coming about as a consequence of learning the co-occurrence statistics of words in a text dataset representing ‘background knowledge’. This followed on from previous work showing that VAEs produce semantic intrusions[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 "Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).").

In brief, the DRM task involved showing participants a list of words that were semantically related to a ‘lure word’, which was not present in the list. There was a tendency for both false recognition and false recall of the lure word. We focused on modelling the recall task, but the same model could be extended to recognition (with recognition memory measured by the reconstruction error of the network).

The generative network was pre-trained on a set of word lists extracted from simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 "Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016)."), representing learning from replayed memories before the DRM stimuli (although replay was not simulated explicitly). Words occurring in <0.05% or \>10% of documents were discarded to keep the vocabulary to a manageable size of 4,206 words (this meant that some rarer words in the DRM lists were removed). The word lists were converted to vectors of word counts of length 4,206, in which the value at index _i_ of the vector for a given list indicated the count of word _i_ in the document. As these representations ignore word order, a sequential model was not required (however, this prevented exploring the effect of list position on recall).

Specifically, the variational autoencoder used for this simulation consisted of an input layer followed by a dropout layer[147](https://www.nature.com/articles/s41562-023-01799-z#ref-CR147 "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929–1958 (2014).") projecting to 300 latent variables (sampled from representations of the mean and log variance vectors as usual), and then to an output layer with a sigmoid activation so that predictions were between 0 and 1, with L1 regularization to promote sparsity in this layer. As above, this was implemented using the Keras API for the TensorFlow library[142](https://www.nature.com/articles/s41562-023-01799-z#ref-CR142 "Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) 265–283 (USENIX Assoc., 2016)."),[148](https://www.nature.com/articles/s41562-023-01799-z#ref-CR148 "Chollet, F. et al. Keras Documentation (GitHub, 2015)."), with the VAE trained to reconstruct input vectors in the usual way.

Following pre-training of the generative network, the system encoded the DRM stimuli, with each of the 20 word lists represented as vectors of word counts. One important detail was the addition of a term, given by ‘id\_n’ for the _n_th document in the corpus, representing the unique spatiotemporal context of each word list. (Note that this is a highly simplified representation of the spatiotemporal context[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 "Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002).") for illustration.) This enabled recall to be modelled by presenting the network with the ‘id\_n’ term, and seeing which terms were retrieved.

In the extended model, the latent representation of the word list was encoded in the MHN as the conceptual component, while the unique ‘id\_n’ terms were encoded veridically (as vectors of word counts of length 4,226—the original vocabulary size plus the 20 new ‘id\_n’ terms—with 1 at ‘id\_n’ and 0 elsewhere). The sparse vector representing the unexpected ‘id\_n’ term is analogous to the sparse arrays of poorly predicted pixels in the main simulations of the extended model.

When the MHN was given ‘id\_n’ as an input, it retrieved the hippocampal trace consisting of ‘id\_n’ together with the latent representation of the word list. The latent representation was then decoded to produce the outputs shown in Fig. [7a](https://www.nature.com/articles/s41562-023-01799-z#Fig7) (a dashed line shows the threshold for recall, interpreting the output as a probability so that words with an output \>0.5 are recalled). As in the human data, lure words were often but not always recalled. The system also forgot some words and produced additional semantic intrusions, for example, ‘vet’ in the case of the ‘doctor’ list.

To test the effect of varying the number of associates, as in ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 "Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997)."), subsets of the DRM lists were encoded in the way described above. Specifically, to test the probability of lure recall with _n_ associates studied, _n_ items from each DRM list were encoded. For each list, this was repeated for 20 randomly sampled combinations of _n_ items. Once again, recall was tested by giving the system ‘id\_n’ as an input.

### Reporting summary

Further information on research design is available in the [Nature Portfolio Reporting Summary](https://www.nature.com/articles/s41562-023-01799-z#MOESM2) linked to this article.

Data availability
-----------------

Code availability
-----------------

Code for all simulations can be found at [https://github.com/ellie-as/generative-memory](https://github.com/ellie-as/generative-memory). Some diagrams were created using [BioRender.com](http://biorender.com/).

References
----------

1.  Tulving, E. How many memory systems are there? _Am. Psychol._ **40**, 385–398 (1985).
    
    [Article](https://doi.org/10.1037%2F0003-066X.40.4.385)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20many%20memory%20systems%20are%20there%3F&journal=Am.%20Psychol.&doi=10.1037%2F0003-066X.40.4.385&volume=40&pages=385-398&publication_year=1985&author=Tulving%2CE)
    
2.  Marr, D. A theory for cerebral neocortex. _Proc. R. Soc. Lond. B_ **176**, 161–234 (1970).
    
    [Article](https://doi.org/10.1098%2Frspb.1970.0040)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaE3M%2Fjs1OgtQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=4394740)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20theory%20for%20cerebral%20neocortex&journal=Proc.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frspb.1970.0040&volume=176&pages=161-234&publication_year=1970&author=Marr%2CD)
    
3.  Marr, D. Simple memory: a theory for archicortex. _Phil. Trans. R. Soc. Lond. B_ **262**, 23–81 (1971).
    
    [Article](https://doi.org/10.1098%2Frstb.1971.0078)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaE38%2FltlKltg%3D%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Simple%20memory%3A%20a%20theory%20for%20archicortex&journal=Phil.%20Trans.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frstb.1971.0078&volume=262&pages=23-81&publication_year=1971&author=Marr%2CD)
    
4.  McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. _Psychol. Rev._ **102**, 419–457 (1995).
    
    [Article](https://doi.org/10.1037%2F0033-295X.102.3.419)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7624455)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Why%20there%20are%20complementary%20learning%20systems%20in%20the%20hippocampus%20and%20neocortex%3A%20insights%20from%20the%20successes%20and%20failures%20of%20connectionist%20models%20of%20learning%20and%20memory&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.102.3.419&volume=102&publication_year=1995&author=McClelland%2CJL&author=McNaughton%2CBL&author=O%E2%80%99Reilly%2CRC)
    
5.  Teyler, T. J. & DiScenna, P. The hippocampal memory indexing theory. _Behav. Neurosci._ **100**, 147–154 (1986).
    
    [Article](https://doi.org/10.1037%2F0735-7044.100.2.147)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=3008780)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20hippocampal%20memory%20indexing%20theory&journal=Behav.%20Neurosci.&doi=10.1037%2F0735-7044.100.2.147&volume=100&publication_year=1986&author=Teyler%2CTJ&author=DiScenna%2CP)
    
6.  Bartlett, F. C. _Remembering: A Study In Experimental and Social Psychology_ (Cambridge Univ. Press, 1932).
    
7.  Schacter, D. L. Constructive memory: past and future. _Dialogues Clin. Neurosci._ **14**, 7–18 (2012).
    
    [Article](https://doi.org/10.31887%2FDCNS.2012.14.1%2Fdschacter)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22577300)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3341652)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Constructive%20memory%3A%20past%20and%20future&journal=Dialogues%20Clin.%20Neurosci.&doi=10.31887%2FDCNS.2012.14.1%2Fdschacter&volume=14&publication_year=2012&author=Schacter%2CDL)
    
8.  Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. _J. Neurol. Neurosurg. Psychiatry_ **20**, 11–21 (1957).
    
    [Article](https://doi.org/10.1136%2Fjnnp.20.1.11)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=13406589)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC497229)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Loss%20of%20recent%20memory%20after%20bilateral%20hippocampal%20lesions&journal=J.%20Neurol.%20Neurosurg.%20Psychiatry&doi=10.1136%2Fjnnp.20.1.11&volume=20&publication_year=1957&author=Scoville%2CWB&author=Milner%2CB)
    
9.  Squire, L. R. & Alvarez, P. Retrograde amnesia and memory consolidation: a neurobiological perspective. _Curr. Opin. Neurobiol._ **5**, 169–177 (1995).
    
    [Article](https://doi.org/10.1016%2F0959-4388%2895%2980023-9)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2MXlsVGit7s%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7620304)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrograde%20amnesia%20and%20memory%20consolidation%3A%20a%20neurobiological%20perspective&journal=Curr.%20Opin.%20Neurobiol.&doi=10.1016%2F0959-4388%2895%2980023-9&volume=5&pages=169-177&publication_year=1995&author=Squire%2CLR&author=Alvarez%2CP)
    
10.  Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. _Proc. Natl Acad. Sci. USA_ **91**, 7041–7045 (1994).
    
    [Article](https://doi.org/10.1073%2Fpnas.91.15.7041)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2czhs1KgsQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8041742)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC44334)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation%20and%20the%20medial%20temporal%20lobe%3A%20a%20simple%20network%20model&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.91.15.7041&volume=91&pages=7041-7045&publication_year=1994&author=Alvarez%2CP&author=Squire%2CLR)
    
11.  Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. _Curr. Opin. Neurobiol._ **7**, 217–227 (1997).
    
    [Article](https://doi.org/10.1016%2FS0959-4388%2897%2980010-4)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9142752)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation%2C%20retrograde%20amnesia%20and%20the%20hippocampal%20complex&journal=Curr.%20Opin.%20Neurobiol.&doi=10.1016%2FS0959-4388%2897%2980010-4&volume=7&publication_year=1997&author=Nadel%2CL&author=Moscovitch%2CM)
    
12.  Wilson, M. A. & McNaughton, B. L. Reactivation of hippocampal ensemble memories during sleep. _Science_ **265**, 676–679 (1994).
    
    [Article](https://doi.org/10.1126%2Fscience.8036517)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2czhtVCltw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8036517)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reactivation%20of%20hippocampal%20ensemble%20memories%20during%20sleep&journal=Science&doi=10.1126%2Fscience.8036517&volume=265&pages=676-679&publication_year=1994&author=Wilson%2CMA&author=McNaughton%2CBL)
    
13.  Diba, K. & Buzsáki, G. Forward and reverse hippocampal place-cell sequences during ripples. _Nat. Neurosci._ **10**, 1241–1242 (2007).
    
    [Article](https://doi.org/10.1038%2Fnn1961)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtVOrsLjN)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17828259)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2039924)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Forward%20and%20reverse%20hippocampal%20place-cell%20sequences%20during%20ripples&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn1961&volume=10&pages=1241-1242&publication_year=2007&author=Diba%2CK&author=Buzs%C3%A1ki%2CG)
    
14.  Girardeau, G., Benchenane, K., Wiener, S. I., Buzsáki, G. & Zugaro, M. B. Selective suppression of hippocampal ripples impairs spatial memory. _Nat. Neurosci._ **12**, 1222–1223 (2009).
    
    [Article](https://doi.org/10.1038%2Fnn.2384)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1MXhtFaju7%2FF)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19749750)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Selective%20suppression%20of%20hippocampal%20ripples%20impairs%20spatial%20memory&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.2384&volume=12&pages=1222-1223&publication_year=2009&author=Girardeau%2CG&author=Benchenane%2CK&author=Wiener%2CSI&author=Buzs%C3%A1ki%2CG&author=Zugaro%2CMB)
    
15.  Ego-Stengel, V. & Wilson, M. A. Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat. _Hippocampus_ **20**, 1–10 (2010).
    
    [Article](https://doi.org/10.1002%2Fhipo.20707)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19816984)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2801761)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Disruption%20of%20ripple-associated%20hippocampal%20activity%20during%20rest%20impairs%20spatial%20learning%20in%20the%20rat&journal=Hippocampus&doi=10.1002%2Fhipo.20707&volume=20&pages=1-10&publication_year=2010&author=Ego-Stengel%2CV&author=Wilson%2CMA)
    
16.  Winocur, G. & Moscovitch, M. Memory transformation and systems consolidation. _J. Int. Neuropsychol. Soc._ **17**, 766–780 (2011).
    
    [Article](https://doi.org/10.1017%2FS1355617711000683)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21729403)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20transformation%20and%20systems%20consolidation&journal=J.%20Int.%20Neuropsychol.%20Soc.&doi=10.1017%2FS1355617711000683&volume=17&pages=766-780&publication_year=2011&author=Winocur%2CG&author=Moscovitch%2CM)
    
17.  Norman, Y., Raccah, O., Liu, S., Parvizi, J. & Malach, R. Hippocampal ripples and their coordinated dialogue with the default mode network during recent and remote recollection. _Neuron_ **109**, 2767–2780 (2021).
    
    [Article](https://doi.org/10.1016%2Fj.neuron.2021.06.020)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhs1WkurrO)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34297916)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8693710)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20ripples%20and%20their%20coordinated%20dialogue%20with%20the%20default%20mode%20network%20during%20recent%20and%20remote%20recollection&journal=Neuron&doi=10.1016%2Fj.neuron.2021.06.020&volume=109&pages=2767-2780&publication_year=2021&author=Norman%2CY&author=Raccah%2CO&author=Liu%2CS&author=Parvizi%2CJ&author=Malach%2CR)
    
18.  Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. _Adv. Neural Inf. Process. Syst._ **13**, 24–30 (2000).
    
19.  Káli, S. & Dayan, P. Replay, repair and consolidation. _Adv. Neural Inf. Process. Syst._ **15**, 19–26 (2002).
    
20.  Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. _Adv. Neural Inf. Process. Syst._ **13**, 96–102 (2000).
    
21.  Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. _Psychol. Rev._ **114**, 340–375 (2007).
    
    [Article](https://doi.org/10.1037%2F0033-295X.114.2.340)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17500630)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2678675)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20and%20imagining%20the%20future%3A%20a%20neural%20model%20of%20spatial%20memory%20and%20imagery&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.114.2.340&volume=114&publication_year=2007&author=Byrne%2CP&author=Becker%2CS&author=Burgess%2CN)
    
22.  Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. _Elife_ **7**, e33752 (2018).
    
    [Article](https://doi.org/10.7554%2FeLife.33752)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30176988)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6122954)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20neural-level%20model%20of%20spatial%20memory%20and%20imagery&journal=Elife&doi=10.7554%2FeLife.33752&volume=7&publication_year=2018&author=Bicanski%2CA&author=Burgess%2CN)
    
23.  Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. _Proc. Natl Acad. Sci. USA_ **104**, 1726–1731 (2007).
    
    [Article](https://doi.org/10.1073%2Fpnas.0610561104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhslGisbk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17229836)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1773058)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patients%20with%20hippocampal%20amnesia%20cannot%20imagine%20new%20experiences&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0610561104&volume=104&pages=1726-1731&publication_year=2007&author=Hassabis%2CD&author=Kumaran%2CD&author=Vann%2CSD&author=Maguire%2CEA)
    
24.  Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. _Curr. Opin. Behav. Sci._ **17**, 41–50 (2017).
    
    [Article](https://doi.org/10.1016%2Fj.cobeha.2017.06.002)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29130061)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5675579)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20future%20thinking%3A%20mechanisms%20and%20functions&journal=Curr.%20Opin.%20Behav.%20Sci.&doi=10.1016%2Fj.cobeha.2017.06.002&volume=17&pages=41-50&publication_year=2017&author=Schacter%2CDL&author=Benoit%2CRG&author=Szpunar%2CKK)
    
25.  Spanó, G. et al. Dreaming with hippocampal damage. _Elife_ **9**, e56211 (2020).
    
    [Article](https://doi.org/10.7554%2FeLife.56211)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32508305)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7279885)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Dreaming%20with%20hippocampal%20damage&journal=Elife&doi=10.7554%2FeLife.56211&volume=9&publication_year=2020&author=Span%C3%B3%2CG)
    
26.  McCormick, C., Rosenthal, C. R., Miller, T. D. & Maguire, E. A. Mind-wandering in people with hippocampal damage. _J. Neurosci._ **38**, 2745–2754 (2018).
    
    [Article](https://doi.org/10.1523%2FJNEUROSCI.1812-17.2018)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXit1Gjtr%2FL)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29440532)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5851780)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mind-wandering%20in%20people%20with%20hippocampal%20damage&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.1812-17.2018&volume=38&pages=2745-2754&publication_year=2018&author=McCormick%2CC&author=Rosenthal%2CCR&author=Miller%2CTD&author=Maguire%2CEA)
    
27.  Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. _Neuropsychologia_ **45**, 1363–1377 (2007).
    
    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2006.10.016)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17126370)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20and%20imagining%20the%20future%3A%20common%20and%20distinct%20neural%20substrates%20during%20event%20construction%20and%20elaboration&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2006.10.016&volume=45&pages=1363-1377&publication_year=2007&author=Addis%2CDR&author=Wong%2CAT&author=Schacter%2CDL)
    
28.  Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. _Trends Cogn. Sci._ **11**, 299–306 (2007).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2007.05.001)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17548229)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deconstructing%20episodic%20memory%20with%20construction&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2007.05.001&volume=11&pages=299-306&publication_year=2007&author=Hassabis%2CD&author=Maguire%2CEA)
    
29.  Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at [https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531) (2015).
    
30.  Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. _Nat. Neurosci._ **26**, 1438–1448 (2023).
    
31.  Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_ **183**, 1249–1263 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.cell.2020.10.024)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXitlClsLnE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33181068)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7707106)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20Tolman%E2%80%93Eichenbaum%20machine%3A%20unifying%20space%20and%20relational%20memory%20through%20generalization%20in%20the%20hippocampal%20formation&journal=Cell&doi=10.1016%2Fj.cell.2020.10.024&volume=183&pages=1249-1263&publication_year=2020&author=Whittington%2CJCR)
    
32.  Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. _PLoS Comput. Biol._ **16**, e1008367 (2020).
    
    [Article](https://doi.org/10.1371%2Fjournal.pcbi.1008367)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXit1amur3I)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33057380)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7591090)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Optimal%20forgetting%3A%20semantic%20compression%20of%20episodic%20memories&journal=PLoS%20Comput.%20Biol.&doi=10.1371%2Fjournal.pcbi.1008367&volume=16&publication_year=2020&author=Nagy%2CDG&author=T%C3%B6r%C3%B6k%2CB&author=Orb%C3%A1n%2CG)
    
33.  van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. _Nat. Commun._ **11**, 4069 (2020).
    
    [Article](https://doi.org/10.1038%2Fs41467-020-17866-2)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32792531)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426273)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Brain-inspired%20replay%20for%20continual%20learning%20with%20artificial%20neural%20networks&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-020-17866-2&volume=11&publication_year=2020&author=Ven%2CGM&author=Siegelmann%2CHT&author=Tolias%2CAS)
    
34.  Hemmer, P. & Steyvers, M. A Bayesian account of reconstructive memory. _Top. Cogn. Sci._ **1**, 189–202 (2009).
    
    [Article](https://doi.org/10.1111%2Fj.1756-8765.2008.01010.x)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25164805)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20Bayesian%20account%20of%20reconstructive%20memory&journal=Top.%20Cogn.%20Sci.&doi=10.1111%2Fj.1756-8765.2008.01010.x&volume=1&pages=189-202&publication_year=2009&author=Hemmer%2CP&author=Steyvers%2CM)
    
35.  Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. _Neural Comput._ **34**, 1841–1870 (2022).
    
    [Article](https://doi.org/10.1162%2Fneco_a_01520)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=35896150)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20model%20of%20semantic%20completion%20in%20generative%20episodic%20memory&journal=Neural%20Comput.&doi=10.1162%2Fneco_a_01520&volume=34&publication_year=2022&author=Fayyaz%2CZ)
    
36.  Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. _Nat. Rev. Neurosci._ **8**, 657–661 (2007).
    
    [Article](https://doi.org/10.1038%2Fnrn2213)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17700624)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20to%20imagine%20the%20future%3A%20the%20prospective%20brain&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn2213&volume=8&publication_year=2007&author=Schacter%2CDL&author=Addis%2CDR&author=Buckner%2CRL)
    
37.  Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. _Trends Cogn. Sci._ **24**, 542–556 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2020.04.004)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32513572)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20are%20memories%20for%3F%20The%20hippocampus%20bridges%20past%20experience%20with%20future%20decisions&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2020.04.004&volume=24&pages=542-556&publication_year=2020&author=Biderman%2CN&author=Bakkour%2CA&author=Shohamy%2CD)
    
38.  Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. _Learn. Mem._ **28**, 422–434 (2021).
    
    [Article](https://doi.org/10.1101%2Flm.053410.121)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34663695)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8525423)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mnemonic%20prediction%20errors%20promote%20detailed%20memories&journal=Learn.%20Mem.&doi=10.1101%2Flm.053410.121&volume=28&pages=422-434&publication_year=2021&author=Bein%2CO&author=Plotkin%2CNA&author=Davachi%2CL)
    
39.  Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. _J. Neurosci._ **42**, 9053–9068 (2022).
    
    [Article](https://doi.org/10.1523%2FJNEUROSCI.0708-22.2022)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XjtFKqtL7K)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=36344264)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9732826)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Temporal%20dynamics%20of%20competition%20between%20statistical%20learning%20and%20episodic%20memory%20in%20intracranial%20recordings%20of%20human%20visual%20cortex&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.0708-22.2022&volume=42&pages=9053-9068&publication_year=2022&author=Sherman%2CBE)
    
40.  Barlow, H. B. et al. in _Sensory Communication_ (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013).
    
41.  Barlow, H. B. Unsupervised learning. _Neural Comput._ **1**, 295–311 (1989).
    
    [Article](https://doi.org/10.1162%2Fneco.1989.1.3.295)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Unsupervised%20learning&journal=Neural%20Comput.&doi=10.1162%2Fneco.1989.1.3.295&volume=1&pages=295-311&publication_year=1989&author=Barlow%2CHB)
    
42.  Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. _Proc. Natl Acad. Sci. USA_ **118**, e2018422118 (2021).
    
43.  Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. _Science_ **277**, 376–380 (1997).
    
    [Article](https://doi.org/10.1126%2Fscience.277.5324.376)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2sXkvVels7Y%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9219696)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Differential%20effects%20of%20early%20hippocampal%20pathology%20on%20episodic%20and%20semantic%20memory&journal=Science&doi=10.1126%2Fscience.277.5324.376&volume=277&pages=376-380&publication_year=1997&author=Vargha-Khadem%2CF)
    
44.  Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. _Neuron_ **38**, 127–133 (2003).
    
    [Article](https://doi.org/10.1016%2FS0896-6273%2803%2900146-6)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD3sXjt1Gmtr0%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=12691670)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Semantic%20memory%20and%20the%20human%20hippocampus&journal=Neuron&doi=10.1016%2FS0896-6273%2803%2900146-6&volume=38&pages=127-133&publication_year=2003&author=Manns%2CJR&author=Hopkins%2CRO&author=Squire%2CLR)
    
45.  Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. _Cold Spring Harb. Perspect. Biol._ **7**, a021766 (2015).
    
    [Article](https://doi.org/10.1101%2Fcshperspect.a021766)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26238360)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4526749)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation&journal=Cold%20Spring%20Harb.%20Perspect.%20Biol.&doi=10.1101%2Fcshperspect.a021766&volume=7&publication_year=2015&author=Squire%2CLR&author=Genzel%2CL&author=Wixted%2CJT&author=Morris%2CRG)
    
46.  McKenzie, S. & Eichenbaum, H. Consolidation and reconsolidation: two lives of memories? _Neuron_ **71**, 224–233 (2011).
    
    [Article](https://doi.org/10.1016%2Fj.neuron.2011.06.037)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXps1OgsLs%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21791282)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3145971)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Consolidation%20and%20reconsolidation%3A%20two%20lives%20of%20memories%3F&journal=Neuron&doi=10.1016%2Fj.neuron.2011.06.037&volume=71&pages=224-233&publication_year=2011&author=McKenzie%2CS&author=Eichenbaum%2CH)
    
47.  Durrant, S. J., Taylor, C., Cairney, S. & Lewis, P. A. Sleep-dependent consolidation of statistical learning. _Neuropsychologia_ **49**, 1322–1331 (2011).
    
    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2011.02.015)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21335017)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Sleep-dependent%20consolidation%20of%20statistical%20learning&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2011.02.015&volume=49&pages=1322-1331&publication_year=2011&author=Durrant%2CSJ&author=Taylor%2CC&author=Cairney%2CS&author=Lewis%2CPA)
    
48.  Richards, B. A. et al. Patterns across multiple memories are identified over time. _Nat. Neurosci._ **17**, 981–986 (2014).
    
    [Article](https://doi.org/10.1038%2Fnn.3736)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXovFCgtL8%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=24880213)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patterns%20across%20multiple%20memories%20are%20identified%20over%20time&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.3736&volume=17&pages=981-986&publication_year=2014&author=Richards%2CBA)
    
49.  Ellenbogen, J. M., Hu, P. T., Payne, J. D., Titone, D. & Walker, M. P. Human relational memory requires time and sleep. _Proc. Natl Acad. Sci. USA_ **104**, 7723–7728 (2007).
    
    [Article](https://doi.org/10.1073%2Fpnas.0700094104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXlslahtrc%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17449637)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1863467)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20relational%20memory%20requires%20time%20and%20sleep&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0700094104&volume=104&pages=7723-7728&publication_year=2007&author=Ellenbogen%2CJM&author=Hu%2CPT&author=Payne%2CJD&author=Titone%2CD&author=Walker%2CMP)
    
50.  Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. _Trends Cogn. Sci._ **20**, 512–534 (2016).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2016.05.004)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27315762)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20learning%20systems%20do%20intelligent%20agents%20need%3F%20Complementary%20learning%20systems%20theory%20updated&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2016.05.004&volume=20&pages=512-534&publication_year=2016&author=Kumaran%2CD&author=Hassabis%2CD&author=McClelland%2CJL)
    
51.  Schapiro, A. C., Turk-Browne, N. B., Botvinick, M. M. & Norman, K. A. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. _Phil. Trans. R. Soc. B_ **372**, 20160049 (2017).
    
    [Article](https://doi.org/10.1098%2Frstb.2016.0049)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27872368)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124075)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Complementary%20learning%20systems%20within%20the%20hippocampus%3A%20a%20neural%20network%20modelling%20approach%20to%20reconciling%20episodic%20memory%20with%20statistical%20learning&journal=Phil.%20Trans.%20R.%20Soc.%20B&doi=10.1098%2Frstb.2016.0049&volume=372&publication_year=2017&author=Schapiro%2CAC&author=Turk-Browne%2CNB&author=Botvinick%2CMM&author=Norman%2CKA)
    
52.  Payne, J. D. et al. The role of sleep in false memory formation. _Neurobiol. Learn. Mem._ **92**, 327–334 (2009).
    
    [Article](https://doi.org/10.1016%2Fj.nlm.2009.03.007)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19348959)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2789473)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20role%20of%20sleep%20in%20false%20memory%20formation&journal=Neurobiol.%20Learn.%20Mem.&doi=10.1016%2Fj.nlm.2009.03.007&volume=92&pages=327-334&publication_year=2009&author=Payne%2CJD)
    
53.  Hafting, T., Fyhn, M., Molden, S., Moser, M. B. & Moser, E. I. Microstructure of a spatial map in the entorhinal cortex. _Nature_ **436**, 801–806 (2005).
    
    [Article](https://doi.org/10.1038%2Fnature03721)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXnt1Siurk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=15965463)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Microstructure%20of%20a%20spatial%20map%20in%20the%20entorhinal%20cortex&journal=Nature&doi=10.1038%2Fnature03721&volume=436&pages=801-806&publication_year=2005&author=Hafting%2CT&author=Fyhn%2CM&author=Molden%2CS&author=Moser%2CMB&author=Moser%2CEI)
    
54.  Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. _Science_ **352**, 1464–1468 (2016).
    
    [Article](https://doi.org/10.1126%2Fscience.aaf0941)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XpslOqsLw%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27313047)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5248972)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Organizing%20conceptual%20knowledge%20in%20humans%20with%20a%20gridlike%20code&journal=Science&doi=10.1126%2Fscience.aaf0941&volume=352&pages=1464-1468&publication_year=2016&author=Constantinescu%2CAO&author=O%E2%80%99Reilly%2CJX&author=Behrens%2CTE)
    
55.  Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. _Nat. Commun._ **11**, 46 (2020).
    
    [Article](https://doi.org/10.1038%2Fs41467-019-13930-8)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXmt1aisg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=31911628)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6946809)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%20compression%20during%20concept%20learning&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-019-13930-8&volume=11&publication_year=2020&author=Mack%2CML&author=Preston%2CAR&author=Love%2CBC)
    
56.  Hasselmo, M. E., Wyble, B. P. & Wallenstein, G. V. Encoding and retrieval of episodic memories: role of cholinergic and GABAergic modulation in the hippocampus. _Hippocampus_ **6**, 693–708 (1996).
    
    [Article](https://doi.org/10.1002%2F%28SICI%291098-1063%281996%296%3A6%3C693%3A%3AAID-HIPO12%3E3.0.CO%3B2-W)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2sXhsFCjt7g%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9034856)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Encoding%20and%20retrieval%20of%20episodic%20memories%3A%20role%20of%20cholinergic%20and%20GABAergic%20modulation%20in%20the%20hippocampus&journal=Hippocampus&doi=10.1002%2F%28SICI%291098-1063%281996%296%3A6%3C693%3A%3AAID-HIPO12%3E3.0.CO%3B2-W&volume=6&pages=693-708&publication_year=1996&author=Hasselmo%2CME&author=Wyble%2CBP&author=Wallenstein%2CGV)
    
57.  Tse, D. et al. Schemas and memory consolidation. _Science_ **316**, 76–82 (2007).
    
    [Article](https://doi.org/10.1126%2Fscience.1135935)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXjvVarsbk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17412951)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Schemas%20and%20memory%20consolidation&journal=Science&doi=10.1126%2Fscience.1135935&volume=316&pages=76-82&publication_year=2007&author=Tse%2CD)
    
58.  Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. _PLoS Biol._ **4**, e424 (2006).
    
    [Article](https://doi.org/10.1371%2Fjournal.pbio.0040424)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17132050)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1661685)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=An%20unexpected%20sequence%20of%20events%3A%20mismatch%20detection%20in%20the%20human%20hippocampus&journal=PLoS%20Biol.&doi=10.1371%2Fjournal.pbio.0040424&volume=4&publication_year=2006&author=Kumaran%2CD&author=Maguire%2CEA)
    
59.  Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. _Learn. Mem._ **18**, 523–528 (2011).
    
    [Article](https://doi.org/10.1101%2Flm.2135211)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21775513)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3256570)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Associative%20retrieval%20processes%20in%20the%20human%20medial%20temporal%20lobe%3A%20hippocampal%20retrieval%20success%20and%20CA1%20mismatch%20detection&journal=Learn.%20Mem.&doi=10.1101%2Flm.2135211&volume=18&pages=523-528&publication_year=2011&author=Chen%2CJ&author=Olsen%2CRK&author=Preston%2CAR&author=Glover%2CGH&author=Wagner%2CAD)
    
60.  Hedayati, S., O’Donnell, R. E. & Wyble, B. A model of working memory for latent representations. _Nat. Hum. Behav._ **6**, 709–719 (2022).
    
    [Article](https://doi.org/10.1038%2Fs41562-021-01264-9)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=35115675)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20model%20of%20working%20memory%20for%20latent%20representations&journal=Nat.%20Hum.%20Behav.&doi=10.1038%2Fs41562-021-01264-9&volume=6&publication_year=2022&author=Hedayati%2CS&author=O%E2%80%99Donnell%2CRE&author=Wyble%2CB)
    
61.  Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. _Nat. Rev. Neurosci._ **13**, 587–597 (2012).
    
    [Article](https://doi.org/10.1038%2Fnrn3251)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XpsFOrtbw%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22760181)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3251&volume=13&pages=587-597&publication_year=2012&author=Quiroga%2CRQ)
    
62.  Kolibius, L. D. et al. Hippocampal neurons code individual episodic memories in humans. _Nat. Hum. Behav_. **7**, 1968–1979 (2023).
    
63.  Ramsauer, H. et al. Hopfield networks is all you need. in _International Conference on Learning Representations_ (2021).
    
64.  Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in _International Conference on Learning Representations_ (2021).
    
65.  Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114) (2013).
    
66.  Kingma, D. P. & Welling, M. An introduction to variational autoencoders. _Found. Trends Mach. Learn._ **12**, 307–392 (2019).
    
67.  Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. _Neural Comput._ **7**, 889–904 (1995).
    
    [Article](https://doi.org/10.1162%2Fneco.1995.7.5.889)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK28%2FitVKrtA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7584891)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20Helmholtz%20machine&journal=Neural%20Comput.&doi=10.1162%2Fneco.1995.7.5.889&volume=7&pages=889-904&publication_year=1995&author=Dayan%2CP&author=Hinton%2CGE&author=Neal%2CRM&author=Zemel%2CRS)
    
68.  Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nat. Neurosci._ **2**, 79–87 (1999).
    
    [Article](https://doi.org/10.1038%2F4580)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=10195184)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Predictive%20coding%20in%20the%20visual%20cortex%3A%20a%20functional%20interpretation%20of%20some%20extra-classical%20receptive-field%20effects&journal=Nat.%20Neurosci.&doi=10.1038%2F4580&volume=2&publication_year=1999&author=Rao%2CRPN&author=Ballard%2CDH)
    
69.  Friston, K. The free-energy principle: a unified brain theory? _Nat. Rev. Neurosci._ **11**, 127–138 (2010).
    
    [Article](https://doi.org/10.1038%2Fnrn2787)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20068583)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20free-energy%20principle%3A%20a%20unified%20brain%20theory%3F&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn2787&volume=11&publication_year=2010&author=Friston%2CK)
    
70.  Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. _Trends Cogn. Sci._ **21**, 618–631 (2017).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2017.04.013)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=28551107)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Neurobiology%20of%20schemas%20and%20schema-mediated%20memory&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2017.04.013&volume=21&pages=618-631&publication_year=2017&author=Gilboa%2CA&author=Marlatte%2CH)
    
71.  Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. _Neuropsychologia_ **53**, 104–114 (2014).
    
    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2013.11.010)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=24280650)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20is%20a%20memory%20schema%3F%20A%20historical%20perspective%20on%20current%20neuroscience%20literature&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2013.11.010&volume=53&pages=104-114&publication_year=2014&author=Ghosh%2CVE&author=Gilboa%2CA)
    
72.  Chambers, D. & Reisberg, D. Can mental images be ambiguous? _J. Exp. Psychol. Hum. Percept. Perform._ **11**, 317–328 (1985).
    
    [Article](https://doi.org/10.1037%2F0096-1523.11.3.317)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Can%20mental%20images%20be%20ambiguous%3F&journal=J.%20Exp.%20Psychol.%20Hum.%20Percept.%20Perform.&doi=10.1037%2F0096-1523.11.3.317&volume=11&publication_year=1985&author=Chambers%2CD&author=Reisberg%2CD)
    
73.  Moser, E. I., Kropff, E. & Moser, M. B. Place cells, grid cells, and the brain’s spatial representation system. _Annu. Rev. of Neurosci._ **31**, 69–89 (2008).
    
    [Article](https://doi.org/10.1146%2Fannurev.neuro.31.061307.090723)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXpt12nsr8%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Place%20cells%2C%20grid%20cells%2C%20and%20the%20brain%E2%80%99s%20spatial%20representation%20system&journal=Annu.%20Rev.%20of%20Neurosci.&doi=10.1146%2Fannurev.neuro.31.061307.090723&volume=31&pages=69-89&publication_year=2008&author=Moser%2CEI&author=Kropff%2CE&author=Moser%2CMB)
    
74.  Takashima, A. et al. Declarative memory consolidation in humans: a prospective functional magnetic resonance imaging study. _Proc. Natl Acad. Sci. USA_ **103**, 756–761 (2006).
    
    [Article](https://doi.org/10.1073%2Fpnas.0507774103)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD28XhtVOhs74%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=16407110)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334654)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Declarative%20memory%20consolidation%20in%20humans%3A%20a%20prospective%20functional%20magnetic%20resonance%20imaging%20study&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0507774103&volume=103&pages=756-761&publication_year=2006&author=Takashima%2CA)
    
75.  Gais, S. et al. Sleep transforms the cerebral trace of declarative memories. _Proc. Natl Acad. Sci. USA_ **104**, 18778–18783 (2007).
    
    [Article](https://doi.org/10.1073%2Fpnas.0705454104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtl2ltbfE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=18000060)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2141853)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Sleep%20transforms%20the%20cerebral%20trace%20of%20declarative%20memories&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0705454104&volume=104&pages=18778-18783&publication_year=2007&author=Gais%2CS)
    
76.  Frankland, P. W. & Bontempi, B. The organization of recent and remote memories. _Nat. Rev. Neurosci._ **6**, 119–130 (2005).
    
    [Article](https://doi.org/10.1038%2Fnrn1607)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXoslOjsg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=15685217)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20organization%20of%20recent%20and%20remote%20memories&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn1607&volume=6&pages=119-130&publication_year=2005&author=Frankland%2CPW&author=Bontempi%2CB)
    
77.  van Kesteren, M. T. R., Fernández, G., Norris, D. G. & Hermans, E. J. Persistent schema-dependent hippocampal-neocortical connectivity during memory encoding and postencoding rest in humans. _Proc. Natl Acad. Sci. USA_ **107**, 7550–7555 (2010).
    
    [Article](https://doi.org/10.1073%2Fpnas.0914892107)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20363957)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2867741)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Persistent%20schema-dependent%20hippocampal-neocortical%20connectivity%20during%20memory%20encoding%20and%20postencoding%20rest%20in%20humans&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0914892107&volume=107&pages=7550-7555&publication_year=2010&author=Kesteren%2CMTR&author=Fern%C3%A1ndez%2CG&author=Norris%2CDG&author=Hermans%2CEJ)
    
78.  Benchenane, K. et al. Coherent theta oscillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning. _Neuron_ **66**, 921–936 (2010).
    
    [Article](https://doi.org/10.1016%2Fj.neuron.2010.05.013)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXovFeiur0%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20620877)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Coherent%20theta%20oscillations%20and%20reorganization%20of%20spike%20timing%20in%20the%20hippocampal-prefrontal%20network%20upon%20learning&journal=Neuron&doi=10.1016%2Fj.neuron.2010.05.013&volume=66&pages=921-936&publication_year=2010&author=Benchenane%2CK)
    
79.  Koscik, T. R. & Tranel, D. The human ventromedial prefrontal cortex is critical for transitive inference. _J. Cogn. Neurosci._ **24**, 1191–1204 (2012).
    
    [Article](https://doi.org/10.1162%2Fjocn_a_00203)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22288395)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3626083)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20human%20ventromedial%20prefrontal%20cortex%20is%20critical%20for%20transitive%20inference&journal=J.%20Cogn.%20Neurosci.&doi=10.1162%2Fjocn_a_00203&volume=24&pages=1191-1204&publication_year=2012&author=Koscik%2CTR&author=Tranel%2CD)
    
80.  Spalding, K. N. et al. Ventromedial prefrontal cortex is necessary for normal associative inference and memory integration. _J. Neurosci._ **38**, 3767–3775 (2018).
    
    [Article](https://doi.org/10.1523%2FJNEUROSCI.2501-17.2018)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29555854)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5895999)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%20is%20necessary%20for%20normal%20associative%20inference%20and%20memory%20integration&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.2501-17.2018&volume=38&publication_year=2018&author=Spalding%2CKN)
    
81.  Chan, D. et al. Patterns of temporal lobe atrophy in semantic dementia and Alzheimer’s disease. _Ann. Neurol._ **49**, 433–442 (2001).
    
    [Article](https://doi.org/10.1002%2Fana.92)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3hvVyltA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11310620)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patterns%20of%20temporal%20lobe%20atrophy%20in%20semantic%20dementia%20and%20Alzheimer%E2%80%99s%20disease&journal=Ann.%20Neurol.&doi=10.1002%2Fana.92&volume=49&pages=433-442&publication_year=2001&author=Chan%2CD)
    
82.  Bright, P. et al. Retrograde amnesia in patients with hippocampal, medial temporal, temporal lobe, or frontal pathology. _Learn. Mem._ **13**, 545–557 (2006).
    
    [Article](https://doi.org/10.1101%2Flm.265906)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17015852)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1783611)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrograde%20amnesia%20in%20patients%20with%20hippocampal%2C%20medial%20temporal%2C%20temporal%20lobe%2C%20or%20frontal%20pathology&journal=Learn.%20Mem.&doi=10.1101%2Flm.265906&volume=13&pages=545-557&publication_year=2006&author=Bright%2CP)
    
83.  Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. _Nat. Rev. Neurosci._ **13**, 713–726 (2012).
    
    [Article](https://doi.org/10.1038%2Fnrn3338)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22992647)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Two%20cortical%20systems%20for%20memory-guided%20behaviour&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3338&volume=13&publication_year=2012&author=Ranganath%2CC&author=Ritchey%2CM)
    
84.  Spiers, H. J., Maguire, E. A. & Burgess, N. Hippocampal amnesia. _Neurocase_ **7**, 357–382 (2001).
    
    [Article](https://doi.org/10.1076%2Fneur.7.5.357.16245)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11744778)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20amnesia&journal=Neurocase&doi=10.1076%2Fneur.7.5.357.16245&volume=7&publication_year=2001&author=Spiers%2CHJ&author=Maguire%2CEA&author=Burgess%2CN)
    
85.  Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in _Proc. IEEE Conference on Computer Vision and Pattern Recognition_ 3156–3164 (2015).
    
86.  Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at [https://arxiv.org/abs/2111.09734](https://arxiv.org/abs/2111.09734) (2021).
    
87.  LeCun, Y., Cortes, C. & Burges, C. J. _MNIST Handwritten Digit Database_ (AT&T Labs, 2010).
    
88.  Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. _J. Exp. Psychol. Learn. Mem. Cogn._ **15**, 179–187 (1989).
    
    [Article](https://doi.org/10.1037%2F0278-7393.15.2.179)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=2522508)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Wide-angle%20memories%20of%20close-up%20scenes&journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&doi=10.1037%2F0278-7393.15.2.179&volume=15&publication_year=1989&author=Intraub%2CH&author=Richardson%2CM)
    
89.  Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. _Curr. Biol._ **30**, 537–543 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.cub.2019.12.004)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhvVShsrk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=31983637)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7187786)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Boundaries%20extend%20and%20contract%20in%20scene%20memory%20depending%20on%20image%20properties&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2019.12.004&volume=30&pages=537-543&publication_year=2020&author=Bainbridge%2CWA&author=Baker%2CCI)
    
90.  Intraub, H. Searching for boundary extension. _Curr. Biol._ **30**, R1463–R1464 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.cub.2020.10.031)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXktlyktQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33352122)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Searching%20for%20boundary%20extension&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2020.10.031&volume=30&pages=R1463-R1464&publication_year=2020&author=Intraub%2CH)
    
91.  Bainbridge, W. A. & Baker, C. I. Reply to Intraub. _Curr. Biol._ **30**, R1465–R1466 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.cub.2020.10.032)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXktlCiuw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33352123)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reply%20to%20Intraub&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2020.10.032&volume=30&pages=R1465-R1466&publication_year=2020&author=Bainbridge%2CWA&author=Baker%2CCI)
    
92.  Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. _J. Vis._ [https://doi.org/10.1167/jov.21.9.2124](https://doi.org/10.1167/jov.21.9.2124) (2021).
    
93.  Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. _J. Exp. Psychol._ **58**, 17–22 (1959).
    
    [Article](https://doi.org/10.1037%2Fh0046671)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=13664879)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=On%20the%20prediction%20of%20occurrence%20of%20particular%20verbal%20intrusions%20in%20immediate%20recall&journal=J.%20Exp.%20Psychol.&doi=10.1037%2Fh0046671&volume=58&publication_year=1959&author=Deese%2CJ)
    
94.  Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. _J. Exp. Psychol. Lear. Mem. Cogn._ **21**, 803–814 (1995).
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Creating%20false%20memories%3A%20remembering%20words%20not%20presented%20in%20lists&journal=J.%20Exp.%20Psychol.%20Lear.%20Mem.%20Cogn.&volume=21&publication_year=1995&author=Roediger%2CHL&author=McDermott%2CKB)
    
95.  Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. _J. Exp. Psychol._ **15**, 73–86 (1932).
    
    [Article](https://doi.org/10.1037%2Fh0072671)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=An%20experimental%20study%20of%20the%20effect%20of%20language%20on%20the%20reproduction%20of%20visually%20perceived%20form&journal=J.%20Exp.%20Psychol.&doi=10.1037%2Fh0072671&volume=15&publication_year=1932&author=Carmichael%2CL&author=Hogan%2CHP&author=Walter%2CAA)
    
96.  Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in _Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_ (eds Knight, K. et al.) 839–849 (2016).
    
97.  Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. _Psychol. Sci._ **8**, 231–237 (1997).
    
    [Article](https://doi.org/10.1111%2Fj.1467-9280.1997.tb00417.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Associative%20processes%20in%20false%20recall%20and%20false%20recognition&journal=Psychol.%20Sci.&doi=10.1111%2Fj.1467-9280.1997.tb00417.x&volume=8&pages=231-237&publication_year=1997&author=Robinson%2CKJ&author=Roediger%2CHL)
    
98.  Cipolotti, L. et al. Long-term retrograde amnesia… the crucial role of the hippocampus. _Neuropsychologia_ **39**, 151–172 (2001).
    
    [Article](https://doi.org/10.1016%2FS0028-3932%2800%2900103-2)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3htFGitw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11163373)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Long-term%20retrograde%20amnesia%E2%80%A6%20the%20crucial%20role%20of%20the%20hippocampus&journal=Neuropsychologia&doi=10.1016%2FS0028-3932%2800%2900103-2&volume=39&pages=151-172&publication_year=2001&author=Cipolotti%2CL)
    
99.  Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. _J. Neurosci._ **6**, 2950–2967 (1986).
    
    [Article](https://doi.org/10.1523%2FJNEUROSCI.06-10-02950.1986)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL2s%2FhtVyjtg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=3760943)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568782)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20amnesia%20and%20the%20medial%20temporal%20region%3A%20enduring%20memory%20impairment%20following%20a%20bilateral%20lesion%20limited%20to%20field%20CA1%20of%20the%20hippocampus&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.06-10-02950.1986&volume=6&pages=2950-2967&publication_year=1986&author=Zola-Morgan%2CS&author=Squire%2CLR&author=Amaral%2CDG)
    
100.  Knowlton, B. J., Squire, L. R. & Gluck, M. A. Probabilistic classification learning in amnesia. _Learn. Mem._ **1**, 106–120 (1994).
    
    [Article](https://doi.org/10.1101%2Flm.1.2.106)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK1Mzps12hsg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=10467589)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Probabilistic%20classification%20learning%20in%20amnesia&journal=Learn.%20Mem.&doi=10.1101%2Flm.1.2.106&volume=1&pages=106-120&publication_year=1994&author=Knowlton%2CBJ&author=Squire%2CLR&author=Gluck%2CMA)
    
101.  Hodges, J. R. & Graham, K. S. Episodic memory: insights from semantic dementia. _Phil. Trans. R. Soc. Lond. B_ **356**, 1423–1434 (2001).
    
    [Article](https://doi.org/10.1098%2Frstb.2001.0943)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3MrisVeksQ%3D%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20memory%3A%20insights%20from%20semantic%20dementia&journal=Phil.%20Trans.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frstb.2001.0943&volume=356&pages=1423-1434&publication_year=2001&author=Hodges%2CJR&author=Graham%2CKS)
    
102.  Migo, E., Montaldi, D., Norman, K. A., Quamme, J. & Mayes, A. The contribution of familiarity to recognition memory is a function of test format when using similar foils. _Q. J.Exp. Psychol._ **62**, 1198–1215 (2009).
    
    [Article](https://doi.org/10.1080%2F17470210802391599)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20contribution%20of%20familiarity%20to%20recognition%20memory%20is%20a%20function%20of%20test%20format%20when%20using%20similar%20foils&journal=Q.%20J.Exp.%20Psychol.&doi=10.1080%2F17470210802391599&volume=62&publication_year=2009&author=Migo%2CE&author=Montaldi%2CD&author=Norman%2CKA&author=Quamme%2CJ&author=Mayes%2CA)
    
103.  Moscovitch, M. & Melo, B. Strategic retrieval and the frontal lobes: evidence from confabulation and amnesia. _Neuropsychologia_ **35**, 1017–1034 (1997).
    
    [Article](https://doi.org/10.1016%2FS0028-3932%2897%2900028-6)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2szntVaquw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9226662)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Strategic%20retrieval%20and%20the%20frontal%20lobes%3A%20evidence%20from%20confabulation%20and%20amnesia&journal=Neuropsychologia&doi=10.1016%2FS0028-3932%2897%2900028-6&volume=35&pages=1017-1034&publication_year=1997&author=Moscovitch%2CM&author=Melo%2CB)
    
104.  Lin, W. J., Horner, A. J. & Burgess, N. Ventromedial prefrontal cortex, adding value to autobiographical memories. _Sci. Rep._ **6**, 28630 (2016).
    
    [Article](https://doi.org/10.1038%2Fsrep28630)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhtVKitbfE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27338616)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919650)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%2C%20adding%20value%20to%20autobiographical%20memories&journal=Sci.%20Rep.&doi=10.1038%2Fsrep28630&volume=6&publication_year=2016&author=Lin%2CWJ&author=Horner%2CAJ&author=Burgess%2CN)
    
105.  Gluck, M. A. & Myers, C. E. Hippocampal mediation of stimulus representation: a computational theory. _Hippocampus_ **3**, 491–516 (1993).
    
    [Article](https://doi.org/10.1002%2Fhipo.450030410)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2c%2FptlKrtw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8269040)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20mediation%20of%20stimulus%20representation%3A%20a%20computational%20theory&journal=Hippocampus&doi=10.1002%2Fhipo.450030410&volume=3&pages=491-516&publication_year=1993&author=Gluck%2CMA&author=Myers%2CCE)
    
106.  Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. & Lipson, H. Understanding neural networks through deep visualization. Preprint at [https://arxiv.org/abs/1506.06579](https://arxiv.org/abs/1506.06579) (2015).
    
107.  Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with CLIP latents. Preprint at [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125) (2022).
    
108.  O’Keefe, J. & Dostrovsky, J. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. _Brain Res._ **34**, 171–175 (1971).
    
109.  Ekstrom, A. D. et al. Human hippocampal theta activity during virtual navigation. _Hippocampus_ **15**, 881–889 (2005).
    
    [Article](https://doi.org/10.1002%2Fhipo.20109)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=16114040)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20hippocampal%20theta%20activity%20during%20virtual%20navigation&journal=Hippocampus&doi=10.1002%2Fhipo.20109&volume=15&publication_year=2005&author=Ekstrom%2CAD)
    
110.  Eichenbaum, H. Time cells in the hippocampus: a new dimension for mapping memories. _Nat. Rev. Neurosci._ **15**, 732–744 (2014).
    
    [Article](https://doi.org/10.1038%2Fnrn3827)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25269553)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348090)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Time%20cells%20in%20the%20hippocampus%3A%20a%20new%20dimension%20for%20mapping%20memories&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3827&volume=15&publication_year=2014&author=Eichenbaum%2CH)
    
111.  Umbach, G. et al. Time cells in the human hippocampus and entorhinal cortex support episodic memory. _Proc. Natl Acad. Sci. USA_ **117**, 28463–28474 (2020).
    
    [Article](https://doi.org/10.1073%2Fpnas.2013250117)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33109718)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7668099)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Time%20cells%20in%20the%20human%20hippocampus%20and%20entorhinal%20cortex%20support%20episodic%20memory&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2013250117&volume=117&publication_year=2020&author=Umbach%2CG)
    
112.  Dordek, Y., Soudry, D., Meir, R. & Derdikman, D. Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis. _Elife_ **5**, e10094 (2016).
    
    [Article](https://doi.org/10.7554%2FeLife.10094)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26952211)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4841785)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Extracting%20grid%20cell%20characteristics%20from%20place%20cell%20inputs%20using%20non-negative%20principal%20component%20analysis&journal=Elife&doi=10.7554%2FeLife.10094&volume=5&publication_year=2016&author=Dordek%2CY&author=Soudry%2CD&author=Meir%2CR&author=Derdikman%2CD)
    
113.  Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippocampus as a predictive map. _Nat. Neurosci._ **20**, 1643–1653 (2017).
    
    [Article](https://doi.org/10.1038%2Fnn.4650)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=28967910)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20hippocampus%20as%20a%20predictive%20map&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.4650&volume=20&publication_year=2017&author=Stachenfeld%2CKL&author=Botvinick%2CMM&author=Gershman%2CSJ)
    
114.  Tsao, A. et al. Integrating time from experience in the lateral entorhinal cortex. _Nature_ **561**, 57–62 (2018).
    
    [Article](https://doi.org/10.1038%2Fs41586-018-0459-6)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30158699)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Integrating%20time%20from%20experience%20in%20the%20lateral%20entorhinal%20cortex&journal=Nature&doi=10.1038%2Fs41586-018-0459-6&volume=561&publication_year=2018&author=Tsao%2CA)
    
115.  Bright, I. M. et al. A temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex. _Proc. Natl Acad. Sci. USA_ **117**, 20274–20283 (2020).
    
    [Article](https://doi.org/10.1073%2Fpnas.1917197117)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32747574)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7443936)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20temporal%20record%20of%20the%20past%20with%20a%20spectrum%20of%20time%20constants%20in%20the%20monkey%20entorhinal%20cortex&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.1917197117&volume=117&publication_year=2020&author=Bright%2CIM)
    
116.  Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. _J. Math. Psychol._ **46**, 269–299 (2002).
    
    [Article](https://doi.org/10.1006%2Fjmps.2001.1388)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20distributed%20representation%20of%20temporal%20context&journal=J.%20Math.%20Psychol.&doi=10.1006%2Fjmps.2001.1388&volume=46&publication_year=2002&author=Howard%2CMW&author=Kahana%2CMJ)
    
117.  Moscovitch, M., Cabeza, R., Winocur, G. & Nadel, L. Episodic memory and beyond: the hippocampus and neocortex in transformation. _Annu. Review Psychol._ **67**, 105–134 (2016).
    
    [Article](https://doi.org/10.1146%2Fannurev-psych-113011-143733)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20memory%20and%20beyond%3A%20the%20hippocampus%20and%20neocortex%20in%20transformation&journal=Annu.%20Review%20Psychol.&doi=10.1146%2Fannurev-psych-113011-143733&volume=67&pages=105-134&publication_year=2016&author=Moscovitch%2CM&author=Cabeza%2CR&author=Winocur%2CG&author=Nadel%2CL)
    
118.  Strange, B. A., Witter, M. P., Lein, E. S. & Moser, E. I. Functional organization of the hippocampal longitudinal axis. _Nat. Rev. Neurosci._ **15**, 655–669 (2014).
    
    [Article](https://doi.org/10.1038%2Fnrn3785)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25234264)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Functional%20organization%20of%20the%20hippocampal%20longitudinal%20axis&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3785&volume=15&publication_year=2014&author=Strange%2CBA&author=Witter%2CMP&author=Lein%2CES&author=Moser%2CEI)
    
119.  Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. _Nat. Neurosci._ **7**, 286–294 (2004).
    
    [Article](https://doi.org/10.1038%2Fnn1202)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=14983183)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Off-line%20replay%20maintains%20declarative%20memories%20in%20a%20model%20of%20hippocampal%E2%80%93neocortical%20interactions&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn1202&volume=7&pages=286-294&publication_year=2004&author=K%C3%A1li%2CS&author=Dayan%2CP)
    
120.  van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at [https://arxiv.org/abs/1809.10635](https://arxiv.org/abs/1809.10635) (2018).
    
121.  Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. _Proc. Natl Acad. Sci. USA_ **119**, e2123432119 (2022).
    
122.  Millidge, B., Salvatori, T., Song, Y., Lukasiewicz, T. & Bogacz, R. Universal Hopfield networks: a general framework for single-shot associative memory models. in _International Conference on Machine Learning_ 15561–15583 (PMLR, 2022).
    
123.  Chaudhry, H. T., Zavatone-Veth, J. A., Krotov, D. & Pehlevan, C. Long sequence Hopfield memory. Preprint at [https://arxiv.org/abs/2306.04532](https://arxiv.org/abs/2306.04532) (2023).
    
124.  Tang, M., Barron, H. & Bogacz, R. Sequential memory with temporal predictive coding. _Adv. Neural Inf. Process. Syst._ **27** (2023).
    
125.  Burgess, N. & Hitch, G. J. Memory for serial order: a network model of the phonological loop and its timing. _Psychol. Rev._ **106**, 551–581 (1999).
    
    [Article](https://doi.org/10.1037%2F0033-295X.106.3.551)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20for%20serial%20order%3A%20a%20network%20model%20of%20the%20phonological%20loop%20and%20its%20timing&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.106.3.551&volume=106&publication_year=1999&author=Burgess%2CN&author=Hitch%2CGJ)
    
126.  Radford, A. et al. Language models are unsupervised multitask learners. _OpenAI Blog_ [https://cdn.openai.com/better-language-models/language\_models\_are\_unsupervised\_multitask\_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019).
    
127.  Bird, C. M. How do we remember events? _Curr. Opin. Behav. Sci._ **32**, 120–125 (2020).
    
    [Article](https://doi.org/10.1016%2Fj.cobeha.2020.01.020)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20do%20we%20remember%20events%3F&journal=Curr.%20Opin.%20Behav.%20Sci.&doi=10.1016%2Fj.cobeha.2020.01.020&volume=32&pages=120-125&publication_year=2020&author=Bird%2CCM)
    
128.  Mullally, S. L., Intraub, H. & Maguire, E. A. Attenuated boundary extension produces a paradoxical memory advantage in amnesic patients. _Curr. Biol._ **22**, 261–268 (2012).
    
    [Article](https://doi.org/10.1016%2Fj.cub.2012.01.001)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XitFKjs70%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22264610)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3315012)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Attenuated%20boundary%20extension%20produces%20a%20paradoxical%20memory%20advantage%20in%20amnesic%20patients&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2012.01.001&volume=22&pages=261-268&publication_year=2012&author=Mullally%2CSL&author=Intraub%2CH&author=Maguire%2CEA)
    
129.  De Luca, F. et al. Boundary extension is attenuated in patients with ventromedial prefrontal cortex damage. _Cortex_ **108**, 1–12 (2018).
    
    [Article](https://doi.org/10.1016%2Fj.cortex.2018.07.002)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30086391)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6238077)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Boundary%20extension%20is%20attenuated%20in%20patients%20with%20ventromedial%20prefrontal%20cortex%20damage&journal=Cortex&doi=10.1016%2Fj.cortex.2018.07.002&volume=108&pages=1-12&publication_year=2018&author=Luca%2CF)
    
130.  Van Der Kolk, B. A., Burbridge, J. A. & Suzuki, J. The psychobiology of traumatic memory. Clinical implications of neuroimaging studies. _Ann. N. Y. Acad. Sci._ **821**, 99–113 (1997).
    
131.  Bisby, J. A., Burgess, N. & Brewin, C. R. Reduced memory coherence for negative events and its relationship to posttraumatic stress disorder. _Curr. Dir. Psychol. Sci._ **29**, 267–272 (2020).
    
    [Article](https://doi.org/10.1177%2F0963721420917691)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33214741)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7643751)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reduced%20memory%20coherence%20for%20negative%20events%20and%20its%20relationship%20to%20posttraumatic%20stress%20disorder&journal=Curr.%20Dir.%20Psychol.%20Sci.&doi=10.1177%2F0963721420917691&volume=29&pages=267-272&publication_year=2020&author=Bisby%2CJA&author=Burgess%2CN&author=Brewin%2CCR)
    
132.  Pellicano, E. & Burr, D. When the world becomes ‘too real’: a Bayesian explanation of autistic perception. _Trends Cogn. Sci._ **16**, 504–510 (2012).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2012.08.009)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22959875)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=When%20the%20world%20becomes%20%E2%80%98too%20real%E2%80%99%3A%20a%20Bayesian%20explanation%20of%20autistic%20perception&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2012.08.009&volume=16&pages=504-510&publication_year=2012&author=Pellicano%2CE&author=Burr%2CD)
    
133.  Barry, D. N. & Love, B. C. A neural network account of memory replay and knowledge consolidation. _Cereb. Cortex._ **33**, 83–95 (2022).
    
134.  Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. _Neuron_ **100**, 490–509 (2018).
    
    [Article](https://doi.org/10.1016%2Fj.neuron.2018.10.002)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVSisrnK)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30359611)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20is%20a%20cognitive%20map%3F%20Organizing%20knowledge%20for%20flexible%20behavior&journal=Neuron&doi=10.1016%2Fj.neuron.2018.10.002&volume=100&pages=490-509&publication_year=2018&author=Behrens%2CTE)
    
135.  Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. _Nature_ **595**, 80–84 (2021).
    
    [Article](https://doi.org/10.1038%2Fs41586-021-03652-7)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhtleqs7jF)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34135512)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9549979)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Geometry%20of%20abstract%20learned%20knowledge%20in%20the%20hippocampus&journal=Nature&doi=10.1038%2Fs41586-021-03652-7&volume=595&pages=80-84&publication_year=2021&author=Nieh%2CEH)
    
136.  Burgess, C. & Kim, H. 3D Shapes dataset_._ _GitHub_ [https://github.com/google-deepmind/3d-shapes](https://github.com/google-deepmind/3d-shapes) (2018).
    
137.  Hou, X., Shen, L., Sun, K. & Qiu, G. Deep feature consistent variational autoencoder. in _2017 IEEE Winter Conference on Applications of Computer Vision (WACV)_ 1133–1141 (IEEE, 2017).
    
138.  Stella, F., Baracskay, P., O’Neill, J. & Csicsvari, J. Hippocampal reactivation of random trajectories resembling Brownian diffusion. _Neuron_ **102**, 450–461 (2019).
    
    [Article](https://doi.org/10.1016%2Fj.neuron.2019.01.052)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXjs1yitL4%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30819547)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20reactivation%20of%20random%20trajectories%20resembling%20Brownian%20diffusion&journal=Neuron&doi=10.1016%2Fj.neuron.2019.01.052&volume=102&pages=450-461&publication_year=2019&author=Stella%2CF&author=Baracskay%2CP&author=O%E2%80%99Neill%2CJ&author=Csicsvari%2CJ)
    
139.  González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E. & Bazhenov, M. Can sleep protect memories from catastrophic forgetting? _Elife_ **9**, e51005 (2020).
    
    [Article](https://doi.org/10.7554%2FeLife.51005)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32748786)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7440920)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Can%20sleep%20protect%20memories%20from%20catastrophic%20forgetting%3F&journal=Elife&doi=10.7554%2FeLife.51005&volume=9&publication_year=2020&author=Gonz%C3%A1lez%2COC&author=Sokolov%2CY&author=Krishnan%2CGP&author=Delanois%2CJE&author=Bazhenov%2CM)
    
140.  Pezzulo, G., Zorzi, M. & Corbetta, M. The secret life of predictive brains: what’s spontaneous activity for? _Trends Cogn. Sci._ **25**, 730–743 (2021).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2021.05.007)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34144895)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8363551)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20secret%20life%20of%20predictive%20brains%3A%20what%E2%80%99s%20spontaneous%20activity%20for%3F&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2021.05.007&volume=25&pages=730-743&publication_year=2021&author=Pezzulo%2CG&author=Zorzi%2CM&author=Corbetta%2CM)
    
141.  Igata, H., Ikegaya, Y. & Sasaki, T. Prioritized experience replays on a hippocampal predictive map for learning. _Proc. Natl Acad. Sci. USA_ **118**, e2011266118 (2021).
    
    [Article](https://doi.org/10.1073%2Fpnas.2011266118)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXnsVOrsA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33443144)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Prioritized%20experience%20replays%20on%20a%20hippocampal%20predictive%20map%20for%20learning&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2011266118&volume=118&publication_year=2021&author=Igata%2CH&author=Ikegaya%2CY&author=Sasaki%2CT)
    
142.  Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in _12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)_ 265–283 (USENIX Assoc., 2016).
    
143.  Whittington, J. C. R. & Bogacz, R. Theories of error back-propagation in the brain. _Trends Cogn. Sci._ **23**, 235–250 (2019).
    
    [Article](https://doi.org/10.1016%2Fj.tics.2018.12.005)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30704969)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6382460)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Theories%20of%20error%20back-propagation%20in%20the%20brain&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2018.12.005&volume=23&pages=235-250&publication_year=2019&author=Whittington%2CJCR&author=Bogacz%2CR)
    
144.  Khattar, D., Goud, J. S., Gupta, M. & Varma, V. Mvae: multimodal variational autoencoder for fake news detection. in _The World Wide Web Conference_ 2915–2921 (ACM, 2019).
    
145.  Ramesh, A. et al. Zero-shot text-to-image generation. in _International Conference on Machine Learning_ 8821–8831 (PMLR, 2021).
    
146.  McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at [https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426) (2018).
    
147.  Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. _J. Mach. Learn. Res._ **15**, 1929–1958 (2014).
    
    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&journal=J.%20Mach.%20Learn.%20Res.&volume=15&pages=1929-1958&publication_year=2014&author=Srivastava%2CN&author=Hinton%2CG&author=Krizhevsky%2CA&author=Sutskever%2CI&author=Salakhutdinov%2CR)
    
148.  Chollet, F. et al. _Keras Documentation_ (GitHub, 2015).
    

[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41562-023-01799-z?format=refman&flavour=references)

Acknowledgements
----------------

We thank T. Behrens, B. Love and D. Bush for useful discussions, and K. Norman for constructive comments on an earlier version.

Funding support for this work was received from a Wellcome Principal Research Fellowship ‘Neural mechanisms of memory and prediction: Finding structure in experience’ (222457/Z/21/Z) (N.B.), a Wellcome Collaborative Award ‘Organising knowledge for flexible behaviour in the prefrontal-hippocampal circuitry’ (214314/Z/18/Z) (N.B.), and an ERC advanced grant NEUROMEM (N.B.). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.

Author information
------------------

### Authors and Affiliations

1.  UCL Institute of Cognitive Neuroscience, University College London, London, UK
    
    Eleanor Spens & Neil Burgess
    
2.  UCL Queen Square Institute of Neurology, University College London, London, UK
    
    Neil Burgess
    

Authors

1.  Eleanor Spens
    
    You can also search for this author in [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Eleanor%20Spens) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Eleanor%20Spens%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)
    
2.  Neil Burgess
    
    You can also search for this author in [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Neil%20Burgess) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Neil%20Burgess%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)
    

### Contributions

E.S. and N.B. designed the research and wrote the paper. E.S. performed the computational modelling.

### Corresponding authors

Correspondence to [Eleanor Spens](mailto:eleanor.spens.20@ucl.ac.uk) or [Neil Burgess](mailto:n.burgess@ucl.ac.uk).

Ethics declarations
-------------------

### Competing interests

The authors declare no competing interests.

Peer review
-----------

### Peer review information

_Nature Human Behaviour_ thanks Gido van de Ven and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.

Additional information
----------------------

**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Supplementary information
-------------------------

Rights and permissions
----------------------

**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/).

[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=A%20generative%20model%20of%20memory%20construction%20and%20consolidation&author=Eleanor%20Spens%20et%20al&contentID=10.1038%2Fs41562-023-01799-z&copyright=The%20Author%28s%29&publication=2397-3374&publicationDate=2024-01-19&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)

About this article
------------------

[![Image 33: Check for updates. Verify currency and authenticity via CrossMark](blob:https://www.nature.com/df43c82f6cb6dcd2a87db38f317bf9f2)](https://crossmark.crossref.org/dialog/?doi=10.1038/s41562-023-01799-z)

### Cite this article

Spens, E., Burgess, N. A generative model of memory construction and consolidation. _Nat Hum Behav_ **8**, 526–543 (2024). https://doi.org/10.1038/s41562-023-01799-z

[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41562-023-01799-z?format=refman&flavour=citation)

*   Received: 30 May 2023
    
*   Accepted: 05 December 2023
    
*   Published: 19 January 2024
    
*   Issue Date: March 2024
    
*   DOI: https://doi.org/10.1038/s41562-023-01799-z

## Metadata

```json
{
  "title": "A generative model of memory construction and consolidation",
  "description": "Episodic memories are (re)constructed, share neural substrates with imagination, combine unique features with schema-based predictions and show schema-based distortions that increase with consolidation. Here we present a computational model in which hippocampal replay (from an autoassociative network) trains generative models (variational autoencoders) to (re)create sensory experiences from latent variable representations in entorhinal, medial prefrontal and anterolateral temporal cortices via the hippocampal formation. Simulations show effects of memory age and hippocampal lesions in agreement with previous models, but also provide mechanisms for semantic memory, imagination, episodic future thinking, relational inference and schema-based distortions including boundary extension. The model explains how unique sensory and predictable conceptual elements of memories are stored and reconstructed by efficiently combining both hippocampal and neocortical systems, optimizing the use of limited hippocampal storage for new and unusual information. Overall, we believe hippocampal replay training generative models provides a comprehensive account of memory construction, imagination and consolidation. Spens and Burgess develop a computational model that shows how the hippocampus encodes episodic memories and replays them to train generative models of the world. Conceptual and sensory representations of experience can then be recombined for imagination and memory.",
  "url": "https://www.nature.com/articles/s41562-023-01799-z#Sec13",
  "content": "Main\n----\n\nEpisodic memory concerns autobiographical experiences in their spatiotemporal context, whereas semantic memory concerns factual knowledge[1](https://www.nature.com/articles/s41562-023-01799-z#ref-CR1 \"Tulving, E. How many memory systems are there? Am. Psychol. 40, 385–398 (1985).\"). The former is thought to rapidly capture multimodal experience via long-term potentiation in the hippocampus, enabling the latter to learn statistical regularities over multiple experiences in the neocortex[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 \"Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970).\"),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 \"Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971).\"),[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 \"McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).\"),[5](https://www.nature.com/articles/s41562-023-01799-z#ref-CR5 \"Teyler, T. J. & DiScenna, P. The hippocampal memory indexing theory. Behav. Neurosci. 100, 147–154 (1986).\"). Crucially, episodic memory is thought to be constructive; recall is the (re)construction of a past experience, rather than the retrieval of a copy[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 \"Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932).\"),[7](https://www.nature.com/articles/s41562-023-01799-z#ref-CR7 \"Schacter, D. L. Constructive memory: past and future. Dialogues Clin. Neurosci. 14, 7–18 (2012).\"). But the mechanisms behind episodic (re)construction and its link to semantic memory are not well understood.\n\nOld memories can be preserved after hippocampal damage despite amnesia for recent ones[8](https://www.nature.com/articles/s41562-023-01799-z#ref-CR8 \"Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. J. Neurol. Neurosurg. Psychiatry 20, 11–21 (1957).\"), suggesting that memories initially encoded in the hippocampus end up being stored in neocortical areas, an idea known as ‘systems consolidation’[9](https://www.nature.com/articles/s41562-023-01799-z#ref-CR9 \"Squire, L. R. & Alvarez, P. Retrograde amnesia and memory consolidation: a neurobiological perspective. Curr. Opin. Neurobiol. 5, 169–177 (1995).\"). The standard model of systems consolidation involves transfer of information from the hippocampus to the neocortex[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 \"Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970).\"),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 \"Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971).\"),[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 \"McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).\"),[10](https://www.nature.com/articles/s41562-023-01799-z#ref-CR10 \"Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. Proc. Natl Acad. Sci. USA 91, 7041–7045 (1994).\"), whereas other views suggest that episodic and semantic information from the same events can exist in parallel[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 \"Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).\"). Hippocampal ‘replay’ of patterns of neural activity during rest[12](https://www.nature.com/articles/s41562-023-01799-z#ref-CR12 \"Wilson, M. A. & McNaughton, B. L. Reactivation of hippocampal ensemble memories during sleep. Science 265, 676–679 (1994).\"),[13](https://www.nature.com/articles/s41562-023-01799-z#ref-CR13 \"Diba, K. & Buzsáki, G. Forward and reverse hippocampal place-cell sequences during ripples. Nat. Neurosci. 10, 1241–1242 (2007).\") is thought to play a role in consolidation[14](https://www.nature.com/articles/s41562-023-01799-z#ref-CR14 \"Girardeau, G., Benchenane, K., Wiener, S. I., Buzsáki, G. & Zugaro, M. B. Selective suppression of hippocampal ripples impairs spatial memory. Nat. Neurosci. 12, 1222–1223 (2009).\"),[15](https://www.nature.com/articles/s41562-023-01799-z#ref-CR15 \"Ego-Stengel, V. & Wilson, M. A. Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat. Hippocampus 20, 1–10 (2010).\"). However, consolidation does not just change which brain regions support memory traces; it also converts them into a more abstract representation, a process sometimes referred to as semanticization[16](https://www.nature.com/articles/s41562-023-01799-z#ref-CR16 \"Winocur, G. & Moscovitch, M. Memory transformation and systems consolidation. J. Int. Neuropsychol. Soc. 17, 766–780 (2011).\"),[17](https://www.nature.com/articles/s41562-023-01799-z#ref-CR17 \"Norman, Y., Raccah, O., Liu, S., Parvizi, J. & Malach, R. Hippocampal ripples and their coordinated dialogue with the default mode network during recent and remote recollection. Neuron 109, 2767–2780 (2021).\").\n\nGenerative models capture the probability distributions underlying data, enabling the generation of realistic new items by sampling from these distributions. Here we propose that consolidated memory takes the form of a generative network, trained to capture the statistical structure of stored events by learning to reproduce them (see also refs. [18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 \"Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000).\"),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 \"Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002).\")). As consolidation proceeds, the generative network supports both the recall of ‘facts’ (semantic memory) and the reconstruction of experience from these ‘facts’ (episodic memory), in conjunction with additional information from the hippocampus that becomes less necessary as training progresses.\n\nThis builds on existing models of spatial cognition in which recall and imagination of scenes involve the same neural circuits[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 \"Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000).\"),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 \"Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007).\"),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\"), and is supported by evidence from neuropsychology that damage to the hippocampal formation (HF) leads to deficits in imagination[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 \"Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007).\"), episodic future thinking[24](https://www.nature.com/articles/s41562-023-01799-z#ref-CR24 \"Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. Curr. Opin. Behav. Sci. 17, 41–50 (2017).\"), dreaming[25](https://www.nature.com/articles/s41562-023-01799-z#ref-CR25 \"Spanó, G. et al. Dreaming with hippocampal damage. Elife 9, e56211 (2020).\") and daydreaming[26](https://www.nature.com/articles/s41562-023-01799-z#ref-CR26 \"McCormick, C., Rosenthal, C. R., Miller, T. D. & Maguire, E. A. Mind-wandering in people with hippocampal damage. J. Neurosci. 38, 2745–2754 (2018).\"), as well as by neuroimaging evidence that recall and imagination involve similar neural processes[27](https://www.nature.com/articles/s41562-023-01799-z#ref-CR27 \"Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. Neuropsychologia 45, 1363–1377 (2007).\"),[28](https://www.nature.com/articles/s41562-023-01799-z#ref-CR28 \"Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. Trends Cogn. Sci. 11, 299–306 (2007).\").\n\nWe model consolidation as the training of a generative model by an initial autoassociative encoding of memory through ‘teacher–student learning’[29](https://www.nature.com/articles/s41562-023-01799-z#ref-CR29 \"Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at \nhttps://arxiv.org/abs/1503.02531\n(2015).\") during hippocampal replay (see also ref. [30](https://www.nature.com/articles/s41562-023-01799-z#ref-CR30 \"Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. Nat. Neurosci. 26, 1438–1448 (2023).\")). Recall after consolidation has occurred is a generative process mediated by schemas representing common structure across events, as are other forms of scene construction or imagination. Our model builds on: (1) research into the relationship between generative models and consolidation[18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 \"Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000).\"),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 \"Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002).\"), (2) the use of variational autoencoders to model the hippocampal formation[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 \"Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).\"),[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 \"van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020).\") and (3) the view that abstract allocentric latent variables are learned from egocentric sensory representations in spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\").\n\nMore generally, we build on the idea that the memory system learns schemas which encode ‘priors’ for the reconstruction of input patterns[34](https://www.nature.com/articles/s41562-023-01799-z#ref-CR34 \"Hemmer, P. & Steyvers, M. A Bayesian account of reconstructive memory. Top. Cogn. Sci. 1, 189–202 (2009).\"),[35](https://www.nature.com/articles/s41562-023-01799-z#ref-CR35 \"Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. Neural Comput. 34, 1841–1870 (2022).\"). Unpredictable aspects of experience need to be stored in detail for further learning, while fully predicted aspects do not, consistent with the idea that memory helps to predict the future[36](https://www.nature.com/articles/s41562-023-01799-z#ref-CR36 \"Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. Nat. Rev. Neurosci. 8, 657–661 (2007).\"),[37](https://www.nature.com/articles/s41562-023-01799-z#ref-CR37 \"Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. Trends Cogn. Sci. 24, 542–556 (2020).\"),[38](https://www.nature.com/articles/s41562-023-01799-z#ref-CR38 \"Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. Learn. Mem. 28, 422–434 (2021).\"),[39](https://www.nature.com/articles/s41562-023-01799-z#ref-CR39 \"Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. J. Neurosci. 42, 9053–9068 (2022).\"). We suggest that familiar components are encoded in the autoassociative network as concepts (relying on the generative network for reconstruction), while novel components are encoded in greater sensory detail. This is efficient in terms of memory storage[40](https://www.nature.com/articles/s41562-023-01799-z#ref-CR40 \"Barlow, H. B. et al. in Sensory Communication (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013).\"),[41](https://www.nature.com/articles/s41562-023-01799-z#ref-CR41 \"Barlow, H. B. Unsupervised learning. Neural Comput. 1, 295–311 (1989).\"),[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 \"Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).\") and reflects the fact that consolidation can be a gradual transition, during which the autoassociative network supports aspects of memory not yet captured by the generative network. In other words, the generative network can reconstruct predictable aspects of an event from the outset on the basis of existing schemas, but as consolidation progresses, the network updates its schemas to reconstruct the event more accurately until the formerly unpredictable details stored in HF are no longer required.\n\nOur model draws together existing ideas in machine learning to suggest an explanation for the following key features of memory, only subsets of which are captured by previous models:\n\n1.  1.The initial encoding of memory requires only a single exposure to the event and depends on the HF, while the consolidated form of memory is acquired more gradually[2](https://www.nature.com/articles/s41562-023-01799-z#ref-CR2 \"Marr, D. A theory for cerebral neocortex. Proc. R. Soc. Lond. B 176, 161–234 (1970).\"),[3](https://www.nature.com/articles/s41562-023-01799-z#ref-CR3 \"Marr, D. Simple memory: a theory for archicortex. Phil. Trans. R. Soc. Lond. B 262, 23–81 (1971).\"),[10](https://www.nature.com/articles/s41562-023-01799-z#ref-CR10 \"Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. Proc. Natl Acad. Sci. USA 91, 7041–7045 (1994).\"), as in the complementary learning systems (CLS) model[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 \"McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).\").\n    \n2.  2.The semantic content of memories becomes independent of the HF over time[43](https://www.nature.com/articles/s41562-023-01799-z#ref-CR43 \"Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380 (1997).\"),[44](https://www.nature.com/articles/s41562-023-01799-z#ref-CR44 \"Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. Neuron 38, 127–133 (2003).\"),[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 \"Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015).\"), consistent with CLS.\n    \n3.  3.Vivid, detailed episodic memory remains dependent on HF[46](https://www.nature.com/articles/s41562-023-01799-z#ref-CR46 \"McKenzie, S. & Eichenbaum, H. Consolidation and reconsolidation: two lives of memories? Neuron 71, 224–233 (2011).\"), consistent with multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 \"Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).\") (but not with CLS).\n    \n4.  4.Similar neural circuits are involved in recall, imagination and episodic future thinking[27](https://www.nature.com/articles/s41562-023-01799-z#ref-CR27 \"Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. Neuropsychologia 45, 1363–1377 (2007).\"),[28](https://www.nature.com/articles/s41562-023-01799-z#ref-CR28 \"Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. Trends Cogn. Sci. 11, 299–306 (2007).\"), suggesting a common mechanism for event generation, as modelled in spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\").\n    \n5.  5.Consolidation extracts statistical regularities from episodic memories to inform behaviour[47](https://www.nature.com/articles/s41562-023-01799-z#ref-CR47 \"Durrant, S. J., Taylor, C., Cairney, S. & Lewis, P. A. Sleep-dependent consolidation of statistical learning. Neuropsychologia 49, 1322–1331 (2011).\"),[48](https://www.nature.com/articles/s41562-023-01799-z#ref-CR48 \"Richards, B. A. et al. Patterns across multiple memories are identified over time. Nat. Neurosci. 17, 981–986 (2014).\"), and supports relational inference and generalization[49](https://www.nature.com/articles/s41562-023-01799-z#ref-CR49 \"Ellenbogen, J. M., Hu, P. T., Payne, J. D., Titone, D. & Walker, M. P. Human relational memory requires time and sleep. Proc. Natl Acad. Sci. USA 104, 7723–7728 (2007).\"). The Tolman–Eichenbaum machine (TEM)[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\") simulates this in the domain of multiple tasks with common transition structures (see also ref. [50](https://www.nature.com/articles/s41562-023-01799-z#ref-CR50 \"Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016).\")), while ref. [51](https://www.nature.com/articles/s41562-023-01799-z#ref-CR51 \"Schapiro, A. C., Turk-Browne, N. B., Botvinick, M. M. & Norman, K. A. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. Phil. Trans. R. Soc. B 372, 20160049 (2017).\") models how both individual examples and statistical regularities could be learned within HF.\n    \n6.  6.Post-consolidation episodic memories are more prone to schema-based distortions in which semantic or contextual knowledge influences recall[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 \"Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932).\"),[52](https://www.nature.com/articles/s41562-023-01799-z#ref-CR52 \"Payne, J. D. et al. The role of sleep in false memory formation. Neurobiol. Learn. Mem. 92, 327–334 (2009).\"), consistent with the behaviour of generative models[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 \"Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).\").\n    \n7.  7.Neural representations in the entorhinal cortex (EC) such as grid cells[53](https://www.nature.com/articles/s41562-023-01799-z#ref-CR53 \"Hafting, T., Fyhn, M., Molden, S., Moser, M. B. & Moser, E. I. Microstructure of a spatial map in the entorhinal cortex. Nature 436, 801–806 (2005).\") are thought to encode latent structures underlying experiences[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 \"Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016).\"), and other regions of the association cortex, such as the medial prefrontal cortex (mPFC), may compress stimuli to a minimal representation[55](https://www.nature.com/articles/s41562-023-01799-z#ref-CR55 \"Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. Nat. Commun. 11, 46 (2020).\").\n    \n8.  8.Novelty is thought to promote encoding within HF[56](https://www.nature.com/articles/s41562-023-01799-z#ref-CR56 \"Hasselmo, M. E., Wyble, B. P. & Wallenstein, G. V. Encoding and retrieval of episodic memories: role of cholinergic and GABAergic modulation in the hippocampus. Hippocampus 6, 693–708 (1996).\"), while more predictable events consistent with existing schemas are consolidated more rapidly[57](https://www.nature.com/articles/s41562-023-01799-z#ref-CR57 \"Tse, D. et al. Schemas and memory consolidation. Science 316, 76–82 (2007).\"). Activity in the hippocampus can reflect prediction error or mismatch novelty[58](https://www.nature.com/articles/s41562-023-01799-z#ref-CR58 \"Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. PLoS Biol. 4, e424 (2006).\"),[59](https://www.nature.com/articles/s41562-023-01799-z#ref-CR59 \"Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. Learn. Mem. 18, 523–528 (2011).\"), and novelty is thought to affect the degree of compression of representations in memory[60](https://www.nature.com/articles/s41562-023-01799-z#ref-CR60 \"Hedayati, S., O’Donnell, R. E. & Wyble, B. A model of working memory for latent representations. Nat. Hum. Behav. 6, 709–719 (2022).\") to make efficient use of limited HF capacity[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 \"Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).\").\n    \n9.  9.Memory traces in the hippocampus appear to involve a mixture of sensory and conceptual features, with the latter encoded by concept cells[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 \"Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012).\"), potentially bound together by episode-specific neurons[62](https://www.nature.com/articles/s41562-023-01799-z#ref-CR62 \"Kolibius, L. D. et al. Hippocampal neurons code individual episodic memories in humans. Nat. Hum. Behav. 7, 1968–1979 (2023).\"). Few models explore how this could happen.\n    \n\n### Consolidation as the training of a generative model\n\nOur model simulates how the initial representation of memories can be used to train a generative network, which learns to reconstruct memories by capturing the statistical structure of experienced events (or ‘schemas’). First, the hippocampus rapidly encodes an event; then, generative networks gradually take over after being trained on replayed representations from the hippocampus. This makes the memory more abstracted, more supportive of generalization and relational inference, but also more prone to gist-based distortion. The generative networks can be used to reconstruct (for memory) or construct (for imagination) sensory experience, or to support semantic memory and relational inference directly from their latent variable representations (see Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).\n\n**Fig. 1: Architecture of the basic model.**\n\n[![Image 26: figure 1](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig1_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/1)\n\n**a**, First, the hippocampus rapidly encodes an event, modelled as one-shot memorization in an autoassociative network (an MHN). Then, generative networks are trained on replayed representations from the autoassociative network, learning to reconstruct memories by capturing the statistical structure of experienced events. **b**, A more detailed schematic of the generative network to indicate the multiple layers of, and overlap between, the encoder and decoder (where layers closer to the sensory neocortex overlap more). The generation of a sensory experience, for example, visual imagery, requires the decoder to the sensory neocortex via HF. **c**, Random noise inputs to the MHN (top row) reactivate its memories (bottom row) after 10,000 items from the Shapes3D dataset are encoded, with five examples shown. **d**, The generative model (a variational autoencoder) can recall images (bottom row) from a partial input (top row), following training on 10,000 replayed memories sampled from the MHN. **e**, Episodic memory after consolidation: a partial input is mapped to latent variables whose return projections to the sensory neocortex via HF then decode these back into a sensory experience. **f**, Imagination: latent variables are decoded into an experience via HF and return projections to the neocortex. **g**, Semantic memory: a partial input is mapped to latent variables, which capture the ‘key facts’ of the scene. The bottom rows of **e**–**g** illustrate these functions in a model that has encoded the Shapes3D dataset into latent variables (_v_1, _v_2, _v_3, …, _v__n_). Diagrams were created using [BioRender.com](http://biorender.com/).\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/1)\n\nBefore consolidation, the hippocampal autoassociative network encodes the memory. A modern Hopfield network (MHN)[63](https://www.nature.com/articles/s41562-023-01799-z#ref-CR63 \"Ramsauer, H. et al. Hopfield networks is all you need. in International Conference on Learning Representations (2021).\") is used, which can be interpreted such that the feature units activated by an event are bound together by a memory unit[64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 \"Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).\") (see [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14) and [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1)). Teacher–student learning[29](https://www.nature.com/articles/s41562-023-01799-z#ref-CR29 \"Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at \nhttps://arxiv.org/abs/1503.02531\n(2015).\") allows transfer of memories from one neural network to another during consolidation[30](https://www.nature.com/articles/s41562-023-01799-z#ref-CR30 \"Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. Nat. Neurosci. 26, 1438–1448 (2023).\"). Accordingly, we use outputs from the autoassociative network to train the generative network: random inputs to the hippocampus result in the reactivation of memories, and this reactivation results in consolidation. After consolidation, generative networks encode the information contained in memories. Reliance on the generative networks increases over time as they learn to reconstruct a particular event.\n\nSpecifically, the generative networks are implemented as variational autoencoders (VAEs), which are autoencoders with special properties such that the most compressed layer represents a set of latent variables, which can be sampled from to generate realistic new examples corresponding to the training dataset[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 \"Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at \nhttps://arxiv.org/abs/1312.6114\n(2013).\"),[66](https://www.nature.com/articles/s41562-023-01799-z#ref-CR66 \"Kingma, D. P. & Welling, M. An introduction to variational autoencoders. Found. Trends Mach. Learn. 12, 307–392 (2019).\"). Latent variables can be thought of as hidden factors behind the observed data, and directions in the latent space can correspond to meaningful transformations (see [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14)). The VAE’s encoder ‘encodes’ sensory experience as latent variables, while its decoder ‘decodes’ latent variables back to sensory experience. In psychological terms, after training on a class of stimuli, VAEs can reconstruct such stimuli from a partial input according to the schema for that class, and generate novel stimuli consistent with the schema. (Our use of VAEs is illustrative, and we would expect a range of other generative latent variable models, such as predictive coding networks[67](https://www.nature.com/articles/s41562-023-01799-z#ref-CR67 \"Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. Neural Comput. 7, 889–904 (1995).\"),[68](https://www.nature.com/articles/s41562-023-01799-z#ref-CR68 \"Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999).\"),[69](https://www.nature.com/articles/s41562-023-01799-z#ref-CR69 \"Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010).\"), to show similar behaviour.) See [Methods](https://www.nature.com/articles/s41562-023-01799-z#Sec14) and [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for further details.\n\nGenerative networks capture the probability distributions underlying events, or ‘schemas’. In other words, here ‘schemas’ are rules or priors (expected probability distributions) for reconstructing a certain type of stimulus (for example, the schema for an office predicts the presence of co-occurring objects such as desks and chairs, facilitating episode generation), whereas concepts represent categories but not necessarily how to reconstruct them. However, schemas and concepts are closely related, and their meanings can overlap, with conflicting definitions in the psychology literature[70](https://www.nature.com/articles/s41562-023-01799-z#ref-CR70 \"Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. Trends Cogn. Sci. 21, 618–631 (2017).\"),[71](https://www.nature.com/articles/s41562-023-01799-z#ref-CR71 \"Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. Neuropsychologia 53, 104–114 (2014).\").\n\nDuring perception, the generative model provides an ongoing estimate of novelty from its reconstruction error (also known as ‘prediction error’, the difference between input and output representations). Aspects of an event that are consistent with previous experience (that is, with low reconstruction error) do not need to be encoded in detail in the autoassociative ‘teacher’ network[36](https://www.nature.com/articles/s41562-023-01799-z#ref-CR36 \"Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. Nat. Rev. Neurosci. 8, 657–661 (2007).\"),[37](https://www.nature.com/articles/s41562-023-01799-z#ref-CR37 \"Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. Trends Cogn. Sci. 24, 542–556 (2020).\"),[38](https://www.nature.com/articles/s41562-023-01799-z#ref-CR38 \"Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. Learn. Mem. 28, 422–434 (2021).\"),[39](https://www.nature.com/articles/s41562-023-01799-z#ref-CR39 \"Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. J. Neurosci. 42, 9053–9068 (2022).\"). Once the generative network’s reconstruction error is sufficiently low, the hippocampal trace is unnecessary, freeing up capacity for new encodings. However, we have not simulated decay, deletion or capacity constraints in the autoassociative memory part of the model.\n\n### Combining conceptual and sensory features in episodic memory\n\nConsolidation is often considered in terms of fine-grained sensory representations updating coarse-grained conceptual representations, for example, the sight of a particular dog updating the concept of a dog. Modelling hippocampal representations as sensory-like is a reasonable simplification, which we make in simulations of the ‘basic’ model in Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1). However, memories probably bind together representations along a spectrum from coarse-grained and conceptual to fine-grained and sensory. For example, the hippocampal encoding of a day at the beach is likely to bind together coarse-grained concepts such as ‘beach’ and ‘sea’ along with sensory representations such as the melody of an unfamiliar song or the sight of a particular sandcastle, consistent with the evidence for concept cells in the hippocampus[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 \"Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012).\"). (This also fits with the observation that ambiguous images ‘flip’ between interpretations in perception but are stable when held in memory[72](https://www.nature.com/articles/s41562-023-01799-z#ref-CR72 \"Chambers, D. & Reisberg, D. Can mental images be ambiguous? J. Exp. Psychol. Hum. Percept. Perform. 11, 317–328 (1985).\"), reflecting how the conceptual content of memories constrains recall.)\n\nFurthermore, encoding every sensory detail in the hippocampus would be inefficient (elements already predicted by conceptual representations being redundant); an efficient system should take advantage of shared structure across memories to encode only what is necessary[40](https://www.nature.com/articles/s41562-023-01799-z#ref-CR40 \"Barlow, H. B. et al. in Sensory Communication (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013).\"),[41](https://www.nature.com/articles/s41562-023-01799-z#ref-CR41 \"Barlow, H. B. Unsupervised learning. Neural Comput. 1, 295–311 (1989).\"). Accordingly, we suggest that predictable elements are encoded as conceptual features linked to the generative latent variable representation, while unpredictable elements are encoded in a more detailed and veridical form as sensory features.\n\nSuppose someone sees an unfamiliar animal in the forest (Fig. [2b](https://www.nature.com/articles/s41562-023-01799-z#Fig2)). Much of the event might be consistent with an existing forest schema, but the unfamiliar animal would be novel. In the extended model (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2) and section ‘Combining conceptual and unpredictable sensory features’), the reconstruction error per element of the experience is calculated by the generative model during perception, and elements with high reconstruction error are encoded in the autoassociative network as sensory features, along with conceptual features linked to the generative model’s latent variable representation. In other words, each pattern is split into a predictable component (approximating the generative network’s prediction for the pattern), plus an unpredictable component (elements with high prediction error). This produces a sparser vector than storing every element in detail, increasing the capacity of the network[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 \"Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).\").\n\n**Fig. 2: Architecture of the extended model.**\n\n[![Image 27: figure 2](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig2_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/2)\n\n**a**, Each scene is initially encoded as a combination of predictable conceptual features related to the latent variables of the generative network and unpredictable sensory features that were poorly predicted by the generative network. An MHN (in red) encodes both sensory and conceptual features (with connections to the sensory neocortex and latent variables in EC, respectively), binding them together via memory units. Memories may eventually be learned by the generative model (in blue), but consolidation can be a prolonged process, during which time the generative network provides schemas for reconstruction and the autoassociative network supports new or detailed information not yet captured by these schemas. Multiple generative networks can be trained concurrently, with different networks optimized for different tasks. This includes networks with latent variables in EC, mPFC and alTL, each with its own semantic projections. However, in all cases, return projections to the sensory neocortex are via HF. **b**, An illustration of encoding in the extended model. **c**, Encoding ‘scenes’ from the Shapes3D dataset, with each ‘scene’ decomposed into unpredicted sensory features (top) and conceptual features linked to the generative network’s latent variables (bottom). Novel features (white squares overlaid on the image with varying opacity) are added to each ‘scene’. **d**, Recalling ‘scenes’ (with novel features) from the Shapes3D dataset. First, the input is decomposed; then, the MHN performs pattern completion on both sensory and conceptual features. The conceptual features (which in these simulations are simply the generative network’s latent variables) are then decoded into a schema-based prediction, onto which any stored sensory features are overwritten. Diagrams were created using [BioRender.com](http://biorender.com/).\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/2)\n\n### Neural substrates of the model\n\nWhich brain regions do the components of this model represent? The autoassociative network involves the hippocampus binding together the constituents of a memory in the neocortex, whereas the generative network involves neocortical inputs projecting to latent variable representations in the higher association cortex, which then project back to the neocortex via the HF. The entorhinal (EC), medial prefrontal cortex (mPFC) and anterolateral temporal lobe (alTL) are all prime candidates for the site of latent variable representations.\n\nFirst, the EC is the main route between the hippocampus and the neocortex, and is where grid cells, which are thought to be a latent variable representation of spatial or relational structure[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 \"Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016).\"), are most often observed[73](https://www.nature.com/articles/s41562-023-01799-z#ref-CR73 \"Moser, E. I., Kropff, E. & Moser, M. B. Place cells, grid cells, and the brain’s spatial representation system. Annu. Rev. of Neurosci. 31, 69–89 (2008).\"). Second, mPFC and its connections to HF play a crucial role in episodic memory processing[70](https://www.nature.com/articles/s41562-023-01799-z#ref-CR70 \"Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. Trends Cogn. Sci. 21, 618–631 (2017).\"),[74](https://www.nature.com/articles/s41562-023-01799-z#ref-CR74 \"Takashima, A. et al. Declarative memory consolidation in humans: a prospective functional magnetic resonance imaging study. Proc. Natl Acad. Sci. USA 103, 756–761 (2006).\"),[75](https://www.nature.com/articles/s41562-023-01799-z#ref-CR75 \"Gais, S. et al. Sleep transforms the cerebral trace of declarative memories. Proc. Natl Acad. Sci. USA 104, 18778–18783 (2007).\"),[76](https://www.nature.com/articles/s41562-023-01799-z#ref-CR76 \"Frankland, P. W. & Bontempi, B. The organization of recent and remote memories. Nat. Rev. Neurosci. 6, 119–130 (2005).\"),[77](https://www.nature.com/articles/s41562-023-01799-z#ref-CR77 \"van Kesteren, M. T. R., Fernández, G., Norris, D. G. & Hermans, E. J. Persistent schema-dependent hippocampal-neocortical connectivity during memory encoding and postencoding rest in humans. Proc. Natl Acad. Sci. USA 107, 7550–7555 (2010).\"),[78](https://www.nature.com/articles/s41562-023-01799-z#ref-CR78 \"Benchenane, K. et al. Coherent theta oscillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning. Neuron 66, 921–936 (2010).\"), are thought to encode schemas[57](https://www.nature.com/articles/s41562-023-01799-z#ref-CR57 \"Tse, D. et al. Schemas and memory consolidation. Science 316, 76–82 (2007).\"),[71](https://www.nature.com/articles/s41562-023-01799-z#ref-CR71 \"Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. Neuropsychologia 53, 104–114 (2014).\"), are implicated in transitive inference[79](https://www.nature.com/articles/s41562-023-01799-z#ref-CR79 \"Koscik, T. R. & Tranel, D. The human ventromedial prefrontal cortex is critical for transitive inference. J. Cogn. Neurosci. 24, 1191–1204 (2012).\") and the integration of memories[80](https://www.nature.com/articles/s41562-023-01799-z#ref-CR80 \"Spalding, K. N. et al. Ventromedial prefrontal cortex is necessary for normal associative inference and memory integration. J. Neurosci. 38, 3767–3775 (2018).\"), and perform dimensionality reduction by compressing irrelevant features[55](https://www.nature.com/articles/s41562-023-01799-z#ref-CR55 \"Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. Nat. Commun. 11, 46 (2020).\"). Third, the anterior and lateral temporal cortices associated with semantic memory[81](https://www.nature.com/articles/s41562-023-01799-z#ref-CR81 \"Chan, D. et al. Patterns of temporal lobe atrophy in semantic dementia and Alzheimer’s disease. Ann. Neurol. 49, 433–442 (2001).\") and retrograde amnesia[82](https://www.nature.com/articles/s41562-023-01799-z#ref-CR82 \"Bright, P. et al. Retrograde amnesia in patients with hippocampal, medial temporal, temporal lobe, or frontal pathology. Learn. Mem. 13, 545–557 (2006).\") probably contain latent variable representations capturing semantic structure. This might correspond to the ‘anterior temporal network’ associated with semantic dementia[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 \"Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012).\"), while the first network (between sensory and entorhinal cortices) might correspond to the ‘posterior medial network’[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 \"Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012).\"), and to the network mapping between visual scenes and allocentric spatial representations[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 \"Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000).\"),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 \"Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007).\"),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\").\n\nWhich regions constitute the generative network’s decoder? The decoder converts latent variable representations in the higher association cortex back to sensory neocortical representations via HF. Patients with damage to the hippocampus proper but not the EC can generate simple scenes (or fragments thereof), but an intact hippocampus is required for more coherent imagery of complex ones[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 \"Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007).\"). We hypothesize that conceptual units in the hippocampus proper help to generate complex, conceptually coherent scenes (perhaps through a recurrent ‘clean up’ mechanism), but that an intact EC and its return pathway to the sensory neocortex (the ventral visual stream for images) can still decode representations to some extent in their absence.\n\nMultiple generative networks can be trained concurrently from a single autoassociative network through consolidation, with different networks optimized for different tasks. In other words, multiple networks could update their parameters to minimize prediction error on the basis of the same replayed memories. This could consist of a primary VAE with latent variables in the EC, plus additional parallel pathways from the higher sensory cortex to the EC via latent variables in the mPFC or the alTL. (Computationally, the shared connections could be fixed as the alternative pathways are trained.) Note that in all cases, return projections to the sensory neocortex via HF are required to decode latent variables into sensory experiences.\n\nResults\n-------\n\n### Modelling encoding and recall\n\nEach new event is encoded as an autoassociative trace in the hippocampus, modelled as an MHN. Two properties of this network are particularly important: memorization occurs with only one exposure, and random inputs to the network retrieve stored memories sampled from the whole set of memories (modelling replay).\n\nWe model recall as (re)constructing a scene from a partial input. First, we simulate encoding and replay in the autoassociative network. The network memorizes a set of scenes, representing events, as described above. When the network is given a partial input, it retrieves the closest stored memory. Even when the network is given random noise, it retrieves stored memories (see Fig. [1c](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). Second, we simulate recall in the generative network trained on reactivated memories from the autoassociative network, which is able to reconstruct the original image when presented with a partial version of an item from the training data (Fig. [1d](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).\n\nIn the basic model (Fig. [1a](https://www.nature.com/articles/s41562-023-01799-z#Fig1)), the prediction error could be calculated for each event so that only the unpredictable events are stored in the hippocampus, as the predictable ones can already be retrieved by the generative network (however, this is not simulated explicitly). In the extended model (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2) and section ‘Combining conceptual and unpredictable sensory features’), prediction error is calculated for each element of an event, determining which sensory details are stored.\n\n### Modelling semantic memory\n\nExisting semantic memory survives when the hippocampus is lesioned[43](https://www.nature.com/articles/s41562-023-01799-z#ref-CR43 \"Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. Science 277, 376–380 (1997).\"),[44](https://www.nature.com/articles/s41562-023-01799-z#ref-CR44 \"Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. Neuron 38, 127–133 (2003).\"),[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 \"Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015).\"), and hippocampal amnesics can describe remote memories more successfully than recent ones[8](https://www.nature.com/articles/s41562-023-01799-z#ref-CR8 \"Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. J. Neurol. Neurosurg. Psychiatry 20, 11–21 (1957).\"),[84](https://www.nature.com/articles/s41562-023-01799-z#ref-CR84 \"Spiers, H. J., Maguire, E. A. & Burgess, N. Hippocampal amnesia. Neurocase 7, 357–382 (2001).\"), even if they might not recall them ‘episodically’[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 \"Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).\"). This temporal gradient indicates that the semantic component of memories becomes HF-independent. In the model, EC lesions impair all truly episodic recollection since the return projections from the HF are required for the generation of sensory experiences. Here we describe how remote memories could be retrieved ‘in semantic form’ despite lesions including the hippocampus and the EC.\n\nThe latent variable representation of an event in the generative network encodes the key facts about the event and can drive semantic memory directly without decoding the representation back into a sensory experience (Fig. [1g](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). The output route via HF is necessary for turning latent variable representations in mPFC or alTL into a sensory experience, but the latent variables themselves could support semantic retrieval. Thus, when the HF (including the EC) is removed, the model can still support retrieval of semantic information (see section ‘Modelling brain damage’ for details). To show this, we trained models to predict attributes of each image from its latent vector. Figure [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows that semantic ‘decoding accuracy’ increases as training progresses, reflecting the learning of semantic structure as a by-product of learning to reconstruct the sensory input patterns (_r_s(48) = 0.997, _P_ < 0.001, 95% confidence interval (CI) = 0.987, 1.000). While semantic memory is much more complex than simple classification, richer ‘semantic’ outputs such as verbal descriptions can also be decoded from latent variable representations of images[85](https://www.nature.com/articles/s41562-023-01799-z#ref-CR85 \"Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in Proc. IEEE Conference on Computer Vision and Pattern Recognition 3156–3164 (2015).\"),[86](https://www.nature.com/articles/s41562-023-01799-z#ref-CR86 \"Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at \nhttps://arxiv.org/abs/2111.09734\n(2021).\").\n\n**Fig. 3: Learning, relational inference and imagination in the generative model.**\n\n[![Image 28: figure 3](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig3_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/3)\n\n**a**, Reconstruction error (red) and decoding accuracy (blue) improve during training of the generative model. Decoding accuracy refers to the performance of a support vector classifier trained to output the central object’s shape from the latent variables, using 200 examples at the end of each epoch of generative model training. An epoch is one presentation of the training set of 10,000 samples from the hippocampus. **b**, Relational inference as vector arithmetic in the latent space. The three items on the right of each equation are items from the training data. Their latent variable representations are combined as vectors according to the equation, giving the latent variable representation from which the first item is generated. The pair in brackets describes a relation which is applied to the second item to produce the first. In the top row, the object shape changes from a cylinder to a sphere. In the second, the object shape changes from a cylinder to a cube, and the object colour from red to blue. In the third and fourth, more complex transitions change the object colour and shape, wall colour and angle. **c**, Imagining new items via interpolation in latent space. Each row shows points along a line in the latent space between two items from the training data, decoded into images by the generative network’s decoder. **d**, Imagining new items from a category. Samples from each of the shape categories of the support vector classifier in **a** are shown.\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/3)\n\n### Imagination, episodic future thinking and relational inference\n\nHere we model the generation of events that have not been experienced from the generative network’s latent variables. Events can be generated either by external specification of latent variables (imagination) or by transforming the latent variable representations of specific events (relational inference). The former is simulated by sampling from categories in the latent space then decoding the results (Fig. [3d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). The latter is simulated by interpolating between the latent representations of events (Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) or by doing vector arithmetic in the latent space (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). This illustrates that the model has learnt some conceptual structure to the data, supporting reasoning tasks of the form ‘what is to _A_ as _B_ is to _C_?’, and provides a model for the flexible recombination of memories thought to underlie episodic future thinking[24](https://www.nature.com/articles/s41562-023-01799-z#ref-CR24 \"Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. Curr. Opin. Behav. Sci. 17, 41–50 (2017).\").\n\n### Modelling schema-based distortions\n\nThe schema-based distortions observed in human episodic memory increase over time[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 \"Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932).\") and with sleep[52](https://www.nature.com/articles/s41562-023-01799-z#ref-CR52 \"Payne, J. D. et al. The role of sleep in false memory formation. Neurobiol. Learn. Mem. 92, 327–334 (2009).\"), suggesting an association with consolidation. Recall by the generative network distorts memories towards prototypical representations. Figure [4a–d](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows that handwritten digits from the MNIST dataset[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 \"LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).\") ‘recalled’ by a VAE become more prototypical (MNIST is used for this because each image has a single category). Recalled pairs from the same class become more similar, that is, intra-class variation decreases (paired samples _t_\\-test _t_(7,839) = 60.523, _P_ < 0.001, Cohen’s _d_ = −0.684, 95% CI = 0.021, 0.022). The pixel space of MNIST digits before and after recall and the latent space of their encodings also show this effect. In summary, recall with a generative network distorts stimuli towards more prototypical representations even when no class information is given during training. As reliance on the generative model increases, so does the level of distortion.\n\n**Fig. 4: Generative network shows schema-based distortions.**\n\n[![Image 29: figure 4](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig4_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/4)\n\n**a**, MNIST digits (top) and the VAE’s output for each (bottom). Recalled pairs from the same class become more similar. A total of 10,000 items from the MNIST dataset were encoded in the MHN, and 10,000 replayed samples were used to train the VAE. **b**, The variation within each MNIST class is smaller for the recalled items than for the original inputs. For each of the 10 classes, the variance per pixel is calculated across 500 images, and the 784 pixel variances are then plotted for each class before and after recall. In each boxplot, the box gives the interquartile range, its central line gives the median, and its whiskers extend to the 10th and 90th percentiles of the data. **c**,**d**, The pixel spaces of MNIST digits (bottom row) and the latent space of their encodings (top row) show more compact clusters for the generative network’s outputs (**d**) than for its inputs (**c**). Pixel and latent spaces are shown projected into 2D with UMAP[146](https://www.nature.com/articles/s41562-023-01799-z#ref-CR146 \"McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at \nhttps://arxiv.org/abs/1802.03426\n(2018).\") and colour-coded by class. **e**, Examples of boundary extension and contraction. Top row: the noisy input images (from a held-out test set), with an atypically ‘zoomed out’ or ‘zoomed in’ view (by 80% and 120% on the left and right, respectively) for three original images. Bottom row: the predicted images for each input image, which are distorted towards the ‘typical view’ in each case. **f**, Adapted figure from ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 \"Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. \nhttps://doi.org/10.1167/jov.21.9.2124\n(2021).\"), showing the distribution of boundary extension vs contraction as a function of the viewpoint of an image. Specifically, the values are the average of ‘closer’ vs ‘further’ judgements, assigned −1 and 1, respectively, of an identical stimulus image in comparison with the remembered image (with 900 trials per position). Error bars give the standard error of the mean. Example stimuli are shown at the bottom. **g**, In our model, the VAE increases the estimated size of the central object in atypically ‘zoomed out’ views compared with the training data, and decreases it in atypically ‘zoomed in’ views, as in ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 \"Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. \nhttps://doi.org/10.1167/jov.21.9.2124\n(2021).\"). Two hundred images are used at each ‘zoom level’. See **b** for a description of boxplot elements.\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/4)\n\nBoundary extension and contraction exemplify this phenomenon. Boundary extension is the tendency to remember a wider field of view than was observed[88](https://www.nature.com/articles/s41562-023-01799-z#ref-CR88 \"Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. J. Exp. Psychol. Learn. Mem. Cogn. 15, 179–187 (1989).\"), while boundary contraction is the opposite[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\"). Unusually close-up views appear to cause boundary extension, and unusually far away ones boundary contraction[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\"), although this is debated[90](https://www.nature.com/articles/s41562-023-01799-z#ref-CR90 \"Intraub, H. Searching for boundary extension. Curr. Biol. 30, R1463–R1464 (2020).\"),[91](https://www.nature.com/articles/s41562-023-01799-z#ref-CR91 \"Bainbridge, W. A. & Baker, C. I. Reply to Intraub. Curr. Biol. 30, R1465–R1466 (2020).\"). We modelled this by giving the generative network a range of new scenes that were artificially ‘zoomed in’ or ‘zoomed out’ compared with those in its training set; its reconstructions are distorted towards the ‘typical view’ (Fig. [4e](https://www.nature.com/articles/s41562-023-01799-z#Fig4)), as in human data. Figure [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows the change in the object size in memory quantitatively, mirroring the findings in ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 \"Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. \nhttps://doi.org/10.1167/jov.21.9.2124\n(2021).\") (Fig. [4f](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). (Note that the measure of boundary extension vs contraction used by ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 \"Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. \nhttps://doi.org/10.1167/jov.21.9.2124\n(2021).\") is produced by averaging ‘closer’ vs ‘further’ judgements of an identical stimulus image in comparison with the remembered image, rather than the drawing-based measure we use, but the two measures are significantly correlated[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\").)\n\n### Combining conceptual and unpredictable sensory features\n\nIn the extended model, memories stored in the hippocampal autoassociative network combine conceptual features (derived from the generative network’s latent variables) and unpredictable sensory features (those with a high reconstruction error during encoding) (Fig. [2](https://www.nature.com/articles/s41562-023-01799-z#Fig2)). In these simulations, the conceptual features are simply a one-to-one copy of latent variable representations. (Since latent variable representations are not stable as the generative network learns, concepts derived from latent variables seem more likely to be stored than the latent variables themselves, so this is a simplification; see section ‘Extended model’ for further details.)\n\nFigure [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5) shows the stages of recall in the extended model after encoding with a lower or higher prediction error threshold. After decomposing the input into its predictable (conceptual) and unpredictable (sensory) features, the autoassociative network performs pattern completion on the combined representation. The prototypical (that is, predicted) image corresponding to the retrieved conceptual features must then be obtained by decoding the associated latent variable representation into an experience via the return projections to the sensory neocortex. Next, the predictable and unpredictable elements are recombined, simply by overwriting the prototypical prediction with any unpredictable elements, via the connections from the sensory features to the sensory neocortex. The extended model is therefore able to exploit the generative network to reconstruct the predictable aspects of the event from its latent variables, storing only those sensory details that were poorly predicted in the autoassociative network. Equally, as the generative network improves, sensory features stored in the hippocampus may no longer differ significantly from the initial schematic reconstruction in the sensory neocortex, signalling that the hippocampal representation is no longer needed.\n\n**Fig. 5: Retrieval dependence on reconstruction error threshold and replay in the extended model.**\n\n[![Image 30: figure 5](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig5_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/5)\n\n**a**, The stages of recall are shown from left to right (see Fig. [2d](https://www.nature.com/articles/s41562-023-01799-z#Fig2)), where each row represents an example scene. Each scene consists of a standard Shapes3D image with the addition of novel features (several white squares overlaid on the image with varying opacity). **b**, Repeating this process with a higher error threshold for encoding (with the same events and partial inputs) means fewer poorly predicted sensory features are stored in the autoassociative MHN, leading to more prototypical recall with increased reconstruction error. **c**, Average reconstruction error and number of sensory features (that is, pixels) stored in the autoassociative MHN against the error threshold for encoding. One hundred images are tested and error bars give the s.e.m. **d**, Replay in the extended model. The autoassociative network retrieves memories when random noise is given as input, as shown for three example inputs (upper row). As above, the square images show the poorly predicted sensory features and the rectangles below these display the latent variable representations (lower row).\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/5)\n\n### Schema-based distortions in the extended model\n\nThe schema-based distortions shown in the basic model result from the generative network and increase with dependence on it, but memory distortions can also have a rapid onset[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\"). In the extended model, even immediate recall involves a combination of conceptual and sensory features, and the presence of conceptual features induces distortions before consolidation of that specific memory.\n\nIn general, recall is biased towards the ‘mean’ of the class soon after encoding due to the influence of the conceptual representations (Fig. [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5)). This is more pronounced when the error threshold for encoding is high, as there is more reliance on the ‘prototypical’ representations, resulting in the recall of fewer novel features. At a lower error threshold, more sensory detail is encoded, that is, the dimension of the memory trace is higher (_r_s(3) = −1, _P_ < 0.001). This results in a lower reconstruction error (_r_s(3) = 1, _P_ < 0.001), indicating lower distortion but at the expense of efficiency.\n\nExternal context further distorts memory. Reference [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 \"Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).\") asked participants to reproduce ambiguous sketches. A context was established by telling the participants that they would see images from a certain category. After a delay, drawings from memory were distorted to look more like members of the context category. Figure [6b](https://www.nature.com/articles/s41562-023-01799-z#Fig6) shows the result of encoding the same ambiguous image with two different externally provided concepts (a cube in the top row, a sphere in the bottom row), represented by the latent variables for each concept, as opposed to the latent variables predicted by the image itself as in Fig. [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5). During recall, the encoded concept is retrieved in the autoassociative network, determining the prototypical scene reconstructed by the generative network. This biases recall towards the class provided as context, mirroring Fig. [6a](https://www.nature.com/articles/s41562-023-01799-z#Fig6).\n\n**Fig. 6: Schema-based distortions: effects of conceptual context in the extended model.**\n\n[![Image 31: figure 6](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig6_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/6)\n\n**a**, Adapted figure from ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 \"Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).\") showing that recall of an ambiguous item (stimulus figure, centre) depends on its context at encoding (word from list 1, left; or list 2, right), as shown by drawing from memory (reproduced figure, far left and far right). **b**, Memory distortions in the extended model, when the original scene (containing an ambiguous blurred shape) is encoded with a given concept (cube, top; sphere, bottom), represented by the latent variables for that class. Then, a partial input is processed by the generative network to produce predicted conceptual features and the sensory features not predicted by the prototype for that concept (in this case, a white square) for input to the autoassociative MHN. However, pattern completion in the MHN reproduces the originally encoded sensory and conceptual features (cube, top; sphere, bottom), and these are recombined to produce the final output, which is distorted towards the encoded conceptual context.\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/6)\n\nWe also simulate the Deese–Roediger–McDermott (DRM) task[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\") in the extended model to demonstrate its applicability to non-image stimuli. In the DRM task, participants are shown lists of words that are semantically related to ‘lure words’ not present in the list; there is a robust finding that false recognition and recall of the lure words occur[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\"). In the extended model, gist-based semantic intrusions arise as a consequence of learning the co-occurrence statistics of words. First, the VAE is trained to reconstruct the sets of words in simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 \"Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016).\") converted to vectors of word counts, representing background knowledge. The system then encodes the experimental lists as the combination of an ‘id\\_n’ term capturing unique spatiotemporal context, and the VAE’s latent representation of each word list (respectively analogous to the stimulus-unique pixels and the VAE’s latent representation of each image in Fig. [5](https://www.nature.com/articles/s41562-023-01799-z#Fig5)). As in the human data, lure words are often but not always recalled when the system is presented with ‘id\\_n’ (Fig. [7a](https://www.nature.com/articles/s41562-023-01799-z#Fig7)), since the latent variable representations that generate the words in the list also tend to generate the lure word. The system also forgets some words and produces additional semantic intrusions. In addition, the chance of recalling the lure word is higher for longer lists, as in human data from ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 \"Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997).\"), as more related words provide a stronger ‘prior’ for the lure (Fig. [7b](https://www.nature.com/articles/s41562-023-01799-z#Fig7)) (_r_s(10) = 0.998, _P_ < 0.001, 95% CI = 0.982, 1.000).\n\n**Fig. 7: Modelling the DRM task.**\n\n[![Image 32: figure 7](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41562-023-01799-z/MediaObjects/41562_2023_1799_Fig7_HTML.png)](https://www.nature.com/articles/s41562-023-01799-z/figures/7)\n\n**a**, First, the VAE is trained to reconstruct simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 \"Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016).\") converted to vectors of word counts, representing background knowledge. The system then encodes the lists as the combination of an ‘id\\_n’ term capturing unique spatiotemporal context, and the VAE’s latent variable representation of the word list. In each plot, recalled stimuli when the system is presented with ‘id\\_n’ are shown, with output scores treated as probabilities so that words with a score \\>0.5 (above dashed lines) are recalled. Words from the stimulus list are shown in blue, and lures in red. See Fig. 1 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for results for the remaining DRM lists. **b**, The chance of recalling the lure word is higher when longer lists are encoded (blue). Each measurement is averaged across 400 trials (20 random subsets of each of the 20 DRM lists), and error bars give the s.e.m. This qualitatively resembles human data from ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 \"Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997).\") (grey).\n\n[Full size image](https://www.nature.com/articles/s41562-023-01799-z/figures/7)\n\n### Modelling brain damage\n\nRecent episodic memory is impaired following damage to the HF, whereas semantic memory, including the semantic content of remote episodes, appears relatively spared. In the model, the semantic form of a consolidated memory survives damage to the HF due to latent variable representations in the mPFC or the alTL (even if those in the EC are lesioned); Fig. [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3) demonstrates how semantic recall performance improves with the age of a memory, reflecting the temporal gradient of retrograde amnesia (see section ‘Modelling semantic memory’). However, these semantic ‘facts’ cannot be used to generate an experience ‘episodically’ without the generative network’s decoder, in agreement with multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 \"Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).\").\n\nThe extent of retrograde amnesia can vary greatly depending in part on which regions of the HF are damaged[98](https://www.nature.com/articles/s41562-023-01799-z#ref-CR98 \"Cipolotti, L. et al. Long-term retrograde amnesia… the crucial role of the hippocampus. Neuropsychologia 39, 151–172 (2001).\"),[99](https://www.nature.com/articles/s41562-023-01799-z#ref-CR99 \"Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. J. Neurosci. 6, 2950–2967 (1986).\"). The dissociation of retrograde and anterograde amnesia in some cases suggests that the circuits for encoding memories and the circuits for recalling them via the HF only overlap partially[99](https://www.nature.com/articles/s41562-023-01799-z#ref-CR99 \"Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. J. Neurosci. 6, 2950–2967 (1986).\"). For example, if the autoassociative network is damaged but not the generative network’s decoder, the generative network can still perform reconstruction of fully consolidated memories. This could explain varying reports of the gradient of retrograde amnesia when assessing episodic recollection (as opposed to semantic memory), if the generative network’s decoder is intact in patients showing spared episodic recollection of early memories[45](https://www.nature.com/articles/s41562-023-01799-z#ref-CR45 \"Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. Cold Spring Harb. Perspect. Biol. 7, a021766 (2015).\"). Note that the location of damage within the generative network’s decoder also affects the resulting deficit in our model. In particular, patients with damage restricted to the hippocampus proper can (re)construct simple scenes but not more complex ones[23](https://www.nature.com/articles/s41562-023-01799-z#ref-CR23 \"Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. Proc. Natl Acad. Sci. USA 104, 1726–1731 (2007).\").\n\nOur model also shows the characteristic anterograde amnesia after hippocampal damage, as the hippocampus is required to initially bind features together and support off-line training of the generative model. Anterograde semantic learning would also be impaired by hippocampal damage (as the generative network is trained by hippocampal replay). While hippocampal replay need not be the only mechanism for schema acquisition, it would probably be much slower without the benefit of replay. However, semantic learning over short timescales may be relatively unimpaired, as it is less dependent on extracting regularities from long-term memory[100](https://www.nature.com/articles/s41562-023-01799-z#ref-CR100 \"Knowlton, B. J., Squire, L. R. & Gluck, M. A. Probabilistic classification learning in amnesia. Learn. Mem. 1, 106–120 (1994).\").\n\nIn semantic dementia, semantic memory is impaired, and remote episodic memory is impaired more than recent episodic memory[101](https://www.nature.com/articles/s41562-023-01799-z#ref-CR101 \"Hodges, J. R. & Graham, K. S. Episodic memory: insights from semantic dementia. Phil. Trans. R. Soc. Lond. B 356, 1423–1434 (2001).\"). This would be consistent with lesions to the generative network, as recent memories can rely more on the hippocampal autoassociative network. However, the exact effects would depend on the distribution of damage across the various potential generative networks in the EC, mPFC and alTL. Of these, the alTL network is associated with semantic dementia, and the posterior medial network (corresponding to the generative network between the sensory areas and the EC) with Alzheimer’s disease[83](https://www.nature.com/articles/s41562-023-01799-z#ref-CR83 \"Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. Nat. Rev. Neurosci. 13, 713–726 (2012).\").\n\nFinally, neuropsychological evidence suggests a distinction between familiarity and recollection, and furthermore a partial dissociation between different tests of familiarity; patients with selective hippocampal damage can exhibit recognition memory deficits in a simple ‘yes/no’ task with similar foils, but not in a ‘forced choice’ variant involving choosing the more familiar stimulus from a set[102](https://www.nature.com/articles/s41562-023-01799-z#ref-CR102 \"Migo, E., Montaldi, D., Norman, K. A., Quamme, J. & Mayes, A. The contribution of familiarity to recognition memory is a function of test format when using similar foils. Q. J.Exp. Psychol. 62, 1198–1215 (2009).\"). This is consistent with the idea that lower prediction error in the neocortical generative network indicates familiarity, but retrieval of unique details from the hippocampus is required for more definitive recognition memory.\n\nDiscussion\n----------\n\nWe have proposed a model of systems consolidation as the training of a generative neural network, which learns to support episodic memory, and also imagination, semantic memory and inference. This occurs through teacher–student learning. The hippocampal ‘teacher’ rapidly encodes an event, which may combine unpredictable sensory elements (with connections to and from the sensory cortex) and predictable conceptual elements (with connections to and from latent variable representations in the generative network). After exposure to replayed representations from the ‘teacher’, the generative ‘student’ network supports reconstruction of events by forming a schematic representation in the sensory neocortex from latent variables via the HF, with unpredictable sensory elements added from the hippocampus.\n\nIn contrast to the relatively veridical initial encoding, the generative model learns to capture the probability distributions underlying experiences, or ‘schemas’. This enables not just efficient recall, reconstructing memories without the need to store them individually, but also imagination (by sampling from the latent variable distributions) and inference (by using the learned statistics of experience to predict the values of unseen variables). In addition, semantic memory (that is, factual knowledge) develops as a by-product of learning to predict sensory experience. As the generative model becomes more accurate, the need to store and retrieve unpredicted details in the hippocampus decreases (producing a gradient of retrograde amnesia in cases of hippocampal damage). However, the generative network necessarily introduces distortion compared to the initial memory system. Multiple generative networks can be trained in parallel, and we expect this to include networks with latent variables in the EC, mPFC and alTL.\n\nWe now compare the model’s performance to the list of key findings from the introduction:\n\n1.  1.Gradual consolidation follows one-shot encoding: A memory is encoded in the hippocampal ‘teacher’ network after a single exposure, and transferred to the generative ‘student’ network after being replayed repeatedly (Fig. [1c,d](https://www.nature.com/articles/s41562-023-01799-z#Fig1)).\n    \n2.  2.Semantic memory becomes hippocampus-independent: The latent variable representations learned by the generative networks constitute the ‘key facts’ of an episode, supporting semantic memory (Fig. [3a](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).\n    \n3.  3.Episodic memory remains hippocampus-dependent: Return projections to the sensory neocortex via the HF are required to decode the latent variable representations into a sensory experience (Fig. [1](https://www.nature.com/articles/s41562-023-01799-z#Fig1)). (EC is required for even simple (re)construction, while the hippocampus proper helps to generate complex conceptually coherent scenes and retrieves unpredictable details that are not yet consolidated into the generative network; see section ‘Neural substrates of the model’.)\n    \n4.  4.Shared substrate for episode generation: Generative models are a common mechanism for episode generation. Familiar scenes can be reconstructed and new ones can be generated by sampling or transforming existing latent variable representations (Fig. [3b–d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)), providing a model for imagination, scene construction and episodic future thinking.\n    \n5.  5.Consolidation promotes inference and generalization: Relational inference corresponds to vector arithmetic applied to the generative network’s latent variables (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).\n    \n6.  6.Episodic memories are distorted: We show how memory distortions arise from the generative network (Figs. [4](https://www.nature.com/articles/s41562-023-01799-z#Fig4), [6](https://www.nature.com/articles/s41562-023-01799-z#Fig6) and [7](https://www.nature.com/articles/s41562-023-01799-z#Fig7)). This extends the model of ref. [32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 \"Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).\") to relate memory distortion to consolidation.\n    \n7.  7.Association cortex encodes latent structure: Latent variable representations in the EC, mPFC, and alTL provide schemas for episodic recollection and imagination (via HF) and for semantic retrieval and inference.\n    \n8.  8.Prediction error affects memory processing: The generative network is constantly calculating the reconstruction error of experiences[58](https://www.nature.com/articles/s41562-023-01799-z#ref-CR58 \"Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. PLoS Biol. 4, e424 (2006).\"),[59](https://www.nature.com/articles/s41562-023-01799-z#ref-CR59 \"Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. Learn. Mem. 18, 523–528 (2011).\"). Events that are consistent with the existing generative model require less encoding in the autoassociative hippocampal network (Fig. [5](https://www.nature.com/articles/s41562-023-01799-z#Fig5)).\n    \n9.  9.Episodic memories include conceptual features: When an experience combines a mixture of familiar and unfamiliar elements, both concepts and poorly predicted sensory elements are stored in the hippocampus via association to a specific memory unit.\n    \n\nOur model can be seen as an update to the complementary learning systems (CLS)[4](https://www.nature.com/articles/s41562-023-01799-z#ref-CR4 \"McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychol. Rev. 102, 419–457 (1995).\") framework to better account for points 3 to 9 above, reconciling the development of semantic representations in the neocortex (as in CLS) with the continued dependence on the hippocampal formation for episodic recall (as in multiple trace theory[11](https://www.nature.com/articles/s41562-023-01799-z#ref-CR11 \"Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. Curr. Opin. Neurobiol. 7, 217–227 (1997).\")). Furthermore, it provides a unified view of: (1) episode generation, (2) how episodic memories change over time and exhibit distortions and (3) how semantic and episodic information are combined in memory. We build on previous work exploring the role of generative networks in consolidation[18](https://www.nature.com/articles/s41562-023-01799-z#ref-CR18 \"Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. Adv. Neural Inf. Process. Syst. 13, 24–30 (2000).\"),[19](https://www.nature.com/articles/s41562-023-01799-z#ref-CR19 \"Káli, S. & Dayan, P. Replay, repair and consolidation. Adv. Neural Inf. Process. Syst. 15, 19–26 (2002).\"), as models of the hippocampal formation[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 \"Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).\"),[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 \"van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020).\"), as priors for episodic memory[35](https://www.nature.com/articles/s41562-023-01799-z#ref-CR35 \"Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. Neural Comput. 34, 1841–1870 (2022).\") and as models of spatial cognition[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\").\n\nA key aspect of the model is that multiple generative networks can be trained concurrently from a single autoassociative network (Fig. [2a](https://www.nature.com/articles/s41562-023-01799-z#Fig2)) and may be optimized for different tasks. Thus, the latent representations in the mPFC and the alTL may be more closely linked to value or language than those in the EC[103](https://www.nature.com/articles/s41562-023-01799-z#ref-CR103 \"Moscovitch, M. & Melo, B. Strategic retrieval and the frontal lobes: evidence from confabulation and amnesia. Neuropsychologia 35, 1017–1034 (1997).\"),[104](https://www.nature.com/articles/s41562-023-01799-z#ref-CR104 \"Lin, W. J., Horner, A. J. & Burgess, N. Ventromedial prefrontal cortex, adding value to autobiographical memories. Sci. Rep. 6, 28630 (2016).\"). These differences may arise from differences in network structure (for example, the degree of compression) or from additional training objectives that shape their representations[105](https://www.nature.com/articles/s41562-023-01799-z#ref-CR105 \"Gluck, M. A. & Myers, C. E. Hippocampal mediation of stimulus representation: a computational theory. Hippocampus 3, 491–516 (1993).\") (for example, the generative network with latent variables in the mPFC might be trained to predict task-relevant value in addition to the EC representations). We expect the generative networks to overlap closer to their sensory inputs/outputs, where general-purpose features are more useful, and diverge as the representations become more abstract (or task-specific if there are additional training objectives)[106](https://www.nature.com/articles/s41562-023-01799-z#ref-CR106 \"Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. & Lipson, H. Understanding neural networks through deep visualization. Preprint at \nhttps://arxiv.org/abs/1506.06579\n(2015).\"). This may involve a primary VAE with latent variables in the EC, with additional pathways from the higher sensory cortex to the EC routed via latent variables in the mPFC or the alTL.\n\nOur model raises some fundamental questions: Does true episodic memory require event-unique detail, and does this require the hippocampus? Or can prototypical predictions qualify as memory rather than imagination? In the model, event-unique details are initially provided by the hippocampus but can also be provided by the generative network. For example, if you know that someone attended your 8th birthday party and gave you a particular gift, these personal semantic facts need not be hippocampal-dependent but could generate a scene with the right event-specific details, which would seem like episodic memory. The increasingly sophisticated generation of images from text using generative models[107](https://www.nature.com/articles/s41562-023-01799-z#ref-CR107 \"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with CLIP latents. Preprint at \nhttps://arxiv.org/abs/2204.06125\n(2022).\") suggests that episode construction from semantic facts is computationally plausible.\n\nEpisodic memories are defined by their unique spatiotemporal context[1](https://www.nature.com/articles/s41562-023-01799-z#ref-CR1 \"Tulving, E. How many memory systems are there? Am. Psychol. 40, 385–398 (1985).\"). In the model, spatial and temporal context correspond to conceptual features captured by place[108](https://www.nature.com/articles/s41562-023-01799-z#ref-CR108 \"O’Keefe, J. & Dostrovsky, J. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. Brain Res. 34, 171–175 (1971).\"),[109](https://www.nature.com/articles/s41562-023-01799-z#ref-CR109 \"Ekstrom, A. D. et al. Human hippocampal theta activity during virtual navigation. Hippocampus 15, 881–889 (2005).\") or time[110](https://www.nature.com/articles/s41562-023-01799-z#ref-CR110 \"Eichenbaum, H. Time cells in the hippocampus: a new dimension for mapping memories. Nat. Rev. Neurosci. 15, 732–744 (2014).\"),[111](https://www.nature.com/articles/s41562-023-01799-z#ref-CR111 \"Umbach, G. et al. Time cells in the human hippocampus and entorhinal cortex support episodic memory. Proc. Natl Acad. Sci. USA 117, 28463–28474 (2020).\") cells in the hippocampus and might be linked to latent variable representations formed in the EC, such as grid cells in the medial EC, which form an efficient basis for locations in real[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[112](https://www.nature.com/articles/s41562-023-01799-z#ref-CR112 \"Dordek, Y., Soudry, D., Meir, R. & Derdikman, D. Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis. Elife 5, e10094 (2016).\"),[113](https://www.nature.com/articles/s41562-023-01799-z#ref-CR113 \"Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippocampus as a predictive map. Nat. Neurosci. 20, 1643–1653 (2017).\") or cognitive spaces[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"),[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 \"Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016).\"), or temporal context representations in the lateral EC[114](https://www.nature.com/articles/s41562-023-01799-z#ref-CR114 \"Tsao, A. et al. Integrating time from experience in the lateral entorhinal cortex. Nature 561, 57–62 (2018).\"),[115](https://www.nature.com/articles/s41562-023-01799-z#ref-CR115 \"Bright, I. M. et al. A temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex. Proc. Natl Acad. Sci. USA 117, 20274–20283 (2020).\"). Events with specific spatial and temporal context can be generated from these latent variable representations, as has been modelled in detail for space[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 \"Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000).\"),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 \"Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007).\"),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\").\n\nMore generally, this work builds on the spatial cognition literature, in which place and head direction cells act as latent variables in a generative model[20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 \"Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000).\"),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 \"Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007).\"),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\"), allowing the generation of a scene from a specific viewpoint. References [20](https://www.nature.com/articles/s41562-023-01799-z#ref-CR20 \"Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. Adv. Neural Inf. Process. Syst. 13, 96–102 (2000).\"),[21](https://www.nature.com/articles/s41562-023-01799-z#ref-CR21 \"Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. Psychol. Rev. 114, 340–375 (2007).\"),[22](https://www.nature.com/articles/s41562-023-01799-z#ref-CR22 \"Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. Elife 7, e33752 (2018).\") explore how egocentric sensory representations could be transformed into allocentric latent variables before storage in the medial temporal lobe and conversely, how egocentric representations could be reconstructed from allocentric ones to support imagery. The latent representations learned through consolidation in our model correspond loosely to the allocentric representations, and the sensory representations produced by HF to the egocentric ones; only egocentric and sensory representations are directly experienced, whereas allocentric and semantic representations are useful abstractions that can also be exploited for efficient hippocampal encoding.\n\nOur model simplifies the true nature of mnemonic processing in several ways. First, the interaction of sensory and conceptual features in the hippocampus and latent variables in the EC during retrieval could be more complex, with each type of representation contributing to pattern completion of the other as in interactions between items and contextual representations in the Temporal Context Model[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 \"Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002).\"), and might iterate over retrievals from both hippocampal and generative networks[50](https://www.nature.com/articles/s41562-023-01799-z#ref-CR50 \"Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016).\"). Second, our model distinguishes between ‘sensory’ and ‘conceptual’ representations in the hippocampus, respectively linked to the sensory neocortex at the input/output of the generative network and to the latent variable layer in the middle. In reality, a gradient of levels of representation in the hippocampus is more likely, from detailed sensory representations to coarse-grained conceptual ones, respectively linked to lower or higher neocortical areas[117](https://www.nature.com/articles/s41562-023-01799-z#ref-CR117 \"Moscovitch, M., Cabeza, R., Winocur, G. & Nadel, L. Episodic memory and beyond: the hippocampus and neocortex in transformation. Annu. Review Psychol. 67, 105–134 (2016).\"), and might map onto the observed functional gradients along the longitudinal axis of the hippocampus[118](https://www.nature.com/articles/s41562-023-01799-z#ref-CR118 \"Strange, B. A., Witter, M. P., Lein, E. S. & Moser, E. I. Functional organization of the hippocampal longitudinal axis. Nat. Rev. Neurosci. 15, 655–669 (2014).\"). Third, our generative network uses back-propagation of the prediction error between output and input patterns to learn. Generative networks with more plausible (if less efficient) learning rules exist[67](https://www.nature.com/articles/s41562-023-01799-z#ref-CR67 \"Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. Neural Comput. 7, 889–904 (1995).\"),[68](https://www.nature.com/articles/s41562-023-01799-z#ref-CR68 \"Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. 2, 79–87 (1999).\"),[69](https://www.nature.com/articles/s41562-023-01799-z#ref-CR69 \"Friston, K. The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. 11, 127–138 (2010).\"), which have the advantage of producing a prediction error signal at each layer (between top–down prediction and bottom–up recognition), potentially allowing learning of concepts and exceptions at all levels of description. Fourth, considering consolidation as a continual lifelong process rather than during encoding of a single dataset introduces new complexities; these include the instability of latent representations and the prevention of catastrophic forgetting of already consolidated memories as new memories are assimilated into the generative network. The model could be extended to address this, for example, by using replay from the generative network as well as from the hippocampal network, which could reduce catastrophic forgetting and stabilize latent variable representations in both networks[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 \"van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020).\"),[119](https://www.nature.com/articles/s41562-023-01799-z#ref-CR119 \"Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. Nat. Neurosci. 7, 286–294 (2004).\"),[120](https://www.nature.com/articles/s41562-023-01799-z#ref-CR120 \"van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at \nhttps://arxiv.org/abs/1809.10635\n(2018).\"), building on previous research on sleep and learning[121](https://www.nature.com/articles/s41562-023-01799-z#ref-CR121 \"Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. Proc. Natl Acad. Sci. USA 119, e2123432119 (2022).\"). Fifth, we model semantic memory as prediction of categorical information for an ‘event’, but future work should model more complex semantic knowledge, for example, by decoding language from latent representations of multimodal stimuli[85](https://www.nature.com/articles/s41562-023-01799-z#ref-CR85 \"Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in Proc. IEEE Conference on Computer Vision and Pattern Recognition 3156–3164 (2015).\"),[86](https://www.nature.com/articles/s41562-023-01799-z#ref-CR86 \"Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at \nhttps://arxiv.org/abs/2111.09734\n(2021).\"). In particular, the relationship between semantic memory for specific ‘events’ and the broader ‘web’ of general knowledge should be considered.\n\nEpisodic memories contain important sequential structure not modelled by our encoding and reconstruction of simple scenes. Future work could expand the model’s scope to sequential information as follows. A range of stimuli could be represented as sequences of arbitrary symbols (including language, spatial trajectories and transitions on a graph). A heteroassociative variant of an MHN, which is better suited to sequential data, could be used to store such stimuli. Specifically, the interpretation of an MHN that we use[64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 \"Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).\") can capture sequential information if the projections from feature units to memory units correspond to the current state, but the projections from memory units back to feature units correspond to the next state so that one state retrieves the next[122](https://www.nature.com/articles/s41562-023-01799-z#ref-CR122 \"Millidge, B., Salvatori, T., Song, Y., Lukasiewicz, T. & Bogacz, R. Universal Hopfield networks: a general framework for single-shot associative memory models. in International Conference on Machine Learning 15561–15583 (PMLR, 2022).\"),[123](https://www.nature.com/articles/s41562-023-01799-z#ref-CR123 \"Chaudhry, H. T., Zavatone-Veth, J. A., Krotov, D. & Pehlevan, C. Long sequence Hopfield memory. Preprint at \nhttps://arxiv.org/abs/2306.04532\n(2023).\"),[124](https://www.nature.com/articles/s41562-023-01799-z#ref-CR124 \"Tang, M., Barron, H. & Bogacz, R. Sequential memory with temporal predictive coding. Adv. Neural Inf. Process. Syst. 27 (2023).\"). With certain modifications based on previous work involving the role of temporal context in memory[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 \"Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002).\"),[125](https://www.nature.com/articles/s41562-023-01799-z#ref-CR125 \"Burgess, N. & Hitch, G. J. Memory for serial order: a network model of the phonological loop and its timing. Psychol. Rev. 106, 551–581 (1999).\"), asymmetric MHNs can store sequences with complex repetitions and temporal correlations, such as language. We could then implement the student model as a sequential generative network trained to predict the next input during sequential replay (for example, GPT-2 (ref. [126](https://www.nature.com/articles/s41562-023-01799-z#ref-CR126 \"Radford, A. et al. Language models are unsupervised multitask learners. OpenAI Blog \nhttps://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n(2019).\"))). Such networks capture relational structure, developing grid-like latent representations in spatial tasks[31](https://www.nature.com/articles/s41562-023-01799-z#ref-CR31 \"Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell 183, 1249–1263 (2020).\"), and learn the gist of narratives. The sequential model could also be applied to phenomena such as event segmentation[127](https://www.nature.com/articles/s41562-023-01799-z#ref-CR127 \"Bird, C. M. How do we remember events? Curr. Opin. Behav. Sci. 32, 120–125 (2020).\") and memory distortions in narratives[6](https://www.nature.com/articles/s41562-023-01799-z#ref-CR6 \"Bartlett, F. C. Remembering: A Study In Experimental and Social Psychology (Cambridge Univ. Press, 1932).\"). (Note that for more complex sequential data such as videos, pattern completion of both the current stimulus and the next stimulus would be required, potentially needing a combination of autoassociative and heteroassociative connectivity in the hippocampal network.)\n\nOur model makes testable predictions. First, if participants learn stimuli generated from known latent variables, it predicts that these specific latent variable representations should develop in the association cortex over time (and that this representation would support, for example, vector arithmetic and interpolation). This could be tested by representational similarity analysis, which should reveal a more conceptual similarity structure developing in the association cortex through consolidation, as opposed to a similarity structure reflecting the sensory stimuli in earlier sensory cortices. If the stimuli also contained slight variation, that is, they were not entirely described by the latent variables, the development of a latent variable representation should be correlated with gist-based distortions in memory and anti-correlated with hippocampal processing of unpredictable elements.\n\nSecond, the model makes multiple predictions about the effects of brain damage. Just as boundary extension is reduced in patients with damage to the HF[128](https://www.nature.com/articles/s41562-023-01799-z#ref-CR128 \"Mullally, S. L., Intraub, H. & Maguire, E. A. Attenuated boundary extension produces a paradoxical memory advantage in amnesic patients. Curr. Biol. 22, 261–268 (2012).\") or the vmPFC[129](https://www.nature.com/articles/s41562-023-01799-z#ref-CR129 \"De Luca, F. et al. Boundary extension is attenuated in patients with ventromedial prefrontal cortex damage. Cortex 108, 1–12 (2018).\"), we predict that other biases towards the ‘canonical view’ would be attenuated in such patients; for example, healthy controls would distort images with an atypical viewing angle towards a more typical angle in memory, but this would be reduced in, for example, hippocampal patients. Similarly, ambiguous images such as the duck/rabbit drawing ‘flip’ between interpretations in perception but are stable when held in imagery[72](https://www.nature.com/articles/s41562-023-01799-z#ref-CR72 \"Chambers, D. & Reisberg, D. Can mental images be ambiguous? J. Exp. Psychol. Hum. Percept. Perform. 11, 317–328 (1985).\"), presumably due to maintained hippocampal conceptual representations. We predict that this conceptual stability in imagery would also be reduced in such patients. This could also extend to non-scene stimuli: if the ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 \"Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).\") task were tested with both healthy controls and patients with damage to the generative decoder, we would predict reduced contextual distortion in the latter. Furthermore, patients with an inaccurate generative model, for example, due to semantic dementia, might rely more on sensory features to compensate. (Note that the pattern of deficits would depend on both the nature of the priors encoded in the generative network and the error threshold for encoding. In some cases, damage to the generative network could produce atypical ‘priors’ rather than suppressing them. Thus, if the generative network is inaccurate but the error threshold for encoding is high, atypical distortions will be observed rather than a reduction in conceptual distortions.)\n\nThird, the model suggests that the error threshold for encoding could vary depending on the importance of the stimuli or the amount of attentional resource available. For example, emotional salience could lower this threshold, with traumatic memories being encoded in greater sensory detail and with less contextual coherence[130](https://www.nature.com/articles/s41562-023-01799-z#ref-CR130 \"Van Der Kolk, B. A., Burbridge, J. A. & Suzuki, J. The psychobiology of traumatic memory. Clinical implications of neuroimaging studies. Ann. N. Y. Acad. Sci. 821, 99–113 (1997).\"),[131](https://www.nature.com/articles/s41562-023-01799-z#ref-CR131 \"Bisby, J. A., Burgess, N. & Brewin, C. R. Reduced memory coherence for negative events and its relationship to posttraumatic stress disorder. Curr. Dir. Psychol. Sci. 29, 267–272 (2020).\"). Equally, conditions such as autism spectrum disorder, which are potentially attributable to hypo-priors[132](https://www.nature.com/articles/s41562-023-01799-z#ref-CR132 \"Pellicano, E. & Burr, D. When the world becomes ‘too real’: a Bayesian explanation of autistic perception. Trends Cogn. Sci. 16, 504–510 (2012).\"), might be associated with a lower prediction error threshold for veridical storage (and thus reduced conceptual influence on memory and increased sensory detail). In addition, reality monitoring deficits would change the perceived prediction error relative to reality, leading to atypical memory storage (for example, a reduced ability to compensate for prediction errors by storing sensory details).\n\nFourth, biological intelligence excels at generalizing from only a small number of examples. The model predicts that learning to generalize rapidly benefits from having a generative model that can create new examples, for example, by inferring variants (as in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) (see also ref. [133](https://www.nature.com/articles/s41562-023-01799-z#ref-CR133 \"Barry, D. N. & Love, B. C. A neural network account of memory replay and knowledge consolidation. Cereb. Cortex. 33, 83–95 (2022).\")). Finally, the model suggests a link between latent spaces and cognitive maps[134](https://www.nature.com/articles/s41562-023-01799-z#ref-CR134 \"Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron 100, 490–509 (2018).\"). For example, one might predict that the position of a memory in latent space is reflected in place and grid cell firing, as observed for other conceptual representations[54](https://www.nature.com/articles/s41562-023-01799-z#ref-CR54 \"Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 1464–1468 (2016).\"),[134](https://www.nature.com/articles/s41562-023-01799-z#ref-CR134 \"Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron 100, 490–509 (2018).\"),[135](https://www.nature.com/articles/s41562-023-01799-z#ref-CR135 \"Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. Nature 595, 80–84 (2021).\").\n\nIn summary, our proposed model takes inspiration from recent advances in machine learning to capture many of the intriguing phenomena associated with episodic memory, its (re)constructive nature, its relationship to schemas, and consolidation, as well as aspects of imagination, inference and semantic memory.\n\nMethods\n-------\n\n### Data\n\nIn the simulations, images represent events (except for the DRM[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\") task stimuli). The Shapes3D dataset[136](https://www.nature.com/articles/s41562-023-01799-z#ref-CR136 \"Burgess, C. & Kim, H. 3D Shapes dataset. GitHub \nhttps://github.com/google-deepmind/3d-shapes\n(2018).\") was used throughout, except for the use of MNIST[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 \"LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).\") to explore certain distortions. Note that one MHN was used per dataset, and one generative model was trained per dataset from the corresponding MHN’s outputs.\n\n### Basic model\n\nIn our model, the hippocampus rapidly encodes an event, modelled as one-shot memorization in an autoassociative network (an MHN). Then, generative networks are trained on replayed representations from the autoassociative network, learning to reconstruct memories by capturing the statistical structure of experienced events.\n\nThe generative networks used are variational autoencoders, a type of autoencoder with special properties such that randomly sampling values for the latent variables in the model’s ‘bottleneck’ layer generates valid stimuli[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 \"Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at \nhttps://arxiv.org/abs/1312.6114\n(2013).\"). Figure 3 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1), adapted from ref. [137](https://www.nature.com/articles/s41562-023-01799-z#ref-CR137 \"Hou, X., Shen, L., Sun, K. & Qiu, G. Deep feature consistent variational autoencoder. in 2017 IEEE Winter Conference on Applications of Computer Vision (WACV) 1133–1141 (IEEE, 2017).\"), shows how directions in the latent space can correspond to meaningful transformations. While most diagrams show the VAE’s input and output layers in the sensory neocortex as separated (in line with conventions for visualizing neural networks), it is important to note that the input and output layers are in fact the same, as shown in Fig. [1b](https://www.nature.com/articles/s41562-023-01799-z#Fig1). There may be considerable overlap between the encoder and decoder, especially closer to the sensory neocortex, but we did not model this explicitly. The autoassociative model is an MHN, with the property that even random input values will retrieve one of the stored patterns via pattern completion. Specifically, we considered the biological interpretation of the MHN as feature units and memory units suggested by ref. [64](https://www.nature.com/articles/s41562-023-01799-z#ref-CR64 \"Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in International Conference on Learning Representations (2021).\") (see [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details).\n\nWe modelled consolidation as teacher–student learning, where the autoassociative network is the ‘teacher’ and the generative network is the ‘student’ trained on replayed representations from the ‘teacher’. We gave random noise (consisting of uniformly sampled values in each channel for each pixel) as an input to the MHN, then used the outputs of the network to train the VAE. (These outputs represent the high-level sensory representations activated by hippocampal pattern completion, via return projections to the sensory cortex.) The noise input to the autoassociative network could potentially represent random activation during sleep[138](https://www.nature.com/articles/s41562-023-01799-z#ref-CR138 \"Stella, F., Baracskay, P., O’Neill, J. & Csicsvari, J. Hippocampal reactivation of random trajectories resembling Brownian diffusion. Neuron 102, 450–461 (2019).\"),[139](https://www.nature.com/articles/s41562-023-01799-z#ref-CR139 \"González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E. & Bazhenov, M. Can sleep protect memories from catastrophic forgetting? Elife 9, e51005 (2020).\"),[140](https://www.nature.com/articles/s41562-023-01799-z#ref-CR140 \"Pezzulo, G., Zorzi, M. & Corbetta, M. The secret life of predictive brains: what’s spontaneous activity for? Trends Cogn. Sci. 25, 730–743 (2021).\"). Attributes such as reward salience might also influence which memories are replayed but were not modelled here[141](https://www.nature.com/articles/s41562-023-01799-z#ref-CR141 \"Igata, H., Ikegaya, Y. & Sasaki, T. Prioritized experience replays on a hippocampal predictive map for learning. Proc. Natl Acad. Sci. USA 118, e2011266118 (2021).\").\n\nDuring the encoding state in our simulations, images were stored in a continuous MHN with high inverse temperature, _β_, set to 20 (higher values of _β_ produce attractor states corresponding to individual memories, while lower values of _β_ make metastable states more likely). Reference [63](https://www.nature.com/articles/s41562-023-01799-z#ref-CR63 \"Ramsauer, H. et al. Hopfield networks is all you need. in International Conference on Learning Representations (2021).\") provides an excellent Python implementation of MHNs that we used in our code. During the ‘rest’ state, random noise was given as an input _N_ times to the MHN, retrieving _N_ attractor states from the network. (The distribution of retrieved attractor states was not tested but was approximately random, and very few spurious attractors were observed with sufficiently high inverse temperature.) In the main simulations, 10,000 items from the Shapes3D dataset were encoded in the MHN, and 10,000 replayed states were used to train the VAE (that is, _N_ is 10,000). (Rather than replaying new samples from the MHN at each epoch of the VAE’s training, a single set of samples was used for efficiency and simplicity.)\n\nA VAE was then trained on the ‘replayed’ images from the MHN, using the Keras API for TensorFlow[142](https://www.nature.com/articles/s41562-023-01799-z#ref-CR142 \"Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) 265–283 (USENIX Assoc., 2016).\"). The loss function (that is, the error minimized through training) is the sum of two terms, the reconstruction error and the Kullback–Leibler divergence[65](https://www.nature.com/articles/s41562-023-01799-z#ref-CR65 \"Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at \nhttps://arxiv.org/abs/1312.6114\n(2013).\"); the former encourages accurate reconstruction, while the latter (which measures the divergence between the latent variables and a Gaussian distribution) encourages a latent space one can sample from. Specifically, the reconstruction loss in our model is a mean absolute error loss. (Note that the terms reconstruction error and prediction error are used interchangeably throughout.)\n\nThe stochastic gradient descent method used was the AMSGrad variant of the Adam optimizer with early stopping enabled, for a maximum of 50 epochs (where an epoch is a complete pass through the training set). A latent variable vector length of 20, learning rate of 0.001 and Kullback–Leibler weighting of 1 were used in the main results. The variational autoencoders were not optimized for performance, as their purpose is illustrative (more data and hyperparameter tuning would be likely to improve reconstruction accuracy). Architectural choices within the VAE were not principled but were based on successful architectures for similar stimuli in the literature. See [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details of the VAE’s architecture. The VAEs were trained using gradient descent and back-propagation as usual; while this method is biologically implausible due to its non-local nature, more plausible learning algorithms might be feasible[143](https://www.nature.com/articles/s41562-023-01799-z#ref-CR143 \"Whittington, J. C. R. & Bogacz, R. Theories of error back-propagation in the brain. Trends Cogn. Sci. 23, 235–250 (2019).\").\n\nWhile this was not modelled explicitly, once the generative network’s reconstruction error is sufficiently low, the hippocampal trace is unnecessary. As a result, it could be ‘marked for deletion’ or overwritten in some way, freeing up capacity for new encodings. However, we did not simulate decay, deletion or capacity constraints in the autoassociative memory part of the model. In these simulations, the main cause of forgetting would be interference from new memories in the generative model.\n\nNote that throughout the simulations, the input to recall was a noisy version of the encoded stimulus image. Specifically, noise was added by replacing a random fraction (0.1 unless stated otherwise) of values in the image array with zero.\n\nWhile we used only one modality at a time (imagery for the majority of simulations, text for the DRM task), our model is compatible with the multimodal nature of experience, as multimodal inputs to VAEs are possible, which result in a multimodal latent space[144](https://www.nature.com/articles/s41562-023-01799-z#ref-CR144 \"Khattar, D., Goud, J. S., Gupta, M. & Varma, V. Mvae: multimodal variational autoencoder for fake news detection. in The World Wide Web Conference 2915–2921 (ACM, 2019).\"). This could reflect the multimodal nature of concept cells in the hippocampus[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 \"Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012).\").\n\n### Modelling semantic memory\n\nWe modelled semantic memory as the ability to decode latent variables into semantic information without the need to reconstruct the event episodically.\n\nDecoding accuracy was measured by training a support vector machine to classify the central object’s shape from the network’s latent variables, using 200 examples at the end of each epoch and measuring classification accuracy on a held-out test set. (Notably, there was good performance with only a small amount of training data when decoding the latent variables, compared with decoding alternative representations such as the sensory input or intermediate layer activations, that is, few-shot learning is possible by making use of compressed ‘semantic’ representations. See Fig. 2 of [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1).)\n\n### Modelling imagination and inference\n\nIn the generative network, new items can either be generated from externally specified (or randomly sampled) latent variables (imagination), or by transforming the latent variable representations of specific events (relational inference). The former was simulated by sampling from categories in the latent space, then decoding the results (Fig. [3d](https://www.nature.com/articles/s41562-023-01799-z#Fig3)). The latter was simulated by interpolating between the latent representations of events (Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3)) or by doing vector arithmetic in the latent space (Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3)).\n\nExamples of the four different object shapes were generated by Monte Carlo sampling for simplicity, that is, samples from the latent space were classified by the semantic decoding classifier, and examples that activate each category are displayed. (Note that there are many alternative ways to do this, for example, by extracting the decision boundaries from the classifier and sampling within the region corresponding to each class.) Generating imagined scenes from more naturalistic inputs, for example, natural language descriptions, would require a much more sophisticated text to the latent space model, but recent machine learning advances suggest that this is possible[145](https://www.nature.com/articles/s41562-023-01799-z#ref-CR145 \"Ramesh, A. et al. Zero-shot text-to-image generation. in International Conference on Machine Learning 8821–8831 (PMLR, 2021).\").\n\nTo demonstrate interpolation, each row of Fig. [3c](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows items generated from latent variables along a line in the latent space between two real items from the training data. To demonstrate vector arithmetic, each equation in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3) shows ‘result = vector_A_ + (vector_B_ − vector_C_)’ (reflecting relational inference problems of the form ‘what is to _A_ as _B_ is to _C_?’), where the result is produced by taking the relation between vector_B_ and vector_C_, applying that to vector_A_ and decoding the result. In other words, the three items on the right of each equation in Fig. [3b](https://www.nature.com/articles/s41562-023-01799-z#Fig3) are real items from the training data. Their latent variable representations are combined as vectors according to the equation shown, giving the latent variable representation from which the first item is generated. Thus, the pair in brackets describes a relation that is applied to the first item on the right to produce the new item on the left of the equation.\n\n### Modelling schema-based distortions\n\nItems recalled by the generative network become more prototypical, a form of schema-based distortion. This can be shown simply in the basic model, using the MNIST digits dataset[87](https://www.nature.com/articles/s41562-023-01799-z#ref-CR87 \"LeCun, Y., Cortes, C. & Burges, C. J. MNIST Handwritten Digit Database (AT&T Labs, 2010).\") to exemplify ten clearly defined classes of items (Fig. [4](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). To show this quantitatively, we calculated the intra-class variation, measured as the mean variance per pixel, within each MNIST class before and after recall, for 5,000 images from the test set. As expected, the intra-class variation was smaller for the recalled items than for the original inputs. (See [Supplementary Information](https://www.nature.com/articles/s41562-023-01799-z#MOESM1) for details of the model architecture.)\n\nTo visualize this, we projected the pixel and latent spaces before and after recall (of 2,000 images from the MNIST test set) into two dimensions (2D) with uniform manifold approximation and projection (UMAP)[146](https://www.nature.com/articles/s41562-023-01799-z#ref-CR146 \"McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at \nhttps://arxiv.org/abs/1802.03426\n(2018).\"), a dimensionality reduction method, and colour-coded them by class (Fig. [4c,d](https://www.nature.com/articles/s41562-023-01799-z#Fig4)). The pixel space of MNIST digits (bottom row) and the latent space of their encodings (top row) showed more compact clusters for the generative network’s outputs (Fig. [4d](https://www.nature.com/articles/s41562-023-01799-z#Fig4)) than for its inputs (Fig. [4c](https://www.nature.com/articles/s41562-023-01799-z#Fig4)).\n\n### Modelling boundary extension and contraction\n\nBoundary extension is the tendency to remember a wider field of view than was observed for certain stimuli[88](https://www.nature.com/articles/s41562-023-01799-z#ref-CR88 \"Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. J. Exp. Psychol. Learn. Mem. Cogn. 15, 179–187 (1989).\"), while boundary contraction is the tendency to remember a narrower one[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\"). Whether boundaries are extended or contracted seems to depend on the perceived distance of the central object, with unusually close-up (that is, ‘object-oriented’) views causing boundary extension, and unusually far away (that is, ‘scene-oriented’) views causing boundary contraction[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\").\n\nWe tested boundary extension and contraction in the basic model by giving it a range of artificially ‘zoomed in’ or ‘zoomed out’ images, adapted from Shapes3D scenes not seen during training, and observing the outputs. The ‘zoomed in’ view was produced by removing _n_ pixels from the margin. The ‘zoomed out’ view was produced by extrapolating the pixels at the margin outwards by _n_ additional pixels. (In both cases, the new images were then resized to the standard size.) The zoom level is the ratio of the central object size in the output image to the size in the original image, given as a percentage; for example, an image with a zoom level of 80% or a ratio of 0.8 was produced by adding a margin so that the object size is 80% of the original size. As the Shapes3D images are of width and height 64, the number of pixels to add or remove was calculated as ‘margin = (32/ratio) − 32’.\n\nIn Fig. [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4), the change in object size between the noisy input and output was estimated as follows: first the image was converted to a few colours by _k_\\-means clustering of pixels. Then, the colour of the central object was determined by finding the predominant colour in a particular central region of the image. A 1D array of pixels corresponding to a vertical line at the horizontal midpoint of the image was processed to identify the fraction of pixels of the central object colour. This enabled us to calculate the change in object size, which we plotted against the degree of ‘zoom’. (For this object size estimation approach to work, we filtered the Shapes3D dataset to images where the object colour was different from both the wall and floor colour, and additionally to cubes to minimize shadow.)\n\nNote that the measure of boundary extension vs contraction displayed in Fig. [4f](https://www.nature.com/articles/s41562-023-01799-z#Fig4), reproduced from ref. [92](https://www.nature.com/articles/s41562-023-01799-z#ref-CR92 \"Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. J. Vis. \nhttps://doi.org/10.1167/jov.21.9.2124\n(2021).\"), was not based on the degree of distortion, but was produced by averaging ‘closer’ vs ‘further’ judgements of an identical stimulus image in comparison to the remembered image. This differs from our measure in Fig. [4g](https://www.nature.com/articles/s41562-023-01799-z#Fig4), which instead corresponds to the drawing-based measure in ref. [89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\"); however, these measures have been shown to be correlated[89](https://www.nature.com/articles/s41562-023-01799-z#ref-CR89 \"Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. Curr. Biol. 30, 537–543 (2020).\").\n\nFigure [4e](https://www.nature.com/articles/s41562-023-01799-z#Fig4) shows a few examples of boundary extension and contraction. In the left- and right-hand images of each set, the margin _n_ is chosen such that the central object is 80% and 120% of the original size, respectively.\n\n### Extended model\n\nThe extended model was designed to capture the fact that memory traces in the hippocampus bind together a mixture of sensory and conceptual elements, with the latter encoded by concept cells[61](https://www.nature.com/articles/s41562-023-01799-z#ref-CR61 \"Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. Nat. Rev. Neurosci. 13, 587–597 (2012).\"), and the fact that schemas shape the reconstruction of memories even before consolidation, as shown by the rapid onset of schema-based distortions[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\").\n\nIn the extended model, each scene was initially encoded as the combination of a predictable and an unpredictable component. The predictable component consisted of concepts captured by the latent variables of the generative network, and the unpredictable component consisted of parts of the stimuli that were poorly predicted by the generative network. Thus, the MHN model has both conceptual and sensory feature units, which store the predictable and unpredictable aspects of memory, respectively. While memories may eventually become fully dependent on the generative model, consolidation can be a prolonged process during which the generative network provides schemas for reconstruction and the autoassociative network supports new or detailed information not yet captured by schemas. (The VAE trained in the basic model simulations was used in the extended model simulations described below.)\n\nHow did encoding work in our simulations? For a new image, the prediction error of each pixel was calculated by the VAE (simply the magnitude of the difference between the VAE’s input and output). Those pixels with a reconstruction error above the threshold constituted the unpredictable component, while the VAE’s latent variables constituted the predictable component, and these components were combined into a single vector and encoded in the MHN. Note that when the threshold is zero, the reconstruction is guaranteed to be perfect, but as the threshold increases, the reconstruction decreases in accuracy.\n\nHow did recall work before full consolidation? After decomposing the input into its predictable (conceptual) and unpredictable (sensory) components, as described above, the autoassociative network could retrieve a memory. The image corresponding to the conceptual component was then obtained by decoding the stored latent variables. Next, the predictable and unpredictable elements were recombined, simply by overwriting the initial schematic reconstruction in the sensory neocortex with any stored (that is, non-zero) sensory features in the hippocampus. Figure [5a,b](https://www.nature.com/articles/s41562-023-01799-z#Fig5) shows this process. The lower the error threshold for encoding sensory details, the more information was stored in the autoassociative network, reducing the reconstruction error of recall (see also section ‘Modelling schema-based distortions’).\n\nHow did replay work? When the autoassociative network was given random noise, both the unpredictable elements and the corresponding latent variables were retrieved. In Fig. [5d](https://www.nature.com/articles/s41562-023-01799-z#Fig5), the square images show the unpredictable elements of MNIST images and the rectangles below these display the vector of latent variables. (As the generative model improves, the presence of hippocampal sensory features that no longer differ from the initial reconstruction indicates that the hippocampal representation is no longer needed, but this was not simulated explicitly.)\n\nWe note that the latent variable representation is not stable as the generative network learns. If some latent variables are stored in the autoassociative network while the VAE continues to change, the quality of the VAE’s reconstruction will gradually worsen; this is also a feature of previous models[42](https://www.nature.com/articles/s41562-023-01799-z#ref-CR42 \"Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. Proc. Natl Acad. Sci. USA 118, e2018422118 (2021).\"). Some degree of degradation may reflect forgetting, but consolidation can be a prolonged process and hippocampal representations can persist in this time. Therefore, we think that concepts derived from latent variables are more likely to be stored than the latent variables themselves, promoting the stability of hippocampal representations. (For example, in humans, language provides a set of relatively persistent concepts, stabilized by the need to communicate.) Projections from the latent variables can classify attributes with only a small amount of training data (see section ‘Modelling semantic memory’); we suggest that there could be a two-way mapping between latent variables and concepts, which supports categorization of incoming experience as well as semantic memory. However, for simplicity, the conceptual features were simply a one-to-one copy of latent variable representations in these simulations. It may also be possible to stabilize the latent variable representations by reducing catastrophic forgetting in the generative network, for example, by using generative as well as hippocampal replay[33](https://www.nature.com/articles/s41562-023-01799-z#ref-CR33 \"van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. Nat. Commun. 11, 4069 (2020).\"),[119](https://www.nature.com/articles/s41562-023-01799-z#ref-CR119 \"Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. Nat. Neurosci. 7, 286–294 (2004).\"),[120](https://www.nature.com/articles/s41562-023-01799-z#ref-CR120 \"van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at \nhttps://arxiv.org/abs/1809.10635\n(2018).\"), with the generative network trained on its own self-generated representations in addition to new memories. This builds on previous research suggesting that certain stages of sleep are optimized to preserve remote memories, while others consolidate new ones[121](https://www.nature.com/articles/s41562-023-01799-z#ref-CR121 \"Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. Proc. Natl Acad. Sci. USA 119, e2123432119 (2022).\"). This could reduce interference of new learning with remote memories in the generative network, as well as make hippocampal representations in the extended model more stable.\n\n### Modelling schema-based distortions in the extended model\n\n#### Carmichael experiment\n\nWe demonstrated the contextual modulation of memory (as in ref. [95](https://www.nature.com/articles/s41562-023-01799-z#ref-CR95 \"Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. J. Exp. Psychol. 15, 73–86 (1932).\")) in the extended model by manipulating the conceptual component of an ‘event’. To model an external conceptual context being encoded, the original image was stored in the autoassociative network along with activation of a given concept (a cube or a sphere), represented as the latent variables for that class. While in most simulations the latent variables stored in the MHN were simply the output of the VAE’s encoder, here an external context activated the conceptual representation, consistent with activity in the EC, mPFC or alTL driven by extrinsic factors.\n\nDuring recall, a noisy input was processed by the generative network to produce a predicted conceptual feature and the sensory features not predicted by the prototype for that concept, for input to the autoassociative MHN. Pattern completion in the MHN produced the originally encoded sensory and conceptual features, and these were recombined to produce the final output.\n\n#### DRM experiment\n\nThe DRM task is a classic way to measure gist-based memory distortion[93](https://www.nature.com/articles/s41562-023-01799-z#ref-CR93 \"Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. J. Exp. Psychol. 58, 17–22 (1959).\"),[94](https://www.nature.com/articles/s41562-023-01799-z#ref-CR94 \"Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. J. Exp. Psychol. Lear. Mem. Cogn. 21, 803–814 (1995).\"). Here we demonstrated the rapid onset of semantic intrusions in the extended model, coming about as a consequence of learning the co-occurrence statistics of words in a text dataset representing ‘background knowledge’. This followed on from previous work showing that VAEs produce semantic intrusions[32](https://www.nature.com/articles/s41562-023-01799-z#ref-CR32 \"Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. PLoS Comput. Biol. 16, e1008367 (2020).\").\n\nIn brief, the DRM task involved showing participants a list of words that were semantically related to a ‘lure word’, which was not present in the list. There was a tendency for both false recognition and false recall of the lure word. We focused on modelling the recall task, but the same model could be extended to recognition (with recognition memory measured by the reconstruction error of the network).\n\nThe generative network was pre-trained on a set of word lists extracted from simple stories[96](https://www.nature.com/articles/s41562-023-01799-z#ref-CR96 \"Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds Knight, K. et al.) 839–849 (2016).\"), representing learning from replayed memories before the DRM stimuli (although replay was not simulated explicitly). Words occurring in <0.05% or \\>10% of documents were discarded to keep the vocabulary to a manageable size of 4,206 words (this meant that some rarer words in the DRM lists were removed). The word lists were converted to vectors of word counts of length 4,206, in which the value at index _i_ of the vector for a given list indicated the count of word _i_ in the document. As these representations ignore word order, a sequential model was not required (however, this prevented exploring the effect of list position on recall).\n\nSpecifically, the variational autoencoder used for this simulation consisted of an input layer followed by a dropout layer[147](https://www.nature.com/articles/s41562-023-01799-z#ref-CR147 \"Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15, 1929–1958 (2014).\") projecting to 300 latent variables (sampled from representations of the mean and log variance vectors as usual), and then to an output layer with a sigmoid activation so that predictions were between 0 and 1, with L1 regularization to promote sparsity in this layer. As above, this was implemented using the Keras API for the TensorFlow library[142](https://www.nature.com/articles/s41562-023-01799-z#ref-CR142 \"Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) 265–283 (USENIX Assoc., 2016).\"),[148](https://www.nature.com/articles/s41562-023-01799-z#ref-CR148 \"Chollet, F. et al. Keras Documentation (GitHub, 2015).\"), with the VAE trained to reconstruct input vectors in the usual way.\n\nFollowing pre-training of the generative network, the system encoded the DRM stimuli, with each of the 20 word lists represented as vectors of word counts. One important detail was the addition of a term, given by ‘id\\_n’ for the _n_th document in the corpus, representing the unique spatiotemporal context of each word list. (Note that this is a highly simplified representation of the spatiotemporal context[116](https://www.nature.com/articles/s41562-023-01799-z#ref-CR116 \"Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. J. Math. Psychol. 46, 269–299 (2002).\") for illustration.) This enabled recall to be modelled by presenting the network with the ‘id\\_n’ term, and seeing which terms were retrieved.\n\nIn the extended model, the latent representation of the word list was encoded in the MHN as the conceptual component, while the unique ‘id\\_n’ terms were encoded veridically (as vectors of word counts of length 4,226—the original vocabulary size plus the 20 new ‘id\\_n’ terms—with 1 at ‘id\\_n’ and 0 elsewhere). The sparse vector representing the unexpected ‘id\\_n’ term is analogous to the sparse arrays of poorly predicted pixels in the main simulations of the extended model.\n\nWhen the MHN was given ‘id\\_n’ as an input, it retrieved the hippocampal trace consisting of ‘id\\_n’ together with the latent representation of the word list. The latent representation was then decoded to produce the outputs shown in Fig. [7a](https://www.nature.com/articles/s41562-023-01799-z#Fig7) (a dashed line shows the threshold for recall, interpreting the output as a probability so that words with an output \\>0.5 are recalled). As in the human data, lure words were often but not always recalled. The system also forgot some words and produced additional semantic intrusions, for example, ‘vet’ in the case of the ‘doctor’ list.\n\nTo test the effect of varying the number of associates, as in ref. [97](https://www.nature.com/articles/s41562-023-01799-z#ref-CR97 \"Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. Psychol. Sci. 8, 231–237 (1997).\"), subsets of the DRM lists were encoded in the way described above. Specifically, to test the probability of lure recall with _n_ associates studied, _n_ items from each DRM list were encoded. For each list, this was repeated for 20 randomly sampled combinations of _n_ items. Once again, recall was tested by giving the system ‘id\\_n’ as an input.\n\n### Reporting summary\n\nFurther information on research design is available in the [Nature Portfolio Reporting Summary](https://www.nature.com/articles/s41562-023-01799-z#MOESM2) linked to this article.\n\nData availability\n-----------------\n\nCode availability\n-----------------\n\nCode for all simulations can be found at [https://github.com/ellie-as/generative-memory](https://github.com/ellie-as/generative-memory). Some diagrams were created using [BioRender.com](http://biorender.com/).\n\nReferences\n----------\n\n1.  Tulving, E. How many memory systems are there? _Am. Psychol._ **40**, 385–398 (1985).\n    \n    [Article](https://doi.org/10.1037%2F0003-066X.40.4.385)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20many%20memory%20systems%20are%20there%3F&journal=Am.%20Psychol.&doi=10.1037%2F0003-066X.40.4.385&volume=40&pages=385-398&publication_year=1985&author=Tulving%2CE)\n    \n2.  Marr, D. A theory for cerebral neocortex. _Proc. R. Soc. Lond. B_ **176**, 161–234 (1970).\n    \n    [Article](https://doi.org/10.1098%2Frspb.1970.0040)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaE3M%2Fjs1OgtQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=4394740)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20theory%20for%20cerebral%20neocortex&journal=Proc.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frspb.1970.0040&volume=176&pages=161-234&publication_year=1970&author=Marr%2CD)\n    \n3.  Marr, D. Simple memory: a theory for archicortex. _Phil. Trans. R. Soc. Lond. B_ **262**, 23–81 (1971).\n    \n    [Article](https://doi.org/10.1098%2Frstb.1971.0078)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaE38%2FltlKltg%3D%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Simple%20memory%3A%20a%20theory%20for%20archicortex&journal=Phil.%20Trans.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frstb.1971.0078&volume=262&pages=23-81&publication_year=1971&author=Marr%2CD)\n    \n4.  McClelland, J. L., McNaughton, B. L. & O’Reilly, R. C. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. _Psychol. Rev._ **102**, 419–457 (1995).\n    \n    [Article](https://doi.org/10.1037%2F0033-295X.102.3.419)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7624455)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Why%20there%20are%20complementary%20learning%20systems%20in%20the%20hippocampus%20and%20neocortex%3A%20insights%20from%20the%20successes%20and%20failures%20of%20connectionist%20models%20of%20learning%20and%20memory&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.102.3.419&volume=102&publication_year=1995&author=McClelland%2CJL&author=McNaughton%2CBL&author=O%E2%80%99Reilly%2CRC)\n    \n5.  Teyler, T. J. & DiScenna, P. The hippocampal memory indexing theory. _Behav. Neurosci._ **100**, 147–154 (1986).\n    \n    [Article](https://doi.org/10.1037%2F0735-7044.100.2.147)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=3008780)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20hippocampal%20memory%20indexing%20theory&journal=Behav.%20Neurosci.&doi=10.1037%2F0735-7044.100.2.147&volume=100&publication_year=1986&author=Teyler%2CTJ&author=DiScenna%2CP)\n    \n6.  Bartlett, F. C. _Remembering: A Study In Experimental and Social Psychology_ (Cambridge Univ. Press, 1932).\n    \n7.  Schacter, D. L. Constructive memory: past and future. _Dialogues Clin. Neurosci._ **14**, 7–18 (2012).\n    \n    [Article](https://doi.org/10.31887%2FDCNS.2012.14.1%2Fdschacter)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22577300)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3341652)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Constructive%20memory%3A%20past%20and%20future&journal=Dialogues%20Clin.%20Neurosci.&doi=10.31887%2FDCNS.2012.14.1%2Fdschacter&volume=14&publication_year=2012&author=Schacter%2CDL)\n    \n8.  Scoville, W. B. & Milner, B. Loss of recent memory after bilateral hippocampal lesions. _J. Neurol. Neurosurg. Psychiatry_ **20**, 11–21 (1957).\n    \n    [Article](https://doi.org/10.1136%2Fjnnp.20.1.11)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=13406589)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC497229)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Loss%20of%20recent%20memory%20after%20bilateral%20hippocampal%20lesions&journal=J.%20Neurol.%20Neurosurg.%20Psychiatry&doi=10.1136%2Fjnnp.20.1.11&volume=20&publication_year=1957&author=Scoville%2CWB&author=Milner%2CB)\n    \n9.  Squire, L. R. & Alvarez, P. Retrograde amnesia and memory consolidation: a neurobiological perspective. _Curr. Opin. Neurobiol._ **5**, 169–177 (1995).\n    \n    [Article](https://doi.org/10.1016%2F0959-4388%2895%2980023-9)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2MXlsVGit7s%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7620304)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrograde%20amnesia%20and%20memory%20consolidation%3A%20a%20neurobiological%20perspective&journal=Curr.%20Opin.%20Neurobiol.&doi=10.1016%2F0959-4388%2895%2980023-9&volume=5&pages=169-177&publication_year=1995&author=Squire%2CLR&author=Alvarez%2CP)\n    \n10.  Alvarez, P. & Squire, L. R. Memory consolidation and the medial temporal lobe: a simple network model. _Proc. Natl Acad. Sci. USA_ **91**, 7041–7045 (1994).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.91.15.7041)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2czhs1KgsQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8041742)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC44334)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation%20and%20the%20medial%20temporal%20lobe%3A%20a%20simple%20network%20model&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.91.15.7041&volume=91&pages=7041-7045&publication_year=1994&author=Alvarez%2CP&author=Squire%2CLR)\n    \n11.  Nadel, L. & Moscovitch, M. Memory consolidation, retrograde amnesia and the hippocampal complex. _Curr. Opin. Neurobiol._ **7**, 217–227 (1997).\n    \n    [Article](https://doi.org/10.1016%2FS0959-4388%2897%2980010-4)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9142752)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation%2C%20retrograde%20amnesia%20and%20the%20hippocampal%20complex&journal=Curr.%20Opin.%20Neurobiol.&doi=10.1016%2FS0959-4388%2897%2980010-4&volume=7&publication_year=1997&author=Nadel%2CL&author=Moscovitch%2CM)\n    \n12.  Wilson, M. A. & McNaughton, B. L. Reactivation of hippocampal ensemble memories during sleep. _Science_ **265**, 676–679 (1994).\n    \n    [Article](https://doi.org/10.1126%2Fscience.8036517)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2czhtVCltw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8036517)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reactivation%20of%20hippocampal%20ensemble%20memories%20during%20sleep&journal=Science&doi=10.1126%2Fscience.8036517&volume=265&pages=676-679&publication_year=1994&author=Wilson%2CMA&author=McNaughton%2CBL)\n    \n13.  Diba, K. & Buzsáki, G. Forward and reverse hippocampal place-cell sequences during ripples. _Nat. Neurosci._ **10**, 1241–1242 (2007).\n    \n    [Article](https://doi.org/10.1038%2Fnn1961)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtVOrsLjN)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17828259)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2039924)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Forward%20and%20reverse%20hippocampal%20place-cell%20sequences%20during%20ripples&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn1961&volume=10&pages=1241-1242&publication_year=2007&author=Diba%2CK&author=Buzs%C3%A1ki%2CG)\n    \n14.  Girardeau, G., Benchenane, K., Wiener, S. I., Buzsáki, G. & Zugaro, M. B. Selective suppression of hippocampal ripples impairs spatial memory. _Nat. Neurosci._ **12**, 1222–1223 (2009).\n    \n    [Article](https://doi.org/10.1038%2Fnn.2384)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1MXhtFaju7%2FF)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19749750)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Selective%20suppression%20of%20hippocampal%20ripples%20impairs%20spatial%20memory&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.2384&volume=12&pages=1222-1223&publication_year=2009&author=Girardeau%2CG&author=Benchenane%2CK&author=Wiener%2CSI&author=Buzs%C3%A1ki%2CG&author=Zugaro%2CMB)\n    \n15.  Ego-Stengel, V. & Wilson, M. A. Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat. _Hippocampus_ **20**, 1–10 (2010).\n    \n    [Article](https://doi.org/10.1002%2Fhipo.20707)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19816984)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2801761)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Disruption%20of%20ripple-associated%20hippocampal%20activity%20during%20rest%20impairs%20spatial%20learning%20in%20the%20rat&journal=Hippocampus&doi=10.1002%2Fhipo.20707&volume=20&pages=1-10&publication_year=2010&author=Ego-Stengel%2CV&author=Wilson%2CMA)\n    \n16.  Winocur, G. & Moscovitch, M. Memory transformation and systems consolidation. _J. Int. Neuropsychol. Soc._ **17**, 766–780 (2011).\n    \n    [Article](https://doi.org/10.1017%2FS1355617711000683)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21729403)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20transformation%20and%20systems%20consolidation&journal=J.%20Int.%20Neuropsychol.%20Soc.&doi=10.1017%2FS1355617711000683&volume=17&pages=766-780&publication_year=2011&author=Winocur%2CG&author=Moscovitch%2CM)\n    \n17.  Norman, Y., Raccah, O., Liu, S., Parvizi, J. & Malach, R. Hippocampal ripples and their coordinated dialogue with the default mode network during recent and remote recollection. _Neuron_ **109**, 2767–2780 (2021).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuron.2021.06.020)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhs1WkurrO)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34297916)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8693710)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20ripples%20and%20their%20coordinated%20dialogue%20with%20the%20default%20mode%20network%20during%20recent%20and%20remote%20recollection&journal=Neuron&doi=10.1016%2Fj.neuron.2021.06.020&volume=109&pages=2767-2780&publication_year=2021&author=Norman%2CY&author=Raccah%2CO&author=Liu%2CS&author=Parvizi%2CJ&author=Malach%2CR)\n    \n18.  Káli, S. & Dayan, P. Hippocampally-dependent consolidation in a hierarchical model of neocortex. _Adv. Neural Inf. Process. Syst._ **13**, 24–30 (2000).\n    \n19.  Káli, S. & Dayan, P. Replay, repair and consolidation. _Adv. Neural Inf. Process. Syst._ **15**, 19–26 (2002).\n    \n20.  Becker, S. & Burgess, N. Modelling spatial recall, mental imagery and neglect. _Adv. Neural Inf. Process. Syst._ **13**, 96–102 (2000).\n    \n21.  Byrne, P., Becker, S. & Burgess, N. Remembering the past and imagining the future: a neural model of spatial memory and imagery. _Psychol. Rev._ **114**, 340–375 (2007).\n    \n    [Article](https://doi.org/10.1037%2F0033-295X.114.2.340)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17500630)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2678675)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20and%20imagining%20the%20future%3A%20a%20neural%20model%20of%20spatial%20memory%20and%20imagery&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.114.2.340&volume=114&publication_year=2007&author=Byrne%2CP&author=Becker%2CS&author=Burgess%2CN)\n    \n22.  Bicanski, A. & Burgess, N. A neural-level model of spatial memory and imagery. _Elife_ **7**, e33752 (2018).\n    \n    [Article](https://doi.org/10.7554%2FeLife.33752)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30176988)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6122954)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20neural-level%20model%20of%20spatial%20memory%20and%20imagery&journal=Elife&doi=10.7554%2FeLife.33752&volume=7&publication_year=2018&author=Bicanski%2CA&author=Burgess%2CN)\n    \n23.  Hassabis, D., Kumaran, D., Vann, S. D. & Maguire, E. A. Patients with hippocampal amnesia cannot imagine new experiences. _Proc. Natl Acad. Sci. USA_ **104**, 1726–1731 (2007).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.0610561104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhslGisbk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17229836)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1773058)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patients%20with%20hippocampal%20amnesia%20cannot%20imagine%20new%20experiences&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0610561104&volume=104&pages=1726-1731&publication_year=2007&author=Hassabis%2CD&author=Kumaran%2CD&author=Vann%2CSD&author=Maguire%2CEA)\n    \n24.  Schacter, D. L., Benoit, R. G. & Szpunar, K. K. Episodic future thinking: mechanisms and functions. _Curr. Opin. Behav. Sci._ **17**, 41–50 (2017).\n    \n    [Article](https://doi.org/10.1016%2Fj.cobeha.2017.06.002)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29130061)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5675579)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20future%20thinking%3A%20mechanisms%20and%20functions&journal=Curr.%20Opin.%20Behav.%20Sci.&doi=10.1016%2Fj.cobeha.2017.06.002&volume=17&pages=41-50&publication_year=2017&author=Schacter%2CDL&author=Benoit%2CRG&author=Szpunar%2CKK)\n    \n25.  Spanó, G. et al. Dreaming with hippocampal damage. _Elife_ **9**, e56211 (2020).\n    \n    [Article](https://doi.org/10.7554%2FeLife.56211)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32508305)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7279885)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Dreaming%20with%20hippocampal%20damage&journal=Elife&doi=10.7554%2FeLife.56211&volume=9&publication_year=2020&author=Span%C3%B3%2CG)\n    \n26.  McCormick, C., Rosenthal, C. R., Miller, T. D. & Maguire, E. A. Mind-wandering in people with hippocampal damage. _J. Neurosci._ **38**, 2745–2754 (2018).\n    \n    [Article](https://doi.org/10.1523%2FJNEUROSCI.1812-17.2018)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXit1Gjtr%2FL)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29440532)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5851780)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mind-wandering%20in%20people%20with%20hippocampal%20damage&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.1812-17.2018&volume=38&pages=2745-2754&publication_year=2018&author=McCormick%2CC&author=Rosenthal%2CCR&author=Miller%2CTD&author=Maguire%2CEA)\n    \n27.  Addis, D. R., Wong, A. T. & Schacter, D. L. Remembering the past and imagining the future: common and distinct neural substrates during event construction and elaboration. _Neuropsychologia_ **45**, 1363–1377 (2007).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2006.10.016)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17126370)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20and%20imagining%20the%20future%3A%20common%20and%20distinct%20neural%20substrates%20during%20event%20construction%20and%20elaboration&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2006.10.016&volume=45&pages=1363-1377&publication_year=2007&author=Addis%2CDR&author=Wong%2CAT&author=Schacter%2CDL)\n    \n28.  Hassabis, D. & Maguire, E. A. Deconstructing episodic memory with construction. _Trends Cogn. Sci._ **11**, 299–306 (2007).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2007.05.001)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17548229)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Deconstructing%20episodic%20memory%20with%20construction&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2007.05.001&volume=11&pages=299-306&publication_year=2007&author=Hassabis%2CD&author=Maguire%2CEA)\n    \n29.  Hinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at [https://arxiv.org/abs/1503.02531](https://arxiv.org/abs/1503.02531) (2015).\n    \n30.  Sun, W., Advani, M., Spruston, N., Saxe, A. & Fitzgerald, J. E. Organizing memories for generalization in complementary learning systems. _Nat. Neurosci._ **26**, 1438–1448 (2023).\n    \n31.  Whittington, J. C. R. et al. The Tolman–Eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. _Cell_ **183**, 1249–1263 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.cell.2020.10.024)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXitlClsLnE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33181068)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7707106)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20Tolman%E2%80%93Eichenbaum%20machine%3A%20unifying%20space%20and%20relational%20memory%20through%20generalization%20in%20the%20hippocampal%20formation&journal=Cell&doi=10.1016%2Fj.cell.2020.10.024&volume=183&pages=1249-1263&publication_year=2020&author=Whittington%2CJCR)\n    \n32.  Nagy, D. G., Török, B. & Orbán, G. Optimal forgetting: semantic compression of episodic memories. _PLoS Comput. Biol._ **16**, e1008367 (2020).\n    \n    [Article](https://doi.org/10.1371%2Fjournal.pcbi.1008367)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXit1amur3I)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33057380)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7591090)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Optimal%20forgetting%3A%20semantic%20compression%20of%20episodic%20memories&journal=PLoS%20Comput.%20Biol.&doi=10.1371%2Fjournal.pcbi.1008367&volume=16&publication_year=2020&author=Nagy%2CDG&author=T%C3%B6r%C3%B6k%2CB&author=Orb%C3%A1n%2CG)\n    \n33.  van de Ven, G. M., Siegelmann, H. T. & Tolias, A. S. Brain-inspired replay for continual learning with artificial neural networks. _Nat. Commun._ **11**, 4069 (2020).\n    \n    [Article](https://doi.org/10.1038%2Fs41467-020-17866-2)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32792531)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426273)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Brain-inspired%20replay%20for%20continual%20learning%20with%20artificial%20neural%20networks&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-020-17866-2&volume=11&publication_year=2020&author=Ven%2CGM&author=Siegelmann%2CHT&author=Tolias%2CAS)\n    \n34.  Hemmer, P. & Steyvers, M. A Bayesian account of reconstructive memory. _Top. Cogn. Sci._ **1**, 189–202 (2009).\n    \n    [Article](https://doi.org/10.1111%2Fj.1756-8765.2008.01010.x)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25164805)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20Bayesian%20account%20of%20reconstructive%20memory&journal=Top.%20Cogn.%20Sci.&doi=10.1111%2Fj.1756-8765.2008.01010.x&volume=1&pages=189-202&publication_year=2009&author=Hemmer%2CP&author=Steyvers%2CM)\n    \n35.  Fayyaz, Z. et al. A model of semantic completion in generative episodic memory. _Neural Comput._ **34**, 1841–1870 (2022).\n    \n    [Article](https://doi.org/10.1162%2Fneco_a_01520)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=35896150)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20model%20of%20semantic%20completion%20in%20generative%20episodic%20memory&journal=Neural%20Comput.&doi=10.1162%2Fneco_a_01520&volume=34&publication_year=2022&author=Fayyaz%2CZ)\n    \n36.  Schacter, D. L., Addis, D. R. & Buckner, R. L. Remembering the past to imagine the future: the prospective brain. _Nat. Rev. Neurosci._ **8**, 657–661 (2007).\n    \n    [Article](https://doi.org/10.1038%2Fnrn2213)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17700624)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Remembering%20the%20past%20to%20imagine%20the%20future%3A%20the%20prospective%20brain&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn2213&volume=8&publication_year=2007&author=Schacter%2CDL&author=Addis%2CDR&author=Buckner%2CRL)\n    \n37.  Biderman, N., Bakkour, A. & Shohamy, D. What are memories for? The hippocampus bridges past experience with future decisions. _Trends Cogn. Sci._ **24**, 542–556 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2020.04.004)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32513572)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20are%20memories%20for%3F%20The%20hippocampus%20bridges%20past%20experience%20with%20future%20decisions&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2020.04.004&volume=24&pages=542-556&publication_year=2020&author=Biderman%2CN&author=Bakkour%2CA&author=Shohamy%2CD)\n    \n38.  Bein, O., Plotkin, N. A. & Davachi, L. Mnemonic prediction errors promote detailed memories. _Learn. Mem._ **28**, 422–434 (2021).\n    \n    [Article](https://doi.org/10.1101%2Flm.053410.121)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34663695)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8525423)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Mnemonic%20prediction%20errors%20promote%20detailed%20memories&journal=Learn.%20Mem.&doi=10.1101%2Flm.053410.121&volume=28&pages=422-434&publication_year=2021&author=Bein%2CO&author=Plotkin%2CNA&author=Davachi%2CL)\n    \n39.  Sherman, B. E. et al. Temporal dynamics of competition between statistical learning and episodic memory in intracranial recordings of human visual cortex. _J. Neurosci._ **42**, 9053–9068 (2022).\n    \n    [Article](https://doi.org/10.1523%2FJNEUROSCI.0708-22.2022)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB38XjtFKqtL7K)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=36344264)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9732826)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Temporal%20dynamics%20of%20competition%20between%20statistical%20learning%20and%20episodic%20memory%20in%20intracranial%20recordings%20of%20human%20visual%20cortex&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.0708-22.2022&volume=42&pages=9053-9068&publication_year=2022&author=Sherman%2CBE)\n    \n40.  Barlow, H. B. et al. in _Sensory Communication_ (ed. Rosenblith, W. A.) 217–233 (MIT Press, 2013).\n    \n41.  Barlow, H. B. Unsupervised learning. _Neural Comput._ **1**, 295–311 (1989).\n    \n    [Article](https://doi.org/10.1162%2Fneco.1989.1.3.295)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Unsupervised%20learning&journal=Neural%20Comput.&doi=10.1162%2Fneco.1989.1.3.295&volume=1&pages=295-311&publication_year=1989&author=Barlow%2CHB)\n    \n42.  Benna, M. K. & Fusi, S. Place cells may simply be memory cells: memory compression leads to spatial tuning and history dependence. _Proc. Natl Acad. Sci. USA_ **118**, e2018422118 (2021).\n    \n43.  Vargha-Khadem, F. et al. Differential effects of early hippocampal pathology on episodic and semantic memory. _Science_ **277**, 376–380 (1997).\n    \n    [Article](https://doi.org/10.1126%2Fscience.277.5324.376)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2sXkvVels7Y%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9219696)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Differential%20effects%20of%20early%20hippocampal%20pathology%20on%20episodic%20and%20semantic%20memory&journal=Science&doi=10.1126%2Fscience.277.5324.376&volume=277&pages=376-380&publication_year=1997&author=Vargha-Khadem%2CF)\n    \n44.  Manns, J. R., Hopkins, R. O. & Squire, L. R. Semantic memory and the human hippocampus. _Neuron_ **38**, 127–133 (2003).\n    \n    [Article](https://doi.org/10.1016%2FS0896-6273%2803%2900146-6)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD3sXjt1Gmtr0%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=12691670)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Semantic%20memory%20and%20the%20human%20hippocampus&journal=Neuron&doi=10.1016%2FS0896-6273%2803%2900146-6&volume=38&pages=127-133&publication_year=2003&author=Manns%2CJR&author=Hopkins%2CRO&author=Squire%2CLR)\n    \n45.  Squire, L. R., Genzel, L., Wixted, J. T. & Morris, R. G. Memory consolidation. _Cold Spring Harb. Perspect. Biol._ **7**, a021766 (2015).\n    \n    [Article](https://doi.org/10.1101%2Fcshperspect.a021766)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26238360)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4526749)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20consolidation&journal=Cold%20Spring%20Harb.%20Perspect.%20Biol.&doi=10.1101%2Fcshperspect.a021766&volume=7&publication_year=2015&author=Squire%2CLR&author=Genzel%2CL&author=Wixted%2CJT&author=Morris%2CRG)\n    \n46.  McKenzie, S. & Eichenbaum, H. Consolidation and reconsolidation: two lives of memories? _Neuron_ **71**, 224–233 (2011).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuron.2011.06.037)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3MXps1OgsLs%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21791282)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3145971)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Consolidation%20and%20reconsolidation%3A%20two%20lives%20of%20memories%3F&journal=Neuron&doi=10.1016%2Fj.neuron.2011.06.037&volume=71&pages=224-233&publication_year=2011&author=McKenzie%2CS&author=Eichenbaum%2CH)\n    \n47.  Durrant, S. J., Taylor, C., Cairney, S. & Lewis, P. A. Sleep-dependent consolidation of statistical learning. _Neuropsychologia_ **49**, 1322–1331 (2011).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2011.02.015)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21335017)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Sleep-dependent%20consolidation%20of%20statistical%20learning&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2011.02.015&volume=49&pages=1322-1331&publication_year=2011&author=Durrant%2CSJ&author=Taylor%2CC&author=Cairney%2CS&author=Lewis%2CPA)\n    \n48.  Richards, B. A. et al. Patterns across multiple memories are identified over time. _Nat. Neurosci._ **17**, 981–986 (2014).\n    \n    [Article](https://doi.org/10.1038%2Fnn.3736)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXovFCgtL8%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=24880213)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patterns%20across%20multiple%20memories%20are%20identified%20over%20time&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.3736&volume=17&pages=981-986&publication_year=2014&author=Richards%2CBA)\n    \n49.  Ellenbogen, J. M., Hu, P. T., Payne, J. D., Titone, D. & Walker, M. P. Human relational memory requires time and sleep. _Proc. Natl Acad. Sci. USA_ **104**, 7723–7728 (2007).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.0700094104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXlslahtrc%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17449637)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1863467)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20relational%20memory%20requires%20time%20and%20sleep&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0700094104&volume=104&pages=7723-7728&publication_year=2007&author=Ellenbogen%2CJM&author=Hu%2CPT&author=Payne%2CJD&author=Titone%2CD&author=Walker%2CMP)\n    \n50.  Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? Complementary learning systems theory updated. _Trends Cogn. Sci._ **20**, 512–534 (2016).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2016.05.004)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27315762)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20learning%20systems%20do%20intelligent%20agents%20need%3F%20Complementary%20learning%20systems%20theory%20updated&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2016.05.004&volume=20&pages=512-534&publication_year=2016&author=Kumaran%2CD&author=Hassabis%2CD&author=McClelland%2CJL)\n    \n51.  Schapiro, A. C., Turk-Browne, N. B., Botvinick, M. M. & Norman, K. A. Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning. _Phil. Trans. R. Soc. B_ **372**, 20160049 (2017).\n    \n    [Article](https://doi.org/10.1098%2Frstb.2016.0049)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27872368)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5124075)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Complementary%20learning%20systems%20within%20the%20hippocampus%3A%20a%20neural%20network%20modelling%20approach%20to%20reconciling%20episodic%20memory%20with%20statistical%20learning&journal=Phil.%20Trans.%20R.%20Soc.%20B&doi=10.1098%2Frstb.2016.0049&volume=372&publication_year=2017&author=Schapiro%2CAC&author=Turk-Browne%2CNB&author=Botvinick%2CMM&author=Norman%2CKA)\n    \n52.  Payne, J. D. et al. The role of sleep in false memory formation. _Neurobiol. Learn. Mem._ **92**, 327–334 (2009).\n    \n    [Article](https://doi.org/10.1016%2Fj.nlm.2009.03.007)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=19348959)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2789473)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20role%20of%20sleep%20in%20false%20memory%20formation&journal=Neurobiol.%20Learn.%20Mem.&doi=10.1016%2Fj.nlm.2009.03.007&volume=92&pages=327-334&publication_year=2009&author=Payne%2CJD)\n    \n53.  Hafting, T., Fyhn, M., Molden, S., Moser, M. B. & Moser, E. I. Microstructure of a spatial map in the entorhinal cortex. _Nature_ **436**, 801–806 (2005).\n    \n    [Article](https://doi.org/10.1038%2Fnature03721)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXnt1Siurk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=15965463)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Microstructure%20of%20a%20spatial%20map%20in%20the%20entorhinal%20cortex&journal=Nature&doi=10.1038%2Fnature03721&volume=436&pages=801-806&publication_year=2005&author=Hafting%2CT&author=Fyhn%2CM&author=Molden%2CS&author=Moser%2CMB&author=Moser%2CEI)\n    \n54.  Constantinescu, A. O., O’Reilly, J. X. & Behrens, T. E. Organizing conceptual knowledge in humans with a gridlike code. _Science_ **352**, 1464–1468 (2016).\n    \n    [Article](https://doi.org/10.1126%2Fscience.aaf0941)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XpslOqsLw%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27313047)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5248972)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Organizing%20conceptual%20knowledge%20in%20humans%20with%20a%20gridlike%20code&journal=Science&doi=10.1126%2Fscience.aaf0941&volume=352&pages=1464-1468&publication_year=2016&author=Constantinescu%2CAO&author=O%E2%80%99Reilly%2CJX&author=Behrens%2CTE)\n    \n55.  Mack, M. L., Preston, A. R. & Love, B. C. Ventromedial prefrontal cortex compression during concept learning. _Nat. Commun._ **11**, 46 (2020).\n    \n    [Article](https://doi.org/10.1038%2Fs41467-019-13930-8)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXmt1aisg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=31911628)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6946809)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%20compression%20during%20concept%20learning&journal=Nat.%20Commun.&doi=10.1038%2Fs41467-019-13930-8&volume=11&publication_year=2020&author=Mack%2CML&author=Preston%2CAR&author=Love%2CBC)\n    \n56.  Hasselmo, M. E., Wyble, B. P. & Wallenstein, G. V. Encoding and retrieval of episodic memories: role of cholinergic and GABAergic modulation in the hippocampus. _Hippocampus_ **6**, 693–708 (1996).\n    \n    [Article](https://doi.org/10.1002%2F%28SICI%291098-1063%281996%296%3A6%3C693%3A%3AAID-HIPO12%3E3.0.CO%3B2-W)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK2sXhsFCjt7g%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9034856)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Encoding%20and%20retrieval%20of%20episodic%20memories%3A%20role%20of%20cholinergic%20and%20GABAergic%20modulation%20in%20the%20hippocampus&journal=Hippocampus&doi=10.1002%2F%28SICI%291098-1063%281996%296%3A6%3C693%3A%3AAID-HIPO12%3E3.0.CO%3B2-W&volume=6&pages=693-708&publication_year=1996&author=Hasselmo%2CME&author=Wyble%2CBP&author=Wallenstein%2CGV)\n    \n57.  Tse, D. et al. Schemas and memory consolidation. _Science_ **316**, 76–82 (2007).\n    \n    [Article](https://doi.org/10.1126%2Fscience.1135935)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXjvVarsbk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17412951)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Schemas%20and%20memory%20consolidation&journal=Science&doi=10.1126%2Fscience.1135935&volume=316&pages=76-82&publication_year=2007&author=Tse%2CD)\n    \n58.  Kumaran, D. & Maguire, E. A. An unexpected sequence of events: mismatch detection in the human hippocampus. _PLoS Biol._ **4**, e424 (2006).\n    \n    [Article](https://doi.org/10.1371%2Fjournal.pbio.0040424)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17132050)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1661685)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=An%20unexpected%20sequence%20of%20events%3A%20mismatch%20detection%20in%20the%20human%20hippocampus&journal=PLoS%20Biol.&doi=10.1371%2Fjournal.pbio.0040424&volume=4&publication_year=2006&author=Kumaran%2CD&author=Maguire%2CEA)\n    \n59.  Chen, J., Olsen, R. K., Preston, A. R., Glover, G. H. & Wagner, A. D. Associative retrieval processes in the human medial temporal lobe: hippocampal retrieval success and CA1 mismatch detection. _Learn. Mem._ **18**, 523–528 (2011).\n    \n    [Article](https://doi.org/10.1101%2Flm.2135211)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=21775513)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3256570)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Associative%20retrieval%20processes%20in%20the%20human%20medial%20temporal%20lobe%3A%20hippocampal%20retrieval%20success%20and%20CA1%20mismatch%20detection&journal=Learn.%20Mem.&doi=10.1101%2Flm.2135211&volume=18&pages=523-528&publication_year=2011&author=Chen%2CJ&author=Olsen%2CRK&author=Preston%2CAR&author=Glover%2CGH&author=Wagner%2CAD)\n    \n60.  Hedayati, S., O’Donnell, R. E. & Wyble, B. A model of working memory for latent representations. _Nat. Hum. Behav._ **6**, 709–719 (2022).\n    \n    [Article](https://doi.org/10.1038%2Fs41562-021-01264-9)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=35115675)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20model%20of%20working%20memory%20for%20latent%20representations&journal=Nat.%20Hum.%20Behav.&doi=10.1038%2Fs41562-021-01264-9&volume=6&publication_year=2022&author=Hedayati%2CS&author=O%E2%80%99Donnell%2CRE&author=Wyble%2CB)\n    \n61.  Quiroga, R. Q. Concept cells: the building blocks of declarative memory functions. _Nat. Rev. Neurosci._ **13**, 587–597 (2012).\n    \n    [Article](https://doi.org/10.1038%2Fnrn3251)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XpsFOrtbw%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22760181)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Concept%20cells%3A%20the%20building%20blocks%20of%20declarative%20memory%20functions&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3251&volume=13&pages=587-597&publication_year=2012&author=Quiroga%2CRQ)\n    \n62.  Kolibius, L. D. et al. Hippocampal neurons code individual episodic memories in humans. _Nat. Hum. Behav_. **7**, 1968–1979 (2023).\n    \n63.  Ramsauer, H. et al. Hopfield networks is all you need. in _International Conference on Learning Representations_ (2021).\n    \n64.  Krotov, D. & Hopfield, J. Large associative memory problem in neurobiology and machine learning. in _International Conference on Learning Representations_ (2021).\n    \n65.  Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114) (2013).\n    \n66.  Kingma, D. P. & Welling, M. An introduction to variational autoencoders. _Found. Trends Mach. Learn._ **12**, 307–392 (2019).\n    \n67.  Dayan, P., Hinton, G. E., Neal, R. M. & Zemel, R. S. The Helmholtz machine. _Neural Comput._ **7**, 889–904 (1995).\n    \n    [Article](https://doi.org/10.1162%2Fneco.1995.7.5.889)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK28%2FitVKrtA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=7584891)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20Helmholtz%20machine&journal=Neural%20Comput.&doi=10.1162%2Fneco.1995.7.5.889&volume=7&pages=889-904&publication_year=1995&author=Dayan%2CP&author=Hinton%2CGE&author=Neal%2CRM&author=Zemel%2CRS)\n    \n68.  Rao, R. P. N. & Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. _Nat. Neurosci._ **2**, 79–87 (1999).\n    \n    [Article](https://doi.org/10.1038%2F4580)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=10195184)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Predictive%20coding%20in%20the%20visual%20cortex%3A%20a%20functional%20interpretation%20of%20some%20extra-classical%20receptive-field%20effects&journal=Nat.%20Neurosci.&doi=10.1038%2F4580&volume=2&publication_year=1999&author=Rao%2CRPN&author=Ballard%2CDH)\n    \n69.  Friston, K. The free-energy principle: a unified brain theory? _Nat. Rev. Neurosci._ **11**, 127–138 (2010).\n    \n    [Article](https://doi.org/10.1038%2Fnrn2787)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20068583)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20free-energy%20principle%3A%20a%20unified%20brain%20theory%3F&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn2787&volume=11&publication_year=2010&author=Friston%2CK)\n    \n70.  Gilboa, A. & Marlatte, H. Neurobiology of schemas and schema-mediated memory. _Trends Cogn. Sci._ **21**, 618–631 (2017).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2017.04.013)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=28551107)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Neurobiology%20of%20schemas%20and%20schema-mediated%20memory&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2017.04.013&volume=21&pages=618-631&publication_year=2017&author=Gilboa%2CA&author=Marlatte%2CH)\n    \n71.  Ghosh, V. E. & Gilboa, A. What is a memory schema? A historical perspective on current neuroscience literature. _Neuropsychologia_ **53**, 104–114 (2014).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuropsychologia.2013.11.010)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=24280650)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20is%20a%20memory%20schema%3F%20A%20historical%20perspective%20on%20current%20neuroscience%20literature&journal=Neuropsychologia&doi=10.1016%2Fj.neuropsychologia.2013.11.010&volume=53&pages=104-114&publication_year=2014&author=Ghosh%2CVE&author=Gilboa%2CA)\n    \n72.  Chambers, D. & Reisberg, D. Can mental images be ambiguous? _J. Exp. Psychol. Hum. Percept. Perform._ **11**, 317–328 (1985).\n    \n    [Article](https://doi.org/10.1037%2F0096-1523.11.3.317)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Can%20mental%20images%20be%20ambiguous%3F&journal=J.%20Exp.%20Psychol.%20Hum.%20Percept.%20Perform.&doi=10.1037%2F0096-1523.11.3.317&volume=11&publication_year=1985&author=Chambers%2CD&author=Reisberg%2CD)\n    \n73.  Moser, E. I., Kropff, E. & Moser, M. B. Place cells, grid cells, and the brain’s spatial representation system. _Annu. Rev. of Neurosci._ **31**, 69–89 (2008).\n    \n    [Article](https://doi.org/10.1146%2Fannurev.neuro.31.061307.090723)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD1cXpt12nsr8%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Place%20cells%2C%20grid%20cells%2C%20and%20the%20brain%E2%80%99s%20spatial%20representation%20system&journal=Annu.%20Rev.%20of%20Neurosci.&doi=10.1146%2Fannurev.neuro.31.061307.090723&volume=31&pages=69-89&publication_year=2008&author=Moser%2CEI&author=Kropff%2CE&author=Moser%2CMB)\n    \n74.  Takashima, A. et al. Declarative memory consolidation in humans: a prospective functional magnetic resonance imaging study. _Proc. Natl Acad. Sci. USA_ **103**, 756–761 (2006).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.0507774103)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD28XhtVOhs74%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=16407110)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1334654)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Declarative%20memory%20consolidation%20in%20humans%3A%20a%20prospective%20functional%20magnetic%20resonance%20imaging%20study&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0507774103&volume=103&pages=756-761&publication_year=2006&author=Takashima%2CA)\n    \n75.  Gais, S. et al. Sleep transforms the cerebral trace of declarative memories. _Proc. Natl Acad. Sci. USA_ **104**, 18778–18783 (2007).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.0705454104)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2sXhtl2ltbfE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=18000060)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2141853)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Sleep%20transforms%20the%20cerebral%20trace%20of%20declarative%20memories&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0705454104&volume=104&pages=18778-18783&publication_year=2007&author=Gais%2CS)\n    \n76.  Frankland, P. W. & Bontempi, B. The organization of recent and remote memories. _Nat. Rev. Neurosci._ **6**, 119–130 (2005).\n    \n    [Article](https://doi.org/10.1038%2Fnrn1607)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BD2MXoslOjsg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=15685217)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20organization%20of%20recent%20and%20remote%20memories&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn1607&volume=6&pages=119-130&publication_year=2005&author=Frankland%2CPW&author=Bontempi%2CB)\n    \n77.  van Kesteren, M. T. R., Fernández, G., Norris, D. G. & Hermans, E. J. Persistent schema-dependent hippocampal-neocortical connectivity during memory encoding and postencoding rest in humans. _Proc. Natl Acad. Sci. USA_ **107**, 7550–7555 (2010).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.0914892107)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20363957)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2867741)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Persistent%20schema-dependent%20hippocampal-neocortical%20connectivity%20during%20memory%20encoding%20and%20postencoding%20rest%20in%20humans&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.0914892107&volume=107&pages=7550-7555&publication_year=2010&author=Kesteren%2CMTR&author=Fern%C3%A1ndez%2CG&author=Norris%2CDG&author=Hermans%2CEJ)\n    \n78.  Benchenane, K. et al. Coherent theta oscillations and reorganization of spike timing in the hippocampal-prefrontal network upon learning. _Neuron_ **66**, 921–936 (2010).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuron.2010.05.013)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3cXovFeiur0%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=20620877)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Coherent%20theta%20oscillations%20and%20reorganization%20of%20spike%20timing%20in%20the%20hippocampal-prefrontal%20network%20upon%20learning&journal=Neuron&doi=10.1016%2Fj.neuron.2010.05.013&volume=66&pages=921-936&publication_year=2010&author=Benchenane%2CK)\n    \n79.  Koscik, T. R. & Tranel, D. The human ventromedial prefrontal cortex is critical for transitive inference. _J. Cogn. Neurosci._ **24**, 1191–1204 (2012).\n    \n    [Article](https://doi.org/10.1162%2Fjocn_a_00203)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22288395)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3626083)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20human%20ventromedial%20prefrontal%20cortex%20is%20critical%20for%20transitive%20inference&journal=J.%20Cogn.%20Neurosci.&doi=10.1162%2Fjocn_a_00203&volume=24&pages=1191-1204&publication_year=2012&author=Koscik%2CTR&author=Tranel%2CD)\n    \n80.  Spalding, K. N. et al. Ventromedial prefrontal cortex is necessary for normal associative inference and memory integration. _J. Neurosci._ **38**, 3767–3775 (2018).\n    \n    [Article](https://doi.org/10.1523%2FJNEUROSCI.2501-17.2018)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=29555854)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5895999)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%20is%20necessary%20for%20normal%20associative%20inference%20and%20memory%20integration&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.2501-17.2018&volume=38&publication_year=2018&author=Spalding%2CKN)\n    \n81.  Chan, D. et al. Patterns of temporal lobe atrophy in semantic dementia and Alzheimer’s disease. _Ann. Neurol._ **49**, 433–442 (2001).\n    \n    [Article](https://doi.org/10.1002%2Fana.92)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3hvVyltA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11310620)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Patterns%20of%20temporal%20lobe%20atrophy%20in%20semantic%20dementia%20and%20Alzheimer%E2%80%99s%20disease&journal=Ann.%20Neurol.&doi=10.1002%2Fana.92&volume=49&pages=433-442&publication_year=2001&author=Chan%2CD)\n    \n82.  Bright, P. et al. Retrograde amnesia in patients with hippocampal, medial temporal, temporal lobe, or frontal pathology. _Learn. Mem._ **13**, 545–557 (2006).\n    \n    [Article](https://doi.org/10.1101%2Flm.265906)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=17015852)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1783611)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Retrograde%20amnesia%20in%20patients%20with%20hippocampal%2C%20medial%20temporal%2C%20temporal%20lobe%2C%20or%20frontal%20pathology&journal=Learn.%20Mem.&doi=10.1101%2Flm.265906&volume=13&pages=545-557&publication_year=2006&author=Bright%2CP)\n    \n83.  Ranganath, C. & Ritchey, M. Two cortical systems for memory-guided behaviour. _Nat. Rev. Neurosci._ **13**, 713–726 (2012).\n    \n    [Article](https://doi.org/10.1038%2Fnrn3338)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22992647)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Two%20cortical%20systems%20for%20memory-guided%20behaviour&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3338&volume=13&publication_year=2012&author=Ranganath%2CC&author=Ritchey%2CM)\n    \n84.  Spiers, H. J., Maguire, E. A. & Burgess, N. Hippocampal amnesia. _Neurocase_ **7**, 357–382 (2001).\n    \n    [Article](https://doi.org/10.1076%2Fneur.7.5.357.16245)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11744778)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20amnesia&journal=Neurocase&doi=10.1076%2Fneur.7.5.357.16245&volume=7&publication_year=2001&author=Spiers%2CHJ&author=Maguire%2CEA&author=Burgess%2CN)\n    \n85.  Vinyals, O., Toshev, A., Bengio, S. & Erhan, D. Show and tell: a neural image caption generator. in _Proc. IEEE Conference on Computer Vision and Pattern Recognition_ 3156–3164 (2015).\n    \n86.  Mokady, R., Hertz, A. H. & Bermano, A. H. ClipCap: CLIP prefix for image captioning. Preprint at [https://arxiv.org/abs/2111.09734](https://arxiv.org/abs/2111.09734) (2021).\n    \n87.  LeCun, Y., Cortes, C. & Burges, C. J. _MNIST Handwritten Digit Database_ (AT&T Labs, 2010).\n    \n88.  Intraub, H. & Richardson, M. Wide-angle memories of close-up scenes. _J. Exp. Psychol. Learn. Mem. Cogn._ **15**, 179–187 (1989).\n    \n    [Article](https://doi.org/10.1037%2F0278-7393.15.2.179)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=2522508)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Wide-angle%20memories%20of%20close-up%20scenes&journal=J.%20Exp.%20Psychol.%20Learn.%20Mem.%20Cogn.&doi=10.1037%2F0278-7393.15.2.179&volume=15&publication_year=1989&author=Intraub%2CH&author=Richardson%2CM)\n    \n89.  Bainbridge, W. A. & Baker, C. I. Boundaries extend and contract in scene memory depending on image properties. _Curr. Biol._ **30**, 537–543 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.cub.2019.12.004)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhvVShsrk%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=31983637)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7187786)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Boundaries%20extend%20and%20contract%20in%20scene%20memory%20depending%20on%20image%20properties&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2019.12.004&volume=30&pages=537-543&publication_year=2020&author=Bainbridge%2CWA&author=Baker%2CCI)\n    \n90.  Intraub, H. Searching for boundary extension. _Curr. Biol._ **30**, R1463–R1464 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.cub.2020.10.031)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXktlyktQ%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33352122)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Searching%20for%20boundary%20extension&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2020.10.031&volume=30&pages=R1463-R1464&publication_year=2020&author=Intraub%2CH)\n    \n91.  Bainbridge, W. A. & Baker, C. I. Reply to Intraub. _Curr. Biol._ **30**, R1465–R1466 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.cub.2020.10.032)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXktlCiuw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33352123)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reply%20to%20Intraub&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2020.10.032&volume=30&pages=R1465-R1466&publication_year=2020&author=Bainbridge%2CWA&author=Baker%2CCI)\n    \n92.  Park, J., Josephs, E. L. & Konkle, T. Systematic transition from boundary extension to contraction along an object-to-scene continuum. _J. Vis._ [https://doi.org/10.1167/jov.21.9.2124](https://doi.org/10.1167/jov.21.9.2124) (2021).\n    \n93.  Deese, J. On the prediction of occurrence of particular verbal intrusions in immediate recall. _J. Exp. Psychol._ **58**, 17–22 (1959).\n    \n    [Article](https://doi.org/10.1037%2Fh0046671)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=13664879)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=On%20the%20prediction%20of%20occurrence%20of%20particular%20verbal%20intrusions%20in%20immediate%20recall&journal=J.%20Exp.%20Psychol.&doi=10.1037%2Fh0046671&volume=58&publication_year=1959&author=Deese%2CJ)\n    \n94.  Roediger, H. L. & McDermott, K. B. Creating false memories: remembering words not presented in lists. _J. Exp. Psychol. Lear. Mem. Cogn._ **21**, 803–814 (1995).\n    \n    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Creating%20false%20memories%3A%20remembering%20words%20not%20presented%20in%20lists&journal=J.%20Exp.%20Psychol.%20Lear.%20Mem.%20Cogn.&volume=21&publication_year=1995&author=Roediger%2CHL&author=McDermott%2CKB)\n    \n95.  Carmichael, L., Hogan, H. P. & Walter, A. A. An experimental study of the effect of language on the reproduction of visually perceived form. _J. Exp. Psychol._ **15**, 73–86 (1932).\n    \n    [Article](https://doi.org/10.1037%2Fh0072671)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=An%20experimental%20study%20of%20the%20effect%20of%20language%20on%20the%20reproduction%20of%20visually%20perceived%20form&journal=J.%20Exp.%20Psychol.&doi=10.1037%2Fh0072671&volume=15&publication_year=1932&author=Carmichael%2CL&author=Hogan%2CHP&author=Walter%2CAA)\n    \n96.  Mostafazadeh, N. et al. A corpus and cloze evaluation for deeper understanding of commonsense stories. in _Proc. 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_ (eds Knight, K. et al.) 839–849 (2016).\n    \n97.  Robinson, K. J. & Roediger, H. L. Associative processes in false recall and false recognition. _Psychol. Sci._ **8**, 231–237 (1997).\n    \n    [Article](https://doi.org/10.1111%2Fj.1467-9280.1997.tb00417.x)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Associative%20processes%20in%20false%20recall%20and%20false%20recognition&journal=Psychol.%20Sci.&doi=10.1111%2Fj.1467-9280.1997.tb00417.x&volume=8&pages=231-237&publication_year=1997&author=Robinson%2CKJ&author=Roediger%2CHL)\n    \n98.  Cipolotti, L. et al. Long-term retrograde amnesia… the crucial role of the hippocampus. _Neuropsychologia_ **39**, 151–172 (2001).\n    \n    [Article](https://doi.org/10.1016%2FS0028-3932%2800%2900103-2)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3htFGitw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=11163373)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Long-term%20retrograde%20amnesia%E2%80%A6%20the%20crucial%20role%20of%20the%20hippocampus&journal=Neuropsychologia&doi=10.1016%2FS0028-3932%2800%2900103-2&volume=39&pages=151-172&publication_year=2001&author=Cipolotti%2CL)\n    \n99.  Zola-Morgan, S., Squire, L. R. & Amaral, D. G. Human amnesia and the medial temporal region: enduring memory impairment following a bilateral lesion limited to field CA1 of the hippocampus. _J. Neurosci._ **6**, 2950–2967 (1986).\n    \n    [Article](https://doi.org/10.1523%2FJNEUROSCI.06-10-02950.1986)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL2s%2FhtVyjtg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=3760943)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6568782)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20amnesia%20and%20the%20medial%20temporal%20region%3A%20enduring%20memory%20impairment%20following%20a%20bilateral%20lesion%20limited%20to%20field%20CA1%20of%20the%20hippocampus&journal=J.%20Neurosci.&doi=10.1523%2FJNEUROSCI.06-10-02950.1986&volume=6&pages=2950-2967&publication_year=1986&author=Zola-Morgan%2CS&author=Squire%2CLR&author=Amaral%2CDG)\n    \n100.  Knowlton, B. J., Squire, L. R. & Gluck, M. A. Probabilistic classification learning in amnesia. _Learn. Mem._ **1**, 106–120 (1994).\n    \n    [Article](https://doi.org/10.1101%2Flm.1.2.106)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK1Mzps12hsg%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=10467589)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Probabilistic%20classification%20learning%20in%20amnesia&journal=Learn.%20Mem.&doi=10.1101%2Flm.1.2.106&volume=1&pages=106-120&publication_year=1994&author=Knowlton%2CBJ&author=Squire%2CLR&author=Gluck%2CMA)\n    \n101.  Hodges, J. R. & Graham, K. S. Episodic memory: insights from semantic dementia. _Phil. Trans. R. Soc. Lond. B_ **356**, 1423–1434 (2001).\n    \n    [Article](https://doi.org/10.1098%2Frstb.2001.0943)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3MrisVeksQ%3D%3D)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20memory%3A%20insights%20from%20semantic%20dementia&journal=Phil.%20Trans.%20R.%20Soc.%20Lond.%20B&doi=10.1098%2Frstb.2001.0943&volume=356&pages=1423-1434&publication_year=2001&author=Hodges%2CJR&author=Graham%2CKS)\n    \n102.  Migo, E., Montaldi, D., Norman, K. A., Quamme, J. & Mayes, A. The contribution of familiarity to recognition memory is a function of test format when using similar foils. _Q. J.Exp. Psychol._ **62**, 1198–1215 (2009).\n    \n    [Article](https://doi.org/10.1080%2F17470210802391599)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20contribution%20of%20familiarity%20to%20recognition%20memory%20is%20a%20function%20of%20test%20format%20when%20using%20similar%20foils&journal=Q.%20J.Exp.%20Psychol.&doi=10.1080%2F17470210802391599&volume=62&publication_year=2009&author=Migo%2CE&author=Montaldi%2CD&author=Norman%2CKA&author=Quamme%2CJ&author=Mayes%2CA)\n    \n103.  Moscovitch, M. & Melo, B. Strategic retrieval and the frontal lobes: evidence from confabulation and amnesia. _Neuropsychologia_ **35**, 1017–1034 (1997).\n    \n    [Article](https://doi.org/10.1016%2FS0028-3932%2897%2900028-6)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2szntVaquw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=9226662)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Strategic%20retrieval%20and%20the%20frontal%20lobes%3A%20evidence%20from%20confabulation%20and%20amnesia&journal=Neuropsychologia&doi=10.1016%2FS0028-3932%2897%2900028-6&volume=35&pages=1017-1034&publication_year=1997&author=Moscovitch%2CM&author=Melo%2CB)\n    \n104.  Lin, W. J., Horner, A. J. & Burgess, N. Ventromedial prefrontal cortex, adding value to autobiographical memories. _Sci. Rep._ **6**, 28630 (2016).\n    \n    [Article](https://doi.org/10.1038%2Fsrep28630)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhtVKitbfE)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=27338616)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4919650)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Ventromedial%20prefrontal%20cortex%2C%20adding%20value%20to%20autobiographical%20memories&journal=Sci.%20Rep.&doi=10.1038%2Fsrep28630&volume=6&publication_year=2016&author=Lin%2CWJ&author=Horner%2CAJ&author=Burgess%2CN)\n    \n105.  Gluck, M. A. & Myers, C. E. Hippocampal mediation of stimulus representation: a computational theory. _Hippocampus_ **3**, 491–516 (1993).\n    \n    [Article](https://doi.org/10.1002%2Fhipo.450030410)  [CAS](https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2c%2FptlKrtw%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=8269040)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20mediation%20of%20stimulus%20representation%3A%20a%20computational%20theory&journal=Hippocampus&doi=10.1002%2Fhipo.450030410&volume=3&pages=491-516&publication_year=1993&author=Gluck%2CMA&author=Myers%2CCE)\n    \n106.  Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. & Lipson, H. Understanding neural networks through deep visualization. Preprint at [https://arxiv.org/abs/1506.06579](https://arxiv.org/abs/1506.06579) (2015).\n    \n107.  Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with CLIP latents. Preprint at [https://arxiv.org/abs/2204.06125](https://arxiv.org/abs/2204.06125) (2022).\n    \n108.  O’Keefe, J. & Dostrovsky, J. The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat. _Brain Res._ **34**, 171–175 (1971).\n    \n109.  Ekstrom, A. D. et al. Human hippocampal theta activity during virtual navigation. _Hippocampus_ **15**, 881–889 (2005).\n    \n    [Article](https://doi.org/10.1002%2Fhipo.20109)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=16114040)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Human%20hippocampal%20theta%20activity%20during%20virtual%20navigation&journal=Hippocampus&doi=10.1002%2Fhipo.20109&volume=15&publication_year=2005&author=Ekstrom%2CAD)\n    \n110.  Eichenbaum, H. Time cells in the hippocampus: a new dimension for mapping memories. _Nat. Rev. Neurosci._ **15**, 732–744 (2014).\n    \n    [Article](https://doi.org/10.1038%2Fnrn3827)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25269553)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4348090)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Time%20cells%20in%20the%20hippocampus%3A%20a%20new%20dimension%20for%20mapping%20memories&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3827&volume=15&publication_year=2014&author=Eichenbaum%2CH)\n    \n111.  Umbach, G. et al. Time cells in the human hippocampus and entorhinal cortex support episodic memory. _Proc. Natl Acad. Sci. USA_ **117**, 28463–28474 (2020).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.2013250117)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33109718)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7668099)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Time%20cells%20in%20the%20human%20hippocampus%20and%20entorhinal%20cortex%20support%20episodic%20memory&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2013250117&volume=117&publication_year=2020&author=Umbach%2CG)\n    \n112.  Dordek, Y., Soudry, D., Meir, R. & Derdikman, D. Extracting grid cell characteristics from place cell inputs using non-negative principal component analysis. _Elife_ **5**, e10094 (2016).\n    \n    [Article](https://doi.org/10.7554%2FeLife.10094)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=26952211)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4841785)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Extracting%20grid%20cell%20characteristics%20from%20place%20cell%20inputs%20using%20non-negative%20principal%20component%20analysis&journal=Elife&doi=10.7554%2FeLife.10094&volume=5&publication_year=2016&author=Dordek%2CY&author=Soudry%2CD&author=Meir%2CR&author=Derdikman%2CD)\n    \n113.  Stachenfeld, K. L., Botvinick, M. M. & Gershman, S. J. The hippocampus as a predictive map. _Nat. Neurosci._ **20**, 1643–1653 (2017).\n    \n    [Article](https://doi.org/10.1038%2Fnn.4650)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=28967910)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20hippocampus%20as%20a%20predictive%20map&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn.4650&volume=20&publication_year=2017&author=Stachenfeld%2CKL&author=Botvinick%2CMM&author=Gershman%2CSJ)\n    \n114.  Tsao, A. et al. Integrating time from experience in the lateral entorhinal cortex. _Nature_ **561**, 57–62 (2018).\n    \n    [Article](https://doi.org/10.1038%2Fs41586-018-0459-6)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30158699)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Integrating%20time%20from%20experience%20in%20the%20lateral%20entorhinal%20cortex&journal=Nature&doi=10.1038%2Fs41586-018-0459-6&volume=561&publication_year=2018&author=Tsao%2CA)\n    \n115.  Bright, I. M. et al. A temporal record of the past with a spectrum of time constants in the monkey entorhinal cortex. _Proc. Natl Acad. Sci. USA_ **117**, 20274–20283 (2020).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.1917197117)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32747574)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7443936)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20temporal%20record%20of%20the%20past%20with%20a%20spectrum%20of%20time%20constants%20in%20the%20monkey%20entorhinal%20cortex&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.1917197117&volume=117&publication_year=2020&author=Bright%2CIM)\n    \n116.  Howard, M. W. & Kahana, M. J. A distributed representation of temporal context. _J. Math. Psychol._ **46**, 269–299 (2002).\n    \n    [Article](https://doi.org/10.1006%2Fjmps.2001.1388)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=A%20distributed%20representation%20of%20temporal%20context&journal=J.%20Math.%20Psychol.&doi=10.1006%2Fjmps.2001.1388&volume=46&publication_year=2002&author=Howard%2CMW&author=Kahana%2CMJ)\n    \n117.  Moscovitch, M., Cabeza, R., Winocur, G. & Nadel, L. Episodic memory and beyond: the hippocampus and neocortex in transformation. _Annu. Review Psychol._ **67**, 105–134 (2016).\n    \n    [Article](https://doi.org/10.1146%2Fannurev-psych-113011-143733)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Episodic%20memory%20and%20beyond%3A%20the%20hippocampus%20and%20neocortex%20in%20transformation&journal=Annu.%20Review%20Psychol.&doi=10.1146%2Fannurev-psych-113011-143733&volume=67&pages=105-134&publication_year=2016&author=Moscovitch%2CM&author=Cabeza%2CR&author=Winocur%2CG&author=Nadel%2CL)\n    \n118.  Strange, B. A., Witter, M. P., Lein, E. S. & Moser, E. I. Functional organization of the hippocampal longitudinal axis. _Nat. Rev. Neurosci._ **15**, 655–669 (2014).\n    \n    [Article](https://doi.org/10.1038%2Fnrn3785)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=25234264)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Functional%20organization%20of%20the%20hippocampal%20longitudinal%20axis&journal=Nat.%20Rev.%20Neurosci.&doi=10.1038%2Fnrn3785&volume=15&publication_year=2014&author=Strange%2CBA&author=Witter%2CMP&author=Lein%2CES&author=Moser%2CEI)\n    \n119.  Káli, S. & Dayan, P. Off-line replay maintains declarative memories in a model of hippocampal–neocortical interactions. _Nat. Neurosci._ **7**, 286–294 (2004).\n    \n    [Article](https://doi.org/10.1038%2Fnn1202)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=14983183)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Off-line%20replay%20maintains%20declarative%20memories%20in%20a%20model%20of%20hippocampal%E2%80%93neocortical%20interactions&journal=Nat.%20Neurosci.&doi=10.1038%2Fnn1202&volume=7&pages=286-294&publication_year=2004&author=K%C3%A1li%2CS&author=Dayan%2CP)\n    \n120.  van de Ven, G. M. & Tolias, A. S. Generative replay with feedback connections as a general strategy for continual learning. Preprint at [https://arxiv.org/abs/1809.10635](https://arxiv.org/abs/1809.10635) (2018).\n    \n121.  Singh, D., Norman, K. A. & Schapiro, A. C. A model of autonomous interactions between hippocampus and neocortex driving sleep-dependent memory consolidation. _Proc. Natl Acad. Sci. USA_ **119**, e2123432119 (2022).\n    \n122.  Millidge, B., Salvatori, T., Song, Y., Lukasiewicz, T. & Bogacz, R. Universal Hopfield networks: a general framework for single-shot associative memory models. in _International Conference on Machine Learning_ 15561–15583 (PMLR, 2022).\n    \n123.  Chaudhry, H. T., Zavatone-Veth, J. A., Krotov, D. & Pehlevan, C. Long sequence Hopfield memory. Preprint at [https://arxiv.org/abs/2306.04532](https://arxiv.org/abs/2306.04532) (2023).\n    \n124.  Tang, M., Barron, H. & Bogacz, R. Sequential memory with temporal predictive coding. _Adv. Neural Inf. Process. Syst._ **27** (2023).\n    \n125.  Burgess, N. & Hitch, G. J. Memory for serial order: a network model of the phonological loop and its timing. _Psychol. Rev._ **106**, 551–581 (1999).\n    \n    [Article](https://doi.org/10.1037%2F0033-295X.106.3.551)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Memory%20for%20serial%20order%3A%20a%20network%20model%20of%20the%20phonological%20loop%20and%20its%20timing&journal=Psychol.%20Rev.&doi=10.1037%2F0033-295X.106.3.551&volume=106&publication_year=1999&author=Burgess%2CN&author=Hitch%2CGJ)\n    \n126.  Radford, A. et al. Language models are unsupervised multitask learners. _OpenAI Blog_ [https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019).\n    \n127.  Bird, C. M. How do we remember events? _Curr. Opin. Behav. Sci._ **32**, 120–125 (2020).\n    \n    [Article](https://doi.org/10.1016%2Fj.cobeha.2020.01.020)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=How%20do%20we%20remember%20events%3F&journal=Curr.%20Opin.%20Behav.%20Sci.&doi=10.1016%2Fj.cobeha.2020.01.020&volume=32&pages=120-125&publication_year=2020&author=Bird%2CCM)\n    \n128.  Mullally, S. L., Intraub, H. & Maguire, E. A. Attenuated boundary extension produces a paradoxical memory advantage in amnesic patients. _Curr. Biol._ **22**, 261–268 (2012).\n    \n    [Article](https://doi.org/10.1016%2Fj.cub.2012.01.001)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XitFKjs70%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22264610)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3315012)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Attenuated%20boundary%20extension%20produces%20a%20paradoxical%20memory%20advantage%20in%20amnesic%20patients&journal=Curr.%20Biol.&doi=10.1016%2Fj.cub.2012.01.001&volume=22&pages=261-268&publication_year=2012&author=Mullally%2CSL&author=Intraub%2CH&author=Maguire%2CEA)\n    \n129.  De Luca, F. et al. Boundary extension is attenuated in patients with ventromedial prefrontal cortex damage. _Cortex_ **108**, 1–12 (2018).\n    \n    [Article](https://doi.org/10.1016%2Fj.cortex.2018.07.002)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30086391)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6238077)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Boundary%20extension%20is%20attenuated%20in%20patients%20with%20ventromedial%20prefrontal%20cortex%20damage&journal=Cortex&doi=10.1016%2Fj.cortex.2018.07.002&volume=108&pages=1-12&publication_year=2018&author=Luca%2CF)\n    \n130.  Van Der Kolk, B. A., Burbridge, J. A. & Suzuki, J. The psychobiology of traumatic memory. Clinical implications of neuroimaging studies. _Ann. N. Y. Acad. Sci._ **821**, 99–113 (1997).\n    \n131.  Bisby, J. A., Burgess, N. & Brewin, C. R. Reduced memory coherence for negative events and its relationship to posttraumatic stress disorder. _Curr. Dir. Psychol. Sci._ **29**, 267–272 (2020).\n    \n    [Article](https://doi.org/10.1177%2F0963721420917691)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33214741)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7643751)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Reduced%20memory%20coherence%20for%20negative%20events%20and%20its%20relationship%20to%20posttraumatic%20stress%20disorder&journal=Curr.%20Dir.%20Psychol.%20Sci.&doi=10.1177%2F0963721420917691&volume=29&pages=267-272&publication_year=2020&author=Bisby%2CJA&author=Burgess%2CN&author=Brewin%2CCR)\n    \n132.  Pellicano, E. & Burr, D. When the world becomes ‘too real’: a Bayesian explanation of autistic perception. _Trends Cogn. Sci._ **16**, 504–510 (2012).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2012.08.009)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=22959875)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=When%20the%20world%20becomes%20%E2%80%98too%20real%E2%80%99%3A%20a%20Bayesian%20explanation%20of%20autistic%20perception&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2012.08.009&volume=16&pages=504-510&publication_year=2012&author=Pellicano%2CE&author=Burr%2CD)\n    \n133.  Barry, D. N. & Love, B. C. A neural network account of memory replay and knowledge consolidation. _Cereb. Cortex._ **33**, 83–95 (2022).\n    \n134.  Behrens, T. E. et al. What is a cognitive map? Organizing knowledge for flexible behavior. _Neuron_ **100**, 490–509 (2018).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuron.2018.10.002)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXitVSisrnK)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30359611)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=What%20is%20a%20cognitive%20map%3F%20Organizing%20knowledge%20for%20flexible%20behavior&journal=Neuron&doi=10.1016%2Fj.neuron.2018.10.002&volume=100&pages=490-509&publication_year=2018&author=Behrens%2CTE)\n    \n135.  Nieh, E. H. et al. Geometry of abstract learned knowledge in the hippocampus. _Nature_ **595**, 80–84 (2021).\n    \n    [Article](https://doi.org/10.1038%2Fs41586-021-03652-7)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXhtleqs7jF)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34135512)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC9549979)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Geometry%20of%20abstract%20learned%20knowledge%20in%20the%20hippocampus&journal=Nature&doi=10.1038%2Fs41586-021-03652-7&volume=595&pages=80-84&publication_year=2021&author=Nieh%2CEH)\n    \n136.  Burgess, C. & Kim, H. 3D Shapes dataset_._ _GitHub_ [https://github.com/google-deepmind/3d-shapes](https://github.com/google-deepmind/3d-shapes) (2018).\n    \n137.  Hou, X., Shen, L., Sun, K. & Qiu, G. Deep feature consistent variational autoencoder. in _2017 IEEE Winter Conference on Applications of Computer Vision (WACV)_ 1133–1141 (IEEE, 2017).\n    \n138.  Stella, F., Baracskay, P., O’Neill, J. & Csicsvari, J. Hippocampal reactivation of random trajectories resembling Brownian diffusion. _Neuron_ **102**, 450–461 (2019).\n    \n    [Article](https://doi.org/10.1016%2Fj.neuron.2019.01.052)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1MXjs1yitL4%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30819547)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Hippocampal%20reactivation%20of%20random%20trajectories%20resembling%20Brownian%20diffusion&journal=Neuron&doi=10.1016%2Fj.neuron.2019.01.052&volume=102&pages=450-461&publication_year=2019&author=Stella%2CF&author=Baracskay%2CP&author=O%E2%80%99Neill%2CJ&author=Csicsvari%2CJ)\n    \n139.  González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E. & Bazhenov, M. Can sleep protect memories from catastrophic forgetting? _Elife_ **9**, e51005 (2020).\n    \n    [Article](https://doi.org/10.7554%2FeLife.51005)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=32748786)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC7440920)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Can%20sleep%20protect%20memories%20from%20catastrophic%20forgetting%3F&journal=Elife&doi=10.7554%2FeLife.51005&volume=9&publication_year=2020&author=Gonz%C3%A1lez%2COC&author=Sokolov%2CY&author=Krishnan%2CGP&author=Delanois%2CJE&author=Bazhenov%2CM)\n    \n140.  Pezzulo, G., Zorzi, M. & Corbetta, M. The secret life of predictive brains: what’s spontaneous activity for? _Trends Cogn. Sci._ **25**, 730–743 (2021).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2021.05.007)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=34144895)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC8363551)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=The%20secret%20life%20of%20predictive%20brains%3A%20what%E2%80%99s%20spontaneous%20activity%20for%3F&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2021.05.007&volume=25&pages=730-743&publication_year=2021&author=Pezzulo%2CG&author=Zorzi%2CM&author=Corbetta%2CM)\n    \n141.  Igata, H., Ikegaya, Y. & Sasaki, T. Prioritized experience replays on a hippocampal predictive map for learning. _Proc. Natl Acad. Sci. USA_ **118**, e2011266118 (2021).\n    \n    [Article](https://doi.org/10.1073%2Fpnas.2011266118)  [CAS](https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXnsVOrsA%3D%3D)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=33443144)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Prioritized%20experience%20replays%20on%20a%20hippocampal%20predictive%20map%20for%20learning&journal=Proc.%20Natl%20Acad.%20Sci.%20USA&doi=10.1073%2Fpnas.2011266118&volume=118&publication_year=2021&author=Igata%2CH&author=Ikegaya%2CY&author=Sasaki%2CT)\n    \n142.  Abadi, M. et al. TensorFlow: a system for large-scale machine learning. in _12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)_ 265–283 (USENIX Assoc., 2016).\n    \n143.  Whittington, J. C. R. & Bogacz, R. Theories of error back-propagation in the brain. _Trends Cogn. Sci._ **23**, 235–250 (2019).\n    \n    [Article](https://doi.org/10.1016%2Fj.tics.2018.12.005)  [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Abstract&list_uids=30704969)  [PubMed Central](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6382460)  [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Theories%20of%20error%20back-propagation%20in%20the%20brain&journal=Trends%20Cogn.%20Sci.&doi=10.1016%2Fj.tics.2018.12.005&volume=23&pages=235-250&publication_year=2019&author=Whittington%2CJCR&author=Bogacz%2CR)\n    \n144.  Khattar, D., Goud, J. S., Gupta, M. & Varma, V. Mvae: multimodal variational autoencoder for fake news detection. in _The World Wide Web Conference_ 2915–2921 (ACM, 2019).\n    \n145.  Ramesh, A. et al. Zero-shot text-to-image generation. in _International Conference on Machine Learning_ 8821–8831 (PMLR, 2021).\n    \n146.  McInnes, L., Healy, J. & Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. Preprint at [https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426) (2018).\n    \n147.  Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R. Dropout: a simple way to prevent neural networks from overfitting. _J. Mach. Learn. Res._ **15**, 1929–1958 (2014).\n    \n    [Google Scholar](http://scholar.google.com/scholar_lookup?&title=Dropout%3A%20a%20simple%20way%20to%20prevent%20neural%20networks%20from%20overfitting&journal=J.%20Mach.%20Learn.%20Res.&volume=15&pages=1929-1958&publication_year=2014&author=Srivastava%2CN&author=Hinton%2CG&author=Krizhevsky%2CA&author=Sutskever%2CI&author=Salakhutdinov%2CR)\n    \n148.  Chollet, F. et al. _Keras Documentation_ (GitHub, 2015).\n    \n\n[Download references](https://citation-needed.springer.com/v2/references/10.1038/s41562-023-01799-z?format=refman&flavour=references)\n\nAcknowledgements\n----------------\n\nWe thank T. Behrens, B. Love and D. Bush for useful discussions, and K. Norman for constructive comments on an earlier version.\n\nFunding support for this work was received from a Wellcome Principal Research Fellowship ‘Neural mechanisms of memory and prediction: Finding structure in experience’ (222457/Z/21/Z) (N.B.), a Wellcome Collaborative Award ‘Organising knowledge for flexible behaviour in the prefrontal-hippocampal circuitry’ (214314/Z/18/Z) (N.B.), and an ERC advanced grant NEUROMEM (N.B.). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.\n\nAuthor information\n------------------\n\n### Authors and Affiliations\n\n1.  UCL Institute of Cognitive Neuroscience, University College London, London, UK\n    \n    Eleanor Spens & Neil Burgess\n    \n2.  UCL Queen Square Institute of Neurology, University College London, London, UK\n    \n    Neil Burgess\n    \n\nAuthors\n\n1.  Eleanor Spens\n    \n    You can also search for this author in [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Eleanor%20Spens) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Eleanor%20Spens%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n    \n2.  Neil Burgess\n    \n    You can also search for this author in [PubMed](http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&term=Neil%20Burgess) [Google Scholar](http://scholar.google.co.uk/scholar?as_q=&num=10&btnG=Search+Scholar&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=%22Neil%20Burgess%22&as_publication=&as_ylo=&as_yhi=&as_allsubj=all&hl=en)\n    \n\n### Contributions\n\nE.S. and N.B. designed the research and wrote the paper. E.S. performed the computational modelling.\n\n### Corresponding authors\n\nCorrespondence to [Eleanor Spens](mailto:eleanor.spens.20@ucl.ac.uk) or [Neil Burgess](mailto:n.burgess@ucl.ac.uk).\n\nEthics declarations\n-------------------\n\n### Competing interests\n\nThe authors declare no competing interests.\n\nPeer review\n-----------\n\n### Peer review information\n\n_Nature Human Behaviour_ thanks Gido van de Ven and the other, anonymous, reviewer(s) for their contribution to the peer review of this work.\n\nAdditional information\n----------------------\n\n**Publisher’s note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nSupplementary information\n-------------------------\n\nRights and permissions\n----------------------\n\n**Open Access** This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/).\n\n[Reprints and permissions](https://s100.copyright.com/AppDispatchServlet?title=A%20generative%20model%20of%20memory%20construction%20and%20consolidation&author=Eleanor%20Spens%20et%20al&contentID=10.1038%2Fs41562-023-01799-z&copyright=The%20Author%28s%29&publication=2397-3374&publicationDate=2024-01-19&publisherName=SpringerNature&orderBeanReset=true&oa=CC%20BY)\n\nAbout this article\n------------------\n\n[![Image 33: Check for updates. Verify currency and authenticity via CrossMark](blob:https://www.nature.com/df43c82f6cb6dcd2a87db38f317bf9f2)](https://crossmark.crossref.org/dialog/?doi=10.1038/s41562-023-01799-z)\n\n### Cite this article\n\nSpens, E., Burgess, N. A generative model of memory construction and consolidation. _Nat Hum Behav_ **8**, 526–543 (2024). https://doi.org/10.1038/s41562-023-01799-z\n\n[Download citation](https://citation-needed.springer.com/v2/references/10.1038/s41562-023-01799-z?format=refman&flavour=citation)\n\n*   Received: 30 May 2023\n    \n*   Accepted: 05 December 2023\n    \n*   Published: 19 January 2024\n    \n*   Issue Date: March 2024\n    \n*   DOI: https://doi.org/10.1038/s41562-023-01799-z",
  "usage": {
    "tokens": 70368
  }
}
```
