---
title: RedStone: Curating General, Code, Math, and QA Data for Large Language Models
description: 
url: https://arxiv.org/html/2412.03398v1#S2
timestamp: 2025-01-20T16:18:37.047Z
domain: arxiv.org
path: html_2412.03398v1
---

# RedStone: Curating General, Code, Math, and QA Data for Large Language Models



## Content

1Introduction
2RedStone
3Experiments
4Related Work
5Conclusion and Future Work
\pdfcolInitStack

tcb@breakable \newmdenv[ font=, linewidth=0.5pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=10pt, innerbottommargin=10pt, ]monobox

RedStone
: Curating General, Code, Math, and QA Data for Large Language Models
Yaoyao CHANG, Lei CUI, Li DONG, Shaohan HUANG, Yangyu HUANG,
Yupan HUANG, Scarlett LI, Tengchao LV, Shuming MA, Qinzheng SUN,
Wenhui WANG, Furu WEI, Ying XIN, Mao YANG, Qiufeng YIN, Xingxing ZHANG.

Microsoft Research
https://aka.ms/GeneralAI

Project Page: https://aka.ms/redstone
The contributors are listed in the alphabetical order by last names. Corresponding author: Furu WEI (fuwei@microsoft.com.)
Abstract

Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge. We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets. Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains. In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks. The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets. Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs. This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs. RedStone code and data samples will be publicly available at https://aka.ms/redstone.

1Introduction

Large Language Models (LLMs) have demonstrated remarkable potential as highly capable AI assistants, particularly in complex reasoning tasks that require expert knowledge across diverse fields [54, 4, 59, 20]. The recent advancements in LLMs have been driven not only by increasing model sizes but also by scaling up dataset sizes correspondingly. These advancements have created a demand for robust data curation pipelines capable of mining vast, diverse datasets across various domains [35, 4, 13, 59, 20].

In addition to general domain data that spans a wide range of topics, there is an increasing need for domain-specific data tailored to specialized fields such as mathematics and code. These domains require high-quality, domain-specific knowledge to enhance model performance. Current approaches for constructing domain-specific datasets often rely on proprietary resources [30], synthetic LLM-generated data [63, 32], or manually annotated data [4, 1, 36], which is often time-consuming and labor-intensive. Moreover, these sources are limited in scope and scale, often resulting in small dataset sizes. Common Crawl, a massive open-access web archive, offers a wealth of diverse, high-quality data across various domains. Although recent efforts, such as RefinedWeb [50] and Redpajama v2 [14], have explored mining data from Common Crawl, their focus has primarily been on general data extraction, with limited attention to domain-specific data.

In this paper, we introduce RedStone, a comprehensive pipeline designed to efficiently extract and filter large-scale datasets from Common Crawl, applicable to both general domain data and domain-specific data. As shown in Figure 1, we propose using Common Crawl as a valuable source for mining knowledge, as its web data often includes annotations that provide extensive context and reasoning. For instance, code snippets and mathematics equations in Common Crawl are frequently accompanied by discussions, explanations, and even execution results, offering richer insights than standalone source code and equations. Open-domain question answering is often interleaved with other text content in the document, which may include reading passages, explanations, or other informative content.

The RedStone pipeline defines general or domain-specific data formats, then extracts and filters relevant data from Common Crawl. The Extraction module processes raw web data using techniques like pattern recognition and NLP to capture the necessary formats for training. The Filtering module refines this data, retaining only the most relevant content through keyword searches, regular expressions, and machine learning models. This approach efficiently generates large-scale datasets, streamlining the process of data sourcing, synthesis, and annotation. For example, in the process of obtaining open question answering data, the extraction module retrieves the main content from web pages, using WET files and CCNet’s paragraph-level deduplication method to ensure completeness. The filtering module then employs a two-stage approach: rule-based filtering identifies common patterns for questions and answers using keywords, while model-based filtering applies a self-trained classifier to further refine the data. This process results in a large, high-quality dataset of open-domain questions, efficiently combining rule-based and machine learning techniques.

We curate general domain data from two available formats of Common Crawl, resulting in the RedStone-Web dataset, which contains approximately 3.17 trillion tokens of general knowledge. For domain-specific data, we focus on constructing datasets for code, mathematics, and question answering, as these areas are crucial to the core capabilities of large language models. The resulting datasets—RedStone-Code, RedStone-Math, and RedStone-QA—comprise 250.2, 15.9, and 51.4 billion tokens, respectively.

Our constructed general domain data, sourced from diverse web pages, enhances the model’s understanding of language in a broad context, while domain-specific data provides specialized knowledge in fields such as code, mathematics, and question answering. We evaluated RedStone across a range of tasks, demonstrating its effectiveness in enhancing model performance. RedStone-Web outperformed other open-source datasets in common sense reasoning tasks, such as ARC-e, HellaSwag, OpenBookQA, and PIQA. Incorporating RedStone-Code into the general dataset significantly boosted performance in code generation benchmarks, including HumanEval and MBPP. Similarly, RedStone-Math outperformed existing datasets on mathematics benchmarks like GSM8k and MATH, demonstrating improved perplexity. Finally, RedStone-QA achieved the highest scores in question-answering tasks, particularly on the MMLU benchmark, confirming the strength of our pipeline in extracting high-quality data across diverse domains.

Our contributions are summarized as follows:

• 

We introduce RedStone, a pipeline for obtaining large-scale diverse data from the web with a detailed description of the pipeline’s components and workflow for reproducibility.

• 

Using RedStone, we mined and constructed several large-scale datasets totaling 3.48 trillion tokens, encompassing both general domain and domain-specific data. The RedStone-Web dataset contains approximately 3.17 trillion tokens of general knowledge, while RedStone-Code, RedStone-Math, and RedStone-QA consist of 250.2 billion, 15.9 billion, and 51.4 billion tokens, respectively.

• 

We demonstrate the effectiveness of RedStone through extensive evaluations across common sense reasoning, code generation, and mathematics tasks. Our general domain dataset, RedStone-Web, outperforms existing open-source datasets in common sense reasoning benchmarks, while the inclusion of RedStone-Code and RedStone-Math significantly improves model performance in code generation and mathematical problem solving. Additionally, RedStone-QA shows notable advancements in question-answering tasks, further establishing RedStone as a robust dataset for diverse pre-training applications.

• 

We discuss strategies for maximizing the potential of Common Crawl. To the best of our knowledge, this is the first systematic exploration of the full potential of Common Crawl, demonstrating its capability as a rich resource for various domain-specific contents.

2RedStone

RedStone is a pipeline designed for the large-scale extraction of various data from web data consisting of Extraction and Filtering modules as detailed below.

• 

Extraction. Extracting raw data to obtain the required format for training. Extraction can involve the use of pattern recognition, natural language processing, and other computational methods to obtain the desired information.

• 

Filtering. Selecting relevant data and excluding unnecessary or irrelevant information to focus on the most pertinent data for analysis. Filtering techniques can include keyword searches, regular expressions, and machine learning models to ensure that only the most relevant data is retained.

This section will detail the process by which RedStone employs these two modules to construct various types of data. Additionally, in terms of dataset types, RedStone categorizes data into general domain data and domain-specific data, below are the specific definitions of these two types.

• 

General domain data helps the model understand language in a broad context, enhancing its comprehension of the real world. Currently, the primary source of pre-training data for LLMs is general domain data, which is crucial for their basic performance. However, existing pipelines for data processing still have limitations, to improve efficiency, certain steps have been simplified. RedStone integrates current mainstream pipelines, restructures the processing steps, and refines certain stages to produce a high-quality general domain dataset, RedStone-Web.

• 

Domain-specific data refers to detailed, context-specific information pertinent to particular fields, tasks, or scenarios, such as mathematics, coding, reasoning, medicine, law, engineering, and finance. This high-quality data can significantly improve model performance but is often time-consuming and labor-intensive to construct. Common Crawl contains a substantial amount of domain-specific data, which can be extracted at scale using specialized pipelines and filtering techniques to enhance quality. Additionally, web data often includes discussions and explanations, which help LLMs gain a deeper understanding of specific knowledge. Taking code, math, and QA as examples, RedStone has constructed large-scale domain-specific datasets from Common Crawl, namely RedStone-Code, RedStone-Math, and RedStone-QA.

Figure 1:Using RedStone, we created two types of data: general domain data and domain-specific data. General domain data comprises RedStone-Web, it does not specify a data domain, allowing the model to learn common knowledge across various domains. Domain-specific data includes RedStone-Code, RedStone-Math, and RedStone-QA, enabling the model to acquire specialized knowledge in particular areas or formats. Each example type features the original webpage screenshot on the left and the corresponding data processed by RedStone on the right.

The construction processes for the two types of data will be introduced separately. Details on RedStone-Web within general domain data are provided in Section 2.1, while RedStone-Code, RedStone-Math, and RedStone-QA within domain-specific data are described in Section 2.2, 2.3, and 2.4, respectively.

	Dataset	Tokens (B)
General Domain Data	RedStone-Web	3,170.2
Domain-specific Data	RedStone-Code	250.2
RedStone-Math 	15.9
RedStone-QA 	51.4
Table 1:Dataset statistics for general and specific domain data constructed using RedStone.
2.1RedStone-Web
Figure 2:Subsequent stages of RedStone-Web. RedStone processes Common Crawl data in separate steps, handling WARC and WET files independently before merging them to increase the token count. Over 99% of the tokens in Common Crawl are removed during processing. Since WARC files are in HTML format and inconvenient for token counting, and WARC and WET files represent different forms of the same data, the token count from WET files is used as the original token count for both formats.

To construct general domain data, we extract the main content from raw web pages and filter data based on the quality of the pages. As illustrated in Figure 2 and shown in Table 2, through extraction and filtering, we obtained a comprehensive dataset, RedStone-Web, containing approximately 3.17 trillion tokens of general knowledge.

General Domain Data	Tokens (T)	Pages (M)	Tokens per Page
WET	1.74	856	2,032
WARC	1.43	1,223	1,169
Total	3.17	2,079	1,524
Table 2:RedStone-Web from Common Crawl.
2.1.1Extraction

Common Crawl is available in WARC and WET formats. WARC files contain the raw HTML responses, while WET files are pre-processed to include only plain text. Each record corresponds to a single webpage from a specific URL, representing an independent document or sample.

We extract general domain data from Common Crawl in both WET and WARC formats, encompassing 93 CommonCrawl snapshots spanning from 2013-20 to 2023-40, resulting in two complementary datasets. Although these formats originate from the same data source and represent different forms of the same dataset, the differences in text extraction between WET and WARC lead to variations in the resulting plain text.

Text Extraction of WET.

For text extraction from WET files, we adapt the deduplication strategy used in CCNet [60] to eliminate noisy text from web pages. Specifically, the deduplication strategy removes frequently appeared common segments (e.g., ’sign in,’ ’follow,’ ’about’) on each page by identifying the duplicates against other paragraph within a shard of files. Instead of limiting the deduplication process to 5GB segments, we expanded the search to one snapshot of the Common Crawl. This comprehensive approach enhances the ability to detect and remove noisy text, resulting in cleaner, higher-quality extracted content.

Text Extraction of WARC.

For text extraction from WARC files, we followed the implementation used by RefinedWeb [50], utilizing the Trafilatura [3]. Trafilatura is designed to extract meaningful content from web pages while excluding irrelevant sections such as menus, headers, and footers. It employs a combination of techniques including HTML parsing, content density analysis, and feature extraction to identify and isolate the main body of text.

Once the high-density text regions are identified, Trafilatura applies feature extraction techniques to further refine the selection. This includes looking for patterns and structures typical of main content, such as paragraphs, headings, and continuous text blocks, while ignoring sections with numerous links or short, unrelated text fragments.

By employing these techniques, Trafilatura effectively filters out non-essential parts of the web pages, resulting in cleaner and more relevant text extraction from WARC files. This approach ensures that the extracted dataset is of higher quality, focusing on the core content while minimizing noise.

2.1.2Filtering

Common Crawl contains a large number of low-quality pages, with RefinedWeb removing nearly 90% of the documents originally in Common Crawl. Therefore, the effectiveness of document quality filtering significantly impacts the final quality of the dataset. High-quality datasets enable faster convergence and better model performance [14, 34, 38]. Consequently, we employed a filtering module to eliminate low-quality texts. This filtering module consists of four stages: language filtering, rule-based filtering, model-based filtering, and deduplication.

Language Filtering.

Common Crawl encompasses nearly all languages globally. We use fastText [29] as the language identification tool to filter out non-English pages with a confidence threshold of 0.5.

Rule-based Filtering.

Given the vast number of original pages, we initially use rule-based filtering to quickly sift through all pages, effectively reducing the subsequent filtering workload.

For the main content extracted from WET files, we adhere to the filtering rules provided by CCNet, using length as a criterion with a threshold 300 to filter out short texts.

For the main content extracted from WARC files, we follow the rules used by RefinedWeb, conducting repetition removal, document-level removal, and sentence-level removal. Repetition removal detects whether there is a significant amount of internal repetition within the text (e.g., a sentence repeated multiple times on a page), document-level removal determines whether to discard the current document based on its features, and sentence-level removal assesses whether to discard specific sentences based on their characteristics.

Although the same set of processing rules can be applied to both WET and WARC files, we follow each pipeline’s specific rules to filter separately. This makes the data more diverse, allowing the two datasets to complement each other and combine into a large-scale pre-training dataset. The detailed steps for WARC files are as follows.

• 

Repetition Removal Rules. Documents that match any of following rules will be discarded.

– 

If the ratio of the number of duplicated sentences to the total sentence count exceeds 0.3;

– 

If the ratio of the count of characters in duplicated sentences to the total character count exceeds 0.2;

– 

If the ratio of the number of duplicated paragraphs to the total sentence count exceeds 0.3;

– 

If the ratio of the count of characters in duplicated paragraphs to the total character count exceeds 0.2;

– 

for each 
𝑛
∈
{
2
⁢
…
⁢
4
}
, if the ratio of the count of characters in most common n-gram to the total character count exceeds threshold, the detail thresholds are given in Table 3.

– 

for each 
𝑛
∈
{
5
⁢
…
⁢
10
}
, if the ratio of the count of characters in duplicated n-grams (each character is only counted once regardless of its occurrence in several overlapping n-grams) to the total character count exceeds threshold, the detail thresholds are given in Table 3.

Rule	Threshold
most common 2-gram	0.20
most common 3-gram	0.18
most common 4-gram	0.16
duplicated 5-gram	0.15
duplicated 6-gram	0.14
duplicated 7-gram	0.13
duplicated 8-gram	0.12
duplicated 9-gram	0.11
duplicated 10-gram	0.10
Table 3:Thresholds of n-gram character ratio related rules.
• 

Document-Level Rules. Documents that match any of following rules will be discarded.

– 

If document does not contain between 50 and 100,000 words;

– 

If mean word length is outside the range of 3 to 10 characters;

– 

If the ratio of the number of hash symbols and ellipses to the total word count exceeds 0.1;

– 

If more than 90% of sentences in the document start with bullet point;

– 

If more than 30% of sentences in the document end with an ellipsis;

– 

If more than 20% of words in the document doesn’t contain alphabetic characters;

– 

If a document does not contain at least two stop words, such as the, be, to, of, and, that, have, and with.

• 

Sentence-Level Rules. Sentences that match any of following rules will be removed, if over 5% words of the document is removed by sentence filters the whole document is discarded.

– 

If over 60% of letters in the sentence is uppercase;

– 

If it only contains numerical characters;

– 

If it is a counter (match regex \̂d+\s+[a-zA-Z]+$, e.g., 3 likes);

– 

If it only contains one word;

– 

If matches of following regex could be found in the sentence

* 

^sign-in

* 

read more...$

* 

items in card

Model-based Filtering.

Rule-based filtering can only identify fixed patterns within the text and cannot assess its quality in terms of syntax, content, or overall coherence. We employed a transformer-based classifier for data filtering, following a process similar to FineWeb-edu [34]. We first sampled data pre-filtered by rule-based methods and had GPT-4 annotate 2,000 samples for quality, evenly split between positive and negative. We then trained stablelm-1.6b on this annotated data to filter the entire dataset. The filtering criteria focused on grammar, logic, and most importantly, knowledge—defined as data that enhances the model’s real-world understanding. This approach ensures the model is trained on high-quality, informative data, improving its performance on downstream tasks.

Deduplication.

The presence of substantial duplicate content on the web necessitates deduplication to reduce the required training steps and increase data diversity [11, 39]. Many studies have demonstrated that deduplication can enhance model performance. For deduplication, we utilized MinHash [5], a popular and efficient technique for detecting duplicate content in large datasets. MinHash approximates the Jaccard similarity between sets, making it suitable for identifying near-duplicate texts without the need for exhaustive pairwise comparisons, which can be computationally prohibitive for massive datasets.

We employ the MinHash-LSH algorithm to extend the deduplication scope across all snapshots. We generate 117 64-bit MinHash signatures (effectively 62-bit due to the limitations of the Meson prime) over 5-gram sequences of documents. These signatures are divided into 9 bands, each containing 13 values, to achieve an estimated 80% Jaccard similarity when an LSH collision occurs.

To reduce computational complexity and enable a broader processing range and faster iteration speed, we opted against using larger MinHash parameters or connected-graph-based deduplication methods.

2.2RedStone-Code

Code is a significant domain for LLMs, particularly in areas such as code generation, code summarization, and code refinement. Leveraging capabilities across different areas, various downstream applications have been developed to improve code quality, accelerate code writing, and fix code bugs. For example, GitHub Copilot1 has been adopted by more than 50,000 organizations and is favored by approximately 55% of developers.

To advance the development of code-related capabilities, several initiatives have focused on creating comprehensive code datasets. One notable example is StarCoder [33], which aggregates code data from the BigCode community. However, these datasets typically consist solely of source code and lack the contextual information or explanations accompanying the code snippets. Unlike previous efforts, our goal is to construct an interleaved code dataset using RedStone that includes both source code and the interleaved text providing context or explanations for the code to enhance understanding and usability.

The original 89 snapshots of Common Crawl contain over 200 billion HTML documents, making efficient processing of these documents a challenging task. We designed the following steps, utilizing the filtering and extraction modules of RedStone. First, we employ the filtering module to exclude pages that are not related to code. Subsequently, we use the extraction module to convert the HTML pages into plain text required for training.

Upon the completion of the aforementioned steps, we have obtained approximately 114.9 million documents, encompassing a total of 250.2 billion tokens, as detailed in Table 1. The detailed steps are as follows.

2.2.1Filtering

Since code snippets in HTML pages are often enclosed in special tags (e.g., <code>), these tags can be used to quickly determine whether a page contains code snippets. Therefore, RedStone-Code uses WARC files as input and employs code tags to filter out pages that do not contain code snippets. Additionally, during the processing of HTML pages, we observed that they often contain a significant amount of noise (e.g., hidden nodes) that could interfere with subsequent content extraction. To address this, we implemented element filtering to remove invisible elements. Moreover, Code snippets sometimes appear within a single element, such as inline or single-line code snippets. Other times, they span multiple elements that share a common parent, such as multi-line code snippets. In these cases, we use rules to determine the overall scope of the code, filter out smaller code elements, and merge the code snippets into a single cohesive unit. The specifics of each step are detailed below.

• 

Document Filtering. To expedite the processing, we implement a pre-filtering operation on the raw HTML content using keyword filters. Specifically, (1) only HTML documents that include the <code or <pre keyword are passed to the next step; (2) To reduce the interweaving situation of two code snippets under the code diff or blame function, we exclude documents whose URLs include blame.php or diff.php.

• 

HTML Element Filtering. There are several ways to hide HTML elements, such as using aria-hidden="true", display:none, and visibility:hidden. Some of these are set in HTML, while others are set via JavaScript or CSS. To prevent duplicated or invisible content from remaining after text extraction, we remove all elements with these properties in the HTML.

• 

Refined Code Filtering. Code snippets in HTML documents can be represented in various ways. Sometimes, a snippet is enclosed in a single <code> element, such as with inline and single-line code snippets. Other times, it spans several <code> elements that share a common parent, as seen with multi-line code snippets. To filter HTML documents for code snippets and identify the root element of each filtered code snippet, we follow these steps:

– 

Traverse each element in the DOM tree of the HTML document and identify all elements of type <code>.

– 

If the parent of each <code> element is either <pre> or <tbody>, treat its parent as a candidate root element for containing the code snippet. Otherwise, the <code> element itself is the candidate root element.

– 

Apply a regular expression to the extracted text from each candidate root element to determine whether it includes a code snippet. Specifically, the candidate root element is considered to contain a code snippet if it matches one of the four conditions in the designed regular expression: keywords of programming languages, code indicators, function calls, and variable assignments.

– 

If a candidate element is identified as containing a code snippet, its type is changed to <code-encode>. Consequently, the entire HTML document is detected as containing code knowledge.

– 

To avoid the interweaving situation of code lines and line numbers in one code snippet, we remove line numbers by identifying and deleting consecutive single-line or alternating-line increasing numbers within each code snippet.

– 

To merge adjacent code snippets that should belong to one code snippet, we detect and combine them by locating a closing code tag immediately followed by an opening code tag.

2.2.2Extraction

In this step, we convert the WARC HTML pages to WET plain text format by adopting the WEATGenerator function in the ia-hadoop-tools library [15], which is the WARC-to-WET conversion method officially used by Common Crawl. We do not use Trafilatura to extract the main content from the HTML because Trafilatura’s recall is relatively low, and it is likely to exclude code snippets from the main content.

2.3RedStone-Math

The ability to reason mathematically is crucial for LLMs. Several studies have aimed to enhance this capability. For instance, the PaLM [13] was fine-tuned on billions of tokens from mathematical documents sourced from arXiv and the web, while OpenWebMath  [51] was trained on 14.7 billion tokens of mathematical webpages from Common Crawl. Both models have shown significant improvements in solving problems that require quantitative reasoning.

Mathematical formulas or code snippets on web pages typically have corresponding HTML tags, making the logic for constructing these two types of data quite similar. There are only slight differences in the processing steps. By making appropriate modifications, we utilized the HTML content from Common Crawl’s WARC files to filter pages containing mathematical content on a large scale.

Additionally, during the construction of RedStone-Math, we discovered a substantial number of mathematical formulas in ASCII format. These formulas exist as plain text on web pages and cannot be identified through HTML tags. Therefore, for these mathematical pages, we designed a new processing workflow specifically to extract these formulas.

Finally, we compiled a comprehensive RedStone-Math totaling 15.9 billion tokens, as illustrated in Table 4. This dataset encompasses mathematical formulas along with their corresponding context, thereby enhancing the model’s ability to understand mathematics. The following sections will describe the construction processes for these two different types of mathematical data.

RedStone-Math 	Tokens (B)	Pages (M)	Tokens per Page
HTML-Math	11.1	2.7	4,041.2
ASCII-Math	4.8	3.7	1,297.9
Total	15.9	6.4	2,460.7
Table 4:RedStone-Math from Common Crawl.
2.3.1HTML-Math
Filtering.

The filtering process encompasses document filtering, HTML element filtering, and fine-grained filtering. The workflow is similar to that used for RedStone-Code. Document filtering removes pages that do not contain mathematical formulas by using a coarse heuristic based on HTML tags typically associated with mathematics. However, content wrapped in mathematical tags is not necessarily a valid formula; it could merely be a few mathematical characters or incorrect expressions. Therefore, more refined mathematical formula filtering rules are required to accurately determine whether a page genuinely contains mathematical formulas. The specifics of each step are detailed below.

• 

Document Filtering. We use keywords such as <math, <annotation, ="math, athjax, math-container, class="tex", tex.cgi, latex.php, katex.min.css, \frac, and codecogs to pre-filter the HTML documents associated with mathematics.

• 

Refined Math Filtering. To verify the syntactical correctness of formulas, it is essential first to identify their positions for extraction. Consequently, this process is divided into two steps: formula localization and formula syntax verification.

– 

To identify mathematical formulas in HTML documents, we use HTML tags and related attributes. For instance, we utilize tags such as <script> and <math>. For LaTeX formulas, we use specific prefixes like <script type="math/tex", <script type="math/latex", <script type="math/asciimath", <span class="math-formula", and <annotation encoding="application/x-tex" to detect them. For MathML formulas, we apply <math and <script type="math/mml" to identify them.

– 

To ensure the grammatical correctness of each mathematical formula, we apply LatexWalker from the pylatexenc library [22] to check the grammatical correctness of each formula, filtering out documents with errors.

Another potential issue is the possibility of duplicate representations of formulas. For instance, a LaTeX annotation, typically represented as <annotation encoding="application/x-tex", is included as child node within a MathML formula represented as <math>. This could result in multiple duplicates of the same formula. Therefore, we need to ensure that each mathematical formula is represented only once, eliminating any duplicate representations, such as formula text along with formula annotations, formula images, and alternative text for images.

Extraction.

The Extraction module follows the same processing procedure as RedStone-Code, which will not be reiterated here.

2.3.2ASCII-Math

During the processing, we observed that many mathematical formulas were directly represented in ASCII math format. These formulas were not enclosed within any math-related HTML tags but existed as plain text within the pages. This type of data is also highly valuable. For this type of data, we employed the following pipeline.

Extraction.

Since the mathematical formulas are no longer encapsulated within HTML tags, we can convert HTML pages into plain text format in advance to facilitate subsequent filtering. Unlike the text extraction process described for HTML tags, all HTML documents in this step are converted into text documents using the Trafilatura, excluding menu, header, and footer content, resulting in cleaner text.

Filtering.

To efficiently extract pages containing mathematical formulas from a vast collection of raw pages, we employ a two-stage filtering approach. Initially, we use rule-based heuristics to quickly and broadly exclude pages without mathematical content. Subsequently, we apply a model for finer-grained filtering on the remaining pages, generating the final dataset.

• 

Rule-based Filtering. We categorize the keywords into two groups: LaTeX symbols and non-LaTeX symbols. For LaTeX symbols, such as \frac, \mu, \dot, \log, and \eq, we have collected over 3,000 keywords in total. For non-LaTeX symbols, such as sqrt, sum, log, +, *, and $, we have gathered more than 20 keywords. Based on these keywords, we filter out text documents that contain fewer than five of them. At this stage, only about 0.1% of the text documents pass the filter and proceed to the next stage for further processing.

• 

Model-based Filtering. After the pre-filtering stage by keywords, we further filter out the text documents by employing a self-trained classification model. Firstly, we construct the training dataset by sampling 100k text documents from the pages previously identified as containing mathematical formulas using HTML tags, and also randomly selecting 100k text documents that exclude the math text documents as negative samples, and totally 200k samples. Secondly, we train an n-gram model using fasttext [29] to balance efficacy and efficiency. During the filtering stage, we apply this self-trained model to further filter the text documents, using a threshold of 0.5.

2.4RedStone-QA

QA datasets often contain a wealth of specific knowledge and are highly valuable for advancing understanding in various fields. To answer these questions, LLMs need to have a comprehensive and accurate understanding of the world, requiring critical thinking, analysis, and discussion. We have created two types of QA datasets: open question answering and multiple-choice question answering.

For the open question answering data, there are various types, such as short answer, fill-in-the-blank, multiple choice, and true-or-false questions. These open questions are often interspersed with other text content in the document, which may include reading passages, explanations, or other informative content. Therefore, the contextual information along with the open questions is essential for a language model to effectively understand and learn.

For the multiple-choice question answering data, during the construction of open question answering data, we observed a significant number of high-quality, valuable multiple-choice questions within the Common Crawl. A considerable proportion of these multiple-choice questions include the stem, options, answers, and explanations. The presence of answers and explanations imbues this data with a high density of knowledge, facilitating a deeper understanding of the real world for the model. Therefore, we further processed the open question answering data to obtain multiple-choice question answering data.

RedStone-QA 	Tokens (B)	Pages (M)	Tokens per Page
Open Question Aswering	51.3	42.5	1,206.8
Multi-choice Question Aswering	0.1	1.6	63.8
Total	51.4	44.1	1165.5
Table 5:RedStone-QA from Common Crawl.

The final dataset of RedStone-QA consists of 44.1 million documents and 51.4 billion tokens, as shown in Table 5. The following sections will describe the data construction process in detail.

2.4.1Open Question Aswering

In processing pages that contain open questions, we utilized the extraction and filtering modules. The extraction module is responsible for extracting main content from the pages, while the filtering module is tasked with identifying and selecting the pages that contain open questions. The specific steps are as follows.

Extraction.

The first step is to extract the main content of the original pages. Due to the low recall when extracting main content from WARC files using Trafilatura, which often results in missing parts of the main content, we use WET files as input for creating the RedStone-OpenQA. To ensure the completeness of the extracted content, we employed CCNet’s paragraph-level exact deduplication method. This extraction procedure is consistent with the one described earlier and can be referenced in Section 2.1.1.

Filtering.

To quickly and efficiently identify pages containing open-domain questions, our filtering module employs a two-stage approach: rule-based filtering and model-based filtering. Initially, we use rule-based filtering to identify common patterns of open-domain questions, thereby discarding pages that are unlikely to contain such questions. This step significantly reduces the number of pages. Subsequently, we apply model-based filtering to the remaining pages to detect the presence of open-domain questions.

• 

Rule-based Filtering. Since the answers contain the knowledge for each question, We filter text documents to ensure they include both questions and answers by designing two sets of patterns for identifying each. For the question pattern, we use keywords such as ‘‘what’’, ‘‘where’’, ‘‘why’’, ‘‘when’’, ‘‘who’’, ‘‘whose’’, ‘‘how’’, ‘‘q&a’’, ‘‘q & a’’, ‘‘q:’’, ‘‘que:’’, ‘‘question:’’, ‘‘quiz:’’, ‘‘exam:’’, ‘‘examination:’’, ‘‘probe:’’, ‘‘request:’’, ‘‘challenge:’’, ‘‘test:’’, ‘‘query:’’, and ‘‘survey:’’. The document must contain at least one of these keywords. For the answer pattern, we utilize keywords such as ‘‘q&a’’, ‘‘q & a’’, ‘‘a:’’, ‘‘ans:’’, ‘‘answer:’’, ‘‘solution:’’, ‘‘reply:’’, ‘‘response:’’, ‘‘result:’’, ‘‘outcome:’’, ‘‘explanation:’’, ‘‘conclusion:’’, ‘‘finding:’’, ‘‘assertion:’’, ‘‘statement:’’, and ‘‘clarification:’’. The document must also contain at least one of these keywords. Only text documents that match both patterns are retained for further filtering.

• 

Model-based Filtering. Rule-based filtering serves only as an initial pre-processing step. To further refine the text documents and ensure they contain open questions, we employ a self-trained classification model. Specifically, (1) we use GPT-3.5-turbo as an annotator to label whether a page contains open questions and to categorize them (e.g., short answer, fill-in-the-blank, multiple choice, etc.), and then constructed a training set of 200k labeled samples, with an equal split between positive and negative samples. Among the positive samples, we ensured a balanced mix of different types of open-domain questions. (2) We then trained an n-gram-based classification model using fastText [29] to determine whether a page contains open-domain questions. (3) Finally, we used the trained model to filter data at scale, thereby obtaining a large dataset of open-domain questions.

2.4.2Multi-choice Question Aswering

The open question aswering data contains pages with various types of open questions, which need to be further filtered to obtain pages containing multiple-choice questions. For multiple-choice questions, we only extract content related to the questions, such as the question stems, answers, and explanations, and finally standardize this content into a uniform format.

We utilize two modules in the RedStone: Filtering and Extraction. The Filtering module identifies pages containing multiple-choice questions, while the Extraction module extracts the multiple-choice questions from these pages.

Filtering.

Among the components of multiple-choice questions, the list of options is more distinctly characterized and thus easier to locate. Consequently, to efficiently filter out pages containing multiple-choice questions from the vast number of web pages, we employed the following two rules:

• 

The serial number is typically in one of the following formats: a, b, c, d, …, 1, 2, 3, 4, …, and i, ii, iii, iv, ….

• 

The delimiter can be one of the following: ., -, ), 
>
, ], and \space.

Additionally, we ensure that multiple-choice questions have corresponding answers by retaining only those text documents that contain one of the following keywords: answer:, solution:, reply:, response:, ans:, a:, and r:. By applying these rules, we can quickly filter and identify a large number of pages that potentially contain multiple choice questions.

Extraction.

We aim to obtain a clean dataset containing only multiple choice questions. Therefore, from the pages identified by the filtering module as potentially containing multiple choice questions, we further locate and extract the questions from the pages. It is important to note that multiple choice questions without answers are useless for model training, so we only extract those that include answers, explanations for the answers are optional.

• 

The serial number is typically in one of the following formats: a, b, c, d, …, 1, 2, 3, 4, …, and i, ii, iii, iv, ….

• 

The delimiter can be one of the following: ., -, ), 
>
, ], and \space.

To locate the multiple choice questions on a page, we use different rules to identify each component: the stem, the options, the answer, and the explanation. A multiple choice question is retained only if it contains at least the stem, options, and answer. we employed the following rules:

• 

Choice List Identification: Locate the candidate of the choice list for each multiple choice question using the rules concluded in the filtering stage.

• 

Stem Identification: The stem usually precedes the choice list, so the textline before the options is selected as the question stem.

• 

Answer Identification: If the text line following each candidate in the choice list starts with any of the following keywords: answer:, solution:, reply:, response:, ans:, a:, or r:, we treat this candidate as a genuine choice list. The text line following the choice list is added as the answer, while the text line preceding it is considered the question.

• 

Explanation Identification: If the text line following the answer starts with explanation:, it is added as the explanation component as well.

Given the various formats of multiple choice questions on the web—for instance, the answer might precede the stem—the order of the components is not fixed. Therefore, after extracting the multiple choice questions, we standardize each component to ensure uniformity. For example, we arrange each multiple choice question in the order of question, option list, answer, and explanation. We prefix the answer with Answer: and, if an explanation is present, we prefix it with Explanation:. The steps are as follows:

• 

Question Formatting: Remove any serial number and delimiter at the start of the question.

• 

Choice List Formatting: Convert the serial numbers of the choice list to A, B, C, D, ... in order and use . as the delimiter between the serial number and choice.

• 

Answer and Explanation Formatting: Precede the answer with Answer: and the explanation, if it exists, with Explanation:.

• 

Component Arrangement: Arrange each multiple choice question in the order of the question, choice list, answer, and explanation.

3Experiments
3.1Settings

Model Setting. For evaluations on general domain datasets, we trained models with 1.3 billion parameters on 50 billion tokens. We utilized the same architecture as the LLaMA model, incorporating SwiGLU [58], RoPE [56], and RMS [66]. For evaluations on the domain-specific dataset, We utilized the same architecture as the StableLM-2-1.6B[6] model. As described in Table 6.

Hyperparameters	General Domain	Domain-specific
Layers	24	32
Embed Dim	2,048	2,560
FFN Dim	5,760	6,912
Attn Head	32	32
Vocab Size	32k	50k
Table 6:Hyperparameters of model

Training Setting. For evaluations on general domain datasets, We follow the GPT-3 paper to set the training parameters. For evaluations on the domain-specific dataset, The learning rate is 2e-5, total training steps is 10k, batch size is 16, max token size is 2k. Further details can be found in Table 7.

Hyperparameters	General Domain	Domain-Specific
Training steps	300,000	10,000
Warmup steps	1,000	1,000
Optimizer	AdamW
Learning rate	2e-4	2e-5
Learning rate decay	Linear
Adam 
𝛽
 	(0.9, 0.95)
Weight decay	0.1
Dropout	0
Batch size of text	512	16
#Token per Batch	1M	32k
Table 7:Hyperparameters of training
3.2Evaluation Datasets

To comprehensively evaluate our proposed datasets, we employed a two-pronged approach, utilizing different benchmarks for general domain and domain-specific datasets.

Tasks	Type
HellaSwag [65] 	Common Sense
Winogrande [57] 	Common Sense
PIQA [7] 	Common Sense
ARC-E [8] 	Question Answering
ARC-C [8] 	Question Answering
OpenBookQA [43] 	Question Answering
Table 8:Evaluation tasks of general domain data

For the general domain dataset, we leveraged the lm-evaluation-harness pipeline2, which includes a variety of tasks that test reading comprehension, common sense reasoning, and question answering abilities. The benchmarks used for this evaluation are listed in Table 8.

Tasks	Type
HumanEval [16] 	Code Synthesis
MBPP [2] 	Code Synthesis
GSM8k [12] 	Mathematics
Minerva Math [19] 	Mathematics
MMLU [27, 26] 	Common Sense
Winogrande [57] 	Common Sense
ARC-C [8] 	Question Answering
ARC-E [8] 	Question Answering
OpenBookQA [43] 	Question Answering
Table 9:Evaluation tasks of domain-specific data

For the domain-specific datasets, we selected benchmarks that are tailored to assess the performance of models in code generation, mathematical reasoning, and question-answering tasks. These benchmarks are detailed in Table 9.

3.3RedStone-Web
Datasets	ARC-C	ARC-E	HellaSwag	OpenBookQA	PIQA	Winogrande	AVERAGE
RedPajama	0.2270	0.4386	0.3171	0.1900	0.5968	0.5296	0.3832
FineWeb	0.1928	0.4428	0.3506	0.1740	0.6681	0.5288	0.3929
RefinedWeb	0.2125	0.4369	0.3380	0.2100	0.6491	0.5264	0.3955
DCLM	0.2159	0.4848	0.3614	0.1760	0.6615	0.5082	0.4013
FineWeb-Edu	0.2722	0.5648	0.3637	0.1940	0.6676	0.5051	0.4279
RedStone-Web 	0.2662	0.5181	0.3722	0.2340	0.6795	0.5162	0.4310
Table 10: Comparison of evaluation tasks across open source datasets.

We evaluated the performance of our general domain dataset, RedStone-Web, across a variety of common sense reasoning tasks and compared it with several other well-known open-source datasets. The results of these evaluations are summarized in Table 10. The results demonstrate that RedStone-Web performs exceptionally well in most tasks, with particularly strong performance in the ARC-E, HellaSwag, OpenBookQA, and PIQA tasks. Specifically, RedStone-Web achieved the highest scores in HellaSwag (0.3722), OpenBookQA (0.2340), and PIQA (0.6795), indicating that it is highly effective in capturing the nuances required for these tasks.

The comprehensive evaluation across these tasks reveals that RedStone-Web is a highly competitive dataset for general pre-training. It surpasses the other datasets in most common sense reasoning tasks. This indicates that our approach of leveraging Common Crawl to extract a diverse and high-quality dataset is effective in enhancing the capabilities of LLMs.

3.4RedStone-Code
Datasets	HumanEval
pass@1	HumanEval
pass@10	MBPP
pass@1	MBPP
pass@10
RedStone-Web 	0.0125	0.0168	0.0751	0.1566
     + RedStone-Code 	0.0555	0.1035	0.1311	0.2458
Table 11:Evaluation of RedStone-Code. ‘‘+’’ indicates the combination of the current dataset with the previous one. In the experiments, the weight of the RedStone-Web data is set to 0.8, and the weight of the RedStone-Code data is set to 0.2.

The results in Table 11 demonstrate the significant performance improvement achieved by incorporating the RedStone-Code dataset into the RedStone-Web dataset. For HumanEval pass@1, the combination yields a score of 0.0555, a substantial increase from the 0.0125 achieved by RedStone-Web alone. Similarly, HumanEval pass@10 improves from 0.0168 to 0.1035. This indicates a marked improvement in the model’s ability to generate correct solutions within the first attempt as well as within ten attempts. A similar trend is observed in the MBPP dataset, where the pass rates also exhibit considerable advancements. The pass@1 metric rises from 0.0751 to 0.1311, and the pass@10 metric increases from 0.1566 to 0.2458.

These experimental results indicate that RedStone-Code can serve as a valuable supplement to LLM pre-training datasets in the domain of code. This integration lays a strong foundation for enhancing the code generation capabilities of LLMs in downstream tasks.

3.5RedStone-Math
Datasets	GSM8k	MATH
OpenWebMath [51] 	3.2503	3.1288
RedStone-Math 	3.1125	3.0557
Table 12:Evaluation of RedStone-Math and perplexity measurement on various mathematics benchmarks. Both OpenWebMath and RedStone-Math were trained from scratch.

Table 12 presents a comparative evaluation of RedStone-Math and OpenWebMath on two mathematics benchmarks, GSM8k and MATH. The evaluation metric used is perplexity, which scales more smoothly than accuracies.

From the results, it is evident that RedStone-Math outperforms OpenWebMath on both datasets. Specifically, RedStone-Math achieves a perplexity of 3.1125 on the GSM8k dataset, compared to OpenWebMath’s perplexity of 3.2503. This represents a significant improvement, suggesting that RedStone-Math enables faster convergence and enhances modeling capability for the types of problems presented in GSM8k. Similarly, on the MATH dataset, RedStone-Math demonstrates a perplexity of 3.0557, whereas OpenWebMath records a perplexity of 3.1288. This further underscores the effectiveness of RedStone-Math in handling diverse mathematical problems.

RedStone-Math consistently achieves lower perplexity scores across both evaluated datasets, highlighting its potential to enhance model performance in the domain of mathematics. It can serve as a valuable supplement to pre-training datasets in this field.

3.6RedStone-OpenQA
Model	MMLU	ARC-Challenge	ARC Easy	Openbookqa	Winogrande	AVERAGE
StableLM-2-1.6B	0.3135	0.3481	0.6860	0.2780	0.6354	0.4522
     + FALN v2	0.3525	0.3601	0.6406	0.2860	0.6125	0.4503
     + Open Orca	0.3569	0.3089	0.5821	0.2660	0.5675	0.4163
     + RedStone-QA 	0.4582	0.3643	0.6839	0.2760	0.6377	0.4840
Table 13:Evaluation of RedStone-QA: RedStone-QA comprises two components—open question answering and multiple-choice question answering, with training weights set to 0.8 and 0.2, respectively.

Table 13 presents the performance evaluation of various models across multiple benchmark datasets, including MMLU, ARC Challenge, ARC Easy, OpenbookQA, and Winogrande. The baseline model, StableLM-2-1.6B, demonstrates robust performance with an average accuracy of 0.4522. After fine-tuning with FALN v2, the model shows slight improvements on the MMLU and ARC Challenge tasks, although the overall average performance slightly decreases from 0.4522 to 0.4503. Fine-tuning with Open Orca maintains competitive performance on the MMLU task but leads to significant drops in scores on the ARC Easy and OpenbookQA tasks, from 0.6860 to 0.5821 and from 0.2780 to 0.2660, respectively. The average performance decreases to 0.4163, indicating that this enhancement may not generalize well across different tasks.

The proposed RedStone-QA dataset achieves the highest scores on most datasets, with an overall average performance significantly improving to 0.4840. Notably, it shows a 14.47% improvement on the MMLU dataset, highlighting the effectiveness of RedStone-QA in enhancing the model’s question-answering capabilities.

4Related Work
4.1General Domain Data Pipelines

Since the advent of BERT [18], there has been a growing trend to leverage large-scale data from Common Crawl for model training. CCNet [60] extracted a substantial amount of high-quality bilingual text from it to build machine translation models. T5 [53] employed a series of heuristic rules to extract text and create the C4 dataset. With the rapid development of LLMs, recent efforts have increasingly focused on constructing larger and higher-quality datasets from Common Crawl to support the training of these models. The Pile [24] used jusText [21] to extract text from Common Crawl, resulting in Pile-CC. LLaMA [59] utilized the CCNet pipeline with modifications to generate a vast amount of pre-training data, though this data was not made publicly available. Subsequently, RedPajama [14] reproduced the training data used in LLaMA and open-sourced the data. Aiming for even higher data quality, RedPajama v2 [14] introduced 46 quality signals to describe data characteristics from various dimensions. RefinedWeb [50] employed content extraction tools to extract main content from HTML pages provided by Common Crawl, yielding cleaner and higher-quality text, though only a small portion of this data was open-sourced. In response, FineWeb [49] reproduced RefinedWeb and open-sourced the data, while also constructing a new filter to exclude educational content, resulting in the higher-quality pre-training dataset FineWeb-edu. DCLM [38] extracted a large volume of text from Common Crawl and developed a custom filter to obtain a substantial amount of instruction-formatted data, significantly enhancing data quality. We also introduce RedStone-Web, a high-quality dataset derived from our RedStone pipeline, which features simpler processing steps, more data, and improved quality.

4.2Domain Specific Data Pipelines

LLMs are currently being widely utilized to address a variety of problems. Among the most prevalent applications are their roles as powerful assistants for writing and editing code, as well as for solving mathematical problems. Additionally, LLMs are frequently engaged in interactive QA formats, making QA capabilities a critical aspect of their evaluation. Given the extensive range of potential applications for LLMs, this paper focuses on advancements in code, math, and QA, which are also key components of RedStone.

Code. Recent advancements in code generation have been significantly driven by Code LLMs. Prominent models such as CodeGen  [45], ERNIE-Code  [17], StarCoder  [40], CodeT5+  [61], CodeLLaMa  [52], and Deepseek-Coder  [25] focus on enhancing coding capabilities during the pre-training stage. Given the substantial data requirements for pre-training, GitHub has emerged as a crucial resource  [37, 41, 46, 23]. For instance, The Stack utilized GHArchive to download 137.36 million repositories from GitHub. After filtering, cleaning, license detection, and deduplication, a training set of 1450.75 GB of data spanning 30 popular programming languages was obtained  [30]. Subsequently, The Stack v2 expanded its data sources to include other high-quality open datasets, such as GitHub issues, pull requests, Kaggle and Jupyter notebooks, code documentation, and other natural language datasets related to math, coding, and reasoning. This expansion resulted in a dataset four times larger than the initial StarCoder dataset. Additionally, models like Code Alpaca  [10], WizardCoder  [42], OctoPack  [44], and Magicoder  [62] focus on enhancing coding capabilities during the instruction tuning stage. This stage typically involves constructing high-quality instruction datasets to improve model performance.

Math. The application of LLMs in solving mathematical problems has seen significant advances, driven by the necessity to manage complex computations and deliver precise solutions. Models such as GPT-3  [4] and Minerva  [32] have been specifically fine-tuned for mathematical problem-solving tasks. These models utilize extensive datasets comprising mathematical texts, problem sets, and solutions to enhance their capabilities. For instance, the OpenWebMath dataset  [51], which includes 14.7 billion tokens of mathematical web pages from Common Crawl, demonstrates the potential of training models on large-scale mathematical data. The training process often involves step-by-step problem-solving, aiding in the understanding and generation of solutions for complex mathematical queries. Furthermore, incorporating symbolic computation and formal methods into LLMs has enhanced their ability to handle algebraic manipulations, calculus, and other advanced mathematical concepts. The integration of these techniques ensures that the models provide not only accurate answers but also human-readable solutions that are easy to follow and verify.

Question and Answer. Interactive Question and Answer (QA) capabilities are a cornerstone of LLM applications. Models like BERT  [18], T5  [53], and GPT-3  [4] have set benchmarks in the QA domain by excelling in tasks such as reading comprehension, open-domain QA, and conversational agents. The performance of these models is often evaluated on standardized datasets like SQuAD  [55], TriviaQA  [28], and Natural Questions  [31]. Additionally, the Massive Multitask Language Understanding (MMLU) dataset  [27] has emerged as a critical benchmark for evaluating the broad knowledge and reasoning abilities of LLMs across diverse subjects, ranging from elementary mathematics to advanced science and humanities. Recent models, such as ChatGPT  [47] and InstructGPT  [48], build on these foundations by incorporating reinforcement learning from human feedback (RLHF) to improve their interactive and conversational abilities. Specialized models like QA-GNN  [64] and DrQA  [9] enhance QA performance by integrating graph neural networks and retrieval-based approaches, respectively. These advancements enable LLMs to provide more accurate, context-aware, and informative answers, making them indispensable tools for customer support, educational tutoring, and information retrieval systems. The continuous refinement of QA datasets and the development of hybrid models that combine retrieval and generative approaches are driving the next generation of QA systems.

5Conclusion and Future Work

This paper introduces RedStone, a comprehensive data pipeline designed to create specialized large-scale datasets by leveraging the vast and diverse data from Common Crawl. The RedStone pipeline is built around Extraction and Filtering modules. These modules can be flexibly combined to efficiently mine data from various domains. We used RedStone to construct large-scale datasets, totaling 3.48 trillion tokens, for RedStone-Web, RedStone-Code, RedStone-Math, and RedStone-QA. The experimental results demonstrate the effectiveness of RedStone. This dataset not only showcases the pipeline’s ability to extract specific types of data but also significantly surpasses existing open-source datasets in terms of quality and scale. We have detailed the dataset construction process to ensure reproducibility and transparency, providing a valuable resource for developing competitive LLMs. This makes it possible to create high-quality, domain-specific datasets at scale. The datasets and code developed through RedStone will be open-sourced to foster further development and collaborative efforts within the research community.

For future work, several promising directions could enhance RedStone’s capabilities. First, integrating more advanced filtering techniques, such as leveraging multimodal signals (e.g., visual or contextual cues) and sophisticated machine learning models, could refine data quality and improve domain diversity. Second, extending the pipeline to support multilingual datasets and incorporating multimodal content, such as images, audio, and videos, would enable the development of LLMs that excel in cross-lingual and multimodal tasks. Lastly, implementing real-time data updates, including automated mechanisms to incorporate fresh and relevant web data while maintaining stringent quality standards, would ensure the datasets remain current and applicable to evolving use cases. Together, these efforts will unlock the full potential of large-scale web data, advancing the creation of specialized and high-performance language models across diverse domains.

References
AI@ [24]	AI@Meta.Llama 3 model card.2024.
AON+ [21]	Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.Program synthesis with large language models, 2021.
Bar [21]	Adrien Barbaresi.Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction.In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122--131. Association for Computational Linguistics, 2021.
BMR+ [20]	Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.Advances in neural information processing systems, 33:1877--1901, 2020.
Bro [97]	Andrei Z Broder.On the resemblance and containment of documents.In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 21--29. IEEE, 1997.
BTM+ [24]	Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al.Stable lm 2 1.6 b technical report.arXiv preprint arXiv:2402.17834, 2024.
BZB+ [20]	Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.Piqa: Reasoning about physical commonsense in natural language.In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.
CCE+ [18]	Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.Think you have solved question answering? try arc, the AI2 reasoning challenge.CoRR, abs/1803.05457, 2018.
CFWB [17]	Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.Reading Wikipedia to answer open-domain questions.In Association for Computational Linguistics (ACL), 2017.
Cha [23]	Sahil Chaudhary.Code alpaca: An instruction-following llama model for code generation.https://github.com/sahil280114/codealpaca, 2023.
CIJ+ [22]	Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.Quantifying memorization across neural language models.arXiv preprint arXiv:2202.07646, 2022.
CKB+ [21]	Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168, 2021.
CND+ [23]	Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.Palm: Scaling language modeling with pathways.Journal of Machine Learning Research, 24(240):1--113, 2023.
Com [23]	Together Computer.Redpajama: an open dataset for training large language models, 2023.
Cra [23]	Common Crawl.Web archiving tools on hadoop, 2023.
CTJ+ [21]	Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.Evaluating large language models trained on code.2021.
CWP+ [22]	Yekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu.Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages.arXiv preprint arXiv:2212.06742, 2022.
DCLT [18]	Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.Bert: Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018.
DGA [22]	Ethan Dyer and Guy Gur-Ari.Minerva: Solving quantitative reasoning problems with language models.June, 30:2022, 2022.
DJP+ [24]	Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.
EN [13]	István Endrédy and Attila Novák.More effective boilerplate removal-the goldminer algorithm.Polibits, (48):79--83, 2013.
Fai [23]	Philippe Faist.Simple latex parser providing latex-to-unicode and unicode-to-latex conversion, 2023.
FAL+ [22]	Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.Incoder: A generative model for code infilling and synthesis.arXiv preprint arXiv:2204.05999, 2022.
GBB+ [20]	Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.The pile: An 800gb dataset of diverse text for language modeling.arXiv preprint arXiv:2101.00027, 2020.
GZY+ [24]	Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al.Deepseek-coder: When the large language model meets programming--the rise of code intelligence.arXiv preprint arXiv:2401.14196, 2024.
[26]	Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.Aligning ai with shared human values.Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[27]	Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding.Proceedings of the International Conference on Learning Representations (ICLR), 2021.
JCWZ [17]	Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.arXiv preprint arXiv:1705.03551, 2017.
JGB+ [16]	Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov.Fasttext.zip: Compressing text classification models.arXiv preprint arXiv:1612.03651, 2016.
KLA+ [22]	Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al.The stack: 3 tb of permissively licensed source code.arXiv preprint arXiv:2211.15533, 2022.
KPR+ [19]	Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.Natural questions: a benchmark for question answering research.Transactions of the Association for Computational Linguistics, 7:453--466, 2019.
LAD+ [22]	Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.Solving quantitative reasoning problems with language models.Advances in Neural Information Processing Systems, 35:3843--3857, 2022.
LAZ+ [23]	Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.Starcoder: may the source be with you!arXiv preprint arXiv:2305.06161, 2023.
LBAvWW [24]	Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf.Fineweb-edu, May 2024.
[35]	Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.Textbooks are all you need ii: phi-1.5 technical report.arXiv preprint arXiv:2309.05463, 2023.
[36]	Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.Textbooks are all you need ii: phi-1.5 technical report.arXiv preprint arXiv:2309.05463, 2023.
LCC+ [22]	Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al.Competition-level code generation with alphacode.Science, 378(6624):1092--1097, 2022.
LFS+ [24]	Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al.Datacomp-lm: In search of the next generation of training sets for language models.arXiv preprint arXiv:2406.11794, 2024.
LIN+ [21]	Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.Deduplicating training data makes language models better.arXiv preprint arXiv:2107.06499, 2021.
LLA+ [24]	Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al.Starcoder 2 and the stack v2: The next generation.arXiv preprint arXiv:2402.19173, 2024.
LSW+ [22]	Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al.The bigscience roots corpus: A 1.6 tb composite multilingual dataset.Advances in Neural Information Processing Systems, 35:31809--31826, 2022.
LXZ+ [23]	Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.Wizardcoder: Empowering code large language models with evol-instruct.arXiv preprint arXiv:2306.08568, 2023.
MCKS [18]	Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.Can a suit of armor conduct electricity? a new dataset for open book question answering.In EMNLP, 2018.
MLZ+ [23]	Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre.Octopack: Instruction tuning code large language models.arXiv preprint arXiv:2308.07124, 2023.
[45]	Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.Codegen: An open large language model for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474, 2022.
[46]	Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.A conversational paradigm for program synthesis.arXiv preprint arXiv:2203.13474, 30, 2022.
Ope [23]	OpenAI.Chatgpt: Chatbot by openai.https://www.openai.com/chatgpt, 2023.
OWJ+ [22]	Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.Training language models to follow instructions with human feedback.Advances in neural information processing systems, 35:27730--27744, 2022.
PKvWW [24]	Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, and Thomas Wolf.Fineweb, 2024.
PMH+ [23]	Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.arXiv preprint arXiv:2306.01116, 2023.
PSAB [23]	Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba.Openwebmath: An open dataset of high-quality mathematical web text.arXiv preprint arXiv:2310.06786, 2023.
RGG+ [23]	Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al.Code llama: Open foundation models for code.arXiv preprint arXiv:2308.12950, 2023.
RSR+ [20]	Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of machine learning research, 21(140):1--67, 2020.
RWC+ [19]	Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.
RZLL [16]	Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.Squad: 100,000+ questions for machine comprehension of text.arXiv preprint arXiv:1606.05250, 2016.
SAL+ [24]	Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024.
SBBC [19]	Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.Winogrande: An adversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019.
Sha [20]	Noam Shazeer.Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.
TLI+ [23]	Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023.
WLC+ [19]	Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.Ccnet: Extracting high quality monolingual datasets from web crawl data.arXiv preprint arXiv:1911.00359, 2019.
WWJH [21]	Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi.Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.arXiv preprint arXiv:2109.00859, 2021.
WWL+ [23]	Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.Magicoder: Source code is all you need.arXiv preprint arXiv:2312.02120, 2023.
XSZ+ [23]	Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.Wizardlm: Empowering large language models to follow complex instructions.arXiv preprint arXiv:2304.12244, 2023.
YRB+ [21]	Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.Qa-gnn: Reasoning with language models and knowledge graphs for question answering.arXiv preprint arXiv:2104.06378, 2021.
ZHB+ [19]	Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.HellaSwag: Can a machine really finish your sentence?In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791--4800, Florence, Italy, July 2019. Association for Computational Linguistics.
ZS [19]	Biao Zhang and Rico Sennrich.Root mean square layer normalization.Advances in Neural Information Processing Systems, 32, 2019.
Appendix AData Samples
Generated on Wed Dec 4 15:25:24 2024 by LaTeXML

## Metadata

```json
{
  "title": "RedStone: Curating General, Code, Math, and QA Data for Large Language Models",
  "description": "",
  "url": "https://arxiv.org/html/2412.03398v1#S2",
  "content": "1Introduction\n2RedStone\n3Experiments\n4Related Work\n5Conclusion and Future Work\n\\pdfcolInitStack\n\ntcb@breakable \\newmdenv[ font=, linewidth=0.5pt, innerleftmargin=10pt, innerrightmargin=10pt, innertopmargin=10pt, innerbottommargin=10pt, ]monobox\n\nRedStone\n: Curating General, Code, Math, and QA Data for Large Language Models\nYaoyao CHANG, Lei CUI, Li DONG, Shaohan HUANG, Yangyu HUANG,\nYupan HUANG, Scarlett LI, Tengchao LV, Shuming MA, Qinzheng SUN,\nWenhui WANG, Furu WEI, Ying XIN, Mao YANG, Qiufeng YIN, Xingxing ZHANG.\n\nMicrosoft Research\nhttps://aka.ms/GeneralAI\n\nProject Page: https://aka.ms/redstone\nThe contributors are listed in the alphabetical order by last names. Corresponding author: Furu WEI (fuwei@microsoft.com.)\nAbstract\n\nPre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge. We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets. Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains. In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks. The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets. Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs. This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs. RedStone code and data samples will be publicly available at https://aka.ms/redstone.\n\n1Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable potential as highly capable AI assistants, particularly in complex reasoning tasks that require expert knowledge across diverse fields [54, 4, 59, 20]. The recent advancements in LLMs have been driven not only by increasing model sizes but also by scaling up dataset sizes correspondingly. These advancements have created a demand for robust data curation pipelines capable of mining vast, diverse datasets across various domains [35, 4, 13, 59, 20].\n\nIn addition to general domain data that spans a wide range of topics, there is an increasing need for domain-specific data tailored to specialized fields such as mathematics and code. These domains require high-quality, domain-specific knowledge to enhance model performance. Current approaches for constructing domain-specific datasets often rely on proprietary resources [30], synthetic LLM-generated data [63, 32], or manually annotated data [4, 1, 36], which is often time-consuming and labor-intensive. Moreover, these sources are limited in scope and scale, often resulting in small dataset sizes. Common Crawl, a massive open-access web archive, offers a wealth of diverse, high-quality data across various domains. Although recent efforts, such as RefinedWeb [50] and Redpajama v2 [14], have explored mining data from Common Crawl, their focus has primarily been on general data extraction, with limited attention to domain-specific data.\n\nIn this paper, we introduce RedStone, a comprehensive pipeline designed to efficiently extract and filter large-scale datasets from Common Crawl, applicable to both general domain data and domain-specific data. As shown in Figure 1, we propose using Common Crawl as a valuable source for mining knowledge, as its web data often includes annotations that provide extensive context and reasoning. For instance, code snippets and mathematics equations in Common Crawl are frequently accompanied by discussions, explanations, and even execution results, offering richer insights than standalone source code and equations. Open-domain question answering is often interleaved with other text content in the document, which may include reading passages, explanations, or other informative content.\n\nThe RedStone pipeline defines general or domain-specific data formats, then extracts and filters relevant data from Common Crawl. The Extraction module processes raw web data using techniques like pattern recognition and NLP to capture the necessary formats for training. The Filtering module refines this data, retaining only the most relevant content through keyword searches, regular expressions, and machine learning models. This approach efficiently generates large-scale datasets, streamlining the process of data sourcing, synthesis, and annotation. For example, in the process of obtaining open question answering data, the extraction module retrieves the main content from web pages, using WET files and CCNet’s paragraph-level deduplication method to ensure completeness. The filtering module then employs a two-stage approach: rule-based filtering identifies common patterns for questions and answers using keywords, while model-based filtering applies a self-trained classifier to further refine the data. This process results in a large, high-quality dataset of open-domain questions, efficiently combining rule-based and machine learning techniques.\n\nWe curate general domain data from two available formats of Common Crawl, resulting in the RedStone-Web dataset, which contains approximately 3.17 trillion tokens of general knowledge. For domain-specific data, we focus on constructing datasets for code, mathematics, and question answering, as these areas are crucial to the core capabilities of large language models. The resulting datasets—RedStone-Code, RedStone-Math, and RedStone-QA—comprise 250.2, 15.9, and 51.4 billion tokens, respectively.\n\nOur constructed general domain data, sourced from diverse web pages, enhances the model’s understanding of language in a broad context, while domain-specific data provides specialized knowledge in fields such as code, mathematics, and question answering. We evaluated RedStone across a range of tasks, demonstrating its effectiveness in enhancing model performance. RedStone-Web outperformed other open-source datasets in common sense reasoning tasks, such as ARC-e, HellaSwag, OpenBookQA, and PIQA. Incorporating RedStone-Code into the general dataset significantly boosted performance in code generation benchmarks, including HumanEval and MBPP. Similarly, RedStone-Math outperformed existing datasets on mathematics benchmarks like GSM8k and MATH, demonstrating improved perplexity. Finally, RedStone-QA achieved the highest scores in question-answering tasks, particularly on the MMLU benchmark, confirming the strength of our pipeline in extracting high-quality data across diverse domains.\n\nOur contributions are summarized as follows:\n\n• \n\nWe introduce RedStone, a pipeline for obtaining large-scale diverse data from the web with a detailed description of the pipeline’s components and workflow for reproducibility.\n\n• \n\nUsing RedStone, we mined and constructed several large-scale datasets totaling 3.48 trillion tokens, encompassing both general domain and domain-specific data. The RedStone-Web dataset contains approximately 3.17 trillion tokens of general knowledge, while RedStone-Code, RedStone-Math, and RedStone-QA consist of 250.2 billion, 15.9 billion, and 51.4 billion tokens, respectively.\n\n• \n\nWe demonstrate the effectiveness of RedStone through extensive evaluations across common sense reasoning, code generation, and mathematics tasks. Our general domain dataset, RedStone-Web, outperforms existing open-source datasets in common sense reasoning benchmarks, while the inclusion of RedStone-Code and RedStone-Math significantly improves model performance in code generation and mathematical problem solving. Additionally, RedStone-QA shows notable advancements in question-answering tasks, further establishing RedStone as a robust dataset for diverse pre-training applications.\n\n• \n\nWe discuss strategies for maximizing the potential of Common Crawl. To the best of our knowledge, this is the first systematic exploration of the full potential of Common Crawl, demonstrating its capability as a rich resource for various domain-specific contents.\n\n2RedStone\n\nRedStone is a pipeline designed for the large-scale extraction of various data from web data consisting of Extraction and Filtering modules as detailed below.\n\n• \n\nExtraction. Extracting raw data to obtain the required format for training. Extraction can involve the use of pattern recognition, natural language processing, and other computational methods to obtain the desired information.\n\n• \n\nFiltering. Selecting relevant data and excluding unnecessary or irrelevant information to focus on the most pertinent data for analysis. Filtering techniques can include keyword searches, regular expressions, and machine learning models to ensure that only the most relevant data is retained.\n\nThis section will detail the process by which RedStone employs these two modules to construct various types of data. Additionally, in terms of dataset types, RedStone categorizes data into general domain data and domain-specific data, below are the specific definitions of these two types.\n\n• \n\nGeneral domain data helps the model understand language in a broad context, enhancing its comprehension of the real world. Currently, the primary source of pre-training data for LLMs is general domain data, which is crucial for their basic performance. However, existing pipelines for data processing still have limitations, to improve efficiency, certain steps have been simplified. RedStone integrates current mainstream pipelines, restructures the processing steps, and refines certain stages to produce a high-quality general domain dataset, RedStone-Web.\n\n• \n\nDomain-specific data refers to detailed, context-specific information pertinent to particular fields, tasks, or scenarios, such as mathematics, coding, reasoning, medicine, law, engineering, and finance. This high-quality data can significantly improve model performance but is often time-consuming and labor-intensive to construct. Common Crawl contains a substantial amount of domain-specific data, which can be extracted at scale using specialized pipelines and filtering techniques to enhance quality. Additionally, web data often includes discussions and explanations, which help LLMs gain a deeper understanding of specific knowledge. Taking code, math, and QA as examples, RedStone has constructed large-scale domain-specific datasets from Common Crawl, namely RedStone-Code, RedStone-Math, and RedStone-QA.\n\nFigure 1:Using RedStone, we created two types of data: general domain data and domain-specific data. General domain data comprises RedStone-Web, it does not specify a data domain, allowing the model to learn common knowledge across various domains. Domain-specific data includes RedStone-Code, RedStone-Math, and RedStone-QA, enabling the model to acquire specialized knowledge in particular areas or formats. Each example type features the original webpage screenshot on the left and the corresponding data processed by RedStone on the right.\n\nThe construction processes for the two types of data will be introduced separately. Details on RedStone-Web within general domain data are provided in Section 2.1, while RedStone-Code, RedStone-Math, and RedStone-QA within domain-specific data are described in Section 2.2, 2.3, and 2.4, respectively.\n\n\tDataset\tTokens (B)\nGeneral Domain Data\tRedStone-Web\t3,170.2\nDomain-specific Data\tRedStone-Code\t250.2\nRedStone-Math \t15.9\nRedStone-QA \t51.4\nTable 1:Dataset statistics for general and specific domain data constructed using RedStone.\n2.1RedStone-Web\nFigure 2:Subsequent stages of RedStone-Web. RedStone processes Common Crawl data in separate steps, handling WARC and WET files independently before merging them to increase the token count. Over 99% of the tokens in Common Crawl are removed during processing. Since WARC files are in HTML format and inconvenient for token counting, and WARC and WET files represent different forms of the same data, the token count from WET files is used as the original token count for both formats.\n\nTo construct general domain data, we extract the main content from raw web pages and filter data based on the quality of the pages. As illustrated in Figure 2 and shown in Table 2, through extraction and filtering, we obtained a comprehensive dataset, RedStone-Web, containing approximately 3.17 trillion tokens of general knowledge.\n\nGeneral Domain Data\tTokens (T)\tPages (M)\tTokens per Page\nWET\t1.74\t856\t2,032\nWARC\t1.43\t1,223\t1,169\nTotal\t3.17\t2,079\t1,524\nTable 2:RedStone-Web from Common Crawl.\n2.1.1Extraction\n\nCommon Crawl is available in WARC and WET formats. WARC files contain the raw HTML responses, while WET files are pre-processed to include only plain text. Each record corresponds to a single webpage from a specific URL, representing an independent document or sample.\n\nWe extract general domain data from Common Crawl in both WET and WARC formats, encompassing 93 CommonCrawl snapshots spanning from 2013-20 to 2023-40, resulting in two complementary datasets. Although these formats originate from the same data source and represent different forms of the same dataset, the differences in text extraction between WET and WARC lead to variations in the resulting plain text.\n\nText Extraction of WET.\n\nFor text extraction from WET files, we adapt the deduplication strategy used in CCNet [60] to eliminate noisy text from web pages. Specifically, the deduplication strategy removes frequently appeared common segments (e.g., ’sign in,’ ’follow,’ ’about’) on each page by identifying the duplicates against other paragraph within a shard of files. Instead of limiting the deduplication process to 5GB segments, we expanded the search to one snapshot of the Common Crawl. This comprehensive approach enhances the ability to detect and remove noisy text, resulting in cleaner, higher-quality extracted content.\n\nText Extraction of WARC.\n\nFor text extraction from WARC files, we followed the implementation used by RefinedWeb [50], utilizing the Trafilatura [3]. Trafilatura is designed to extract meaningful content from web pages while excluding irrelevant sections such as menus, headers, and footers. It employs a combination of techniques including HTML parsing, content density analysis, and feature extraction to identify and isolate the main body of text.\n\nOnce the high-density text regions are identified, Trafilatura applies feature extraction techniques to further refine the selection. This includes looking for patterns and structures typical of main content, such as paragraphs, headings, and continuous text blocks, while ignoring sections with numerous links or short, unrelated text fragments.\n\nBy employing these techniques, Trafilatura effectively filters out non-essential parts of the web pages, resulting in cleaner and more relevant text extraction from WARC files. This approach ensures that the extracted dataset is of higher quality, focusing on the core content while minimizing noise.\n\n2.1.2Filtering\n\nCommon Crawl contains a large number of low-quality pages, with RefinedWeb removing nearly 90% of the documents originally in Common Crawl. Therefore, the effectiveness of document quality filtering significantly impacts the final quality of the dataset. High-quality datasets enable faster convergence and better model performance [14, 34, 38]. Consequently, we employed a filtering module to eliminate low-quality texts. This filtering module consists of four stages: language filtering, rule-based filtering, model-based filtering, and deduplication.\n\nLanguage Filtering.\n\nCommon Crawl encompasses nearly all languages globally. We use fastText [29] as the language identification tool to filter out non-English pages with a confidence threshold of 0.5.\n\nRule-based Filtering.\n\nGiven the vast number of original pages, we initially use rule-based filtering to quickly sift through all pages, effectively reducing the subsequent filtering workload.\n\nFor the main content extracted from WET files, we adhere to the filtering rules provided by CCNet, using length as a criterion with a threshold 300 to filter out short texts.\n\nFor the main content extracted from WARC files, we follow the rules used by RefinedWeb, conducting repetition removal, document-level removal, and sentence-level removal. Repetition removal detects whether there is a significant amount of internal repetition within the text (e.g., a sentence repeated multiple times on a page), document-level removal determines whether to discard the current document based on its features, and sentence-level removal assesses whether to discard specific sentences based on their characteristics.\n\nAlthough the same set of processing rules can be applied to both WET and WARC files, we follow each pipeline’s specific rules to filter separately. This makes the data more diverse, allowing the two datasets to complement each other and combine into a large-scale pre-training dataset. The detailed steps for WARC files are as follows.\n\n• \n\nRepetition Removal Rules. Documents that match any of following rules will be discarded.\n\n– \n\nIf the ratio of the number of duplicated sentences to the total sentence count exceeds 0.3;\n\n– \n\nIf the ratio of the count of characters in duplicated sentences to the total character count exceeds 0.2;\n\n– \n\nIf the ratio of the number of duplicated paragraphs to the total sentence count exceeds 0.3;\n\n– \n\nIf the ratio of the count of characters in duplicated paragraphs to the total character count exceeds 0.2;\n\n– \n\nfor each \n𝑛\n∈\n{\n2\n⁢\n…\n⁢\n4\n}\n, if the ratio of the count of characters in most common n-gram to the total character count exceeds threshold, the detail thresholds are given in Table 3.\n\n– \n\nfor each \n𝑛\n∈\n{\n5\n⁢\n…\n⁢\n10\n}\n, if the ratio of the count of characters in duplicated n-grams (each character is only counted once regardless of its occurrence in several overlapping n-grams) to the total character count exceeds threshold, the detail thresholds are given in Table 3.\n\nRule\tThreshold\nmost common 2-gram\t0.20\nmost common 3-gram\t0.18\nmost common 4-gram\t0.16\nduplicated 5-gram\t0.15\nduplicated 6-gram\t0.14\nduplicated 7-gram\t0.13\nduplicated 8-gram\t0.12\nduplicated 9-gram\t0.11\nduplicated 10-gram\t0.10\nTable 3:Thresholds of n-gram character ratio related rules.\n• \n\nDocument-Level Rules. Documents that match any of following rules will be discarded.\n\n– \n\nIf document does not contain between 50 and 100,000 words;\n\n– \n\nIf mean word length is outside the range of 3 to 10 characters;\n\n– \n\nIf the ratio of the number of hash symbols and ellipses to the total word count exceeds 0.1;\n\n– \n\nIf more than 90% of sentences in the document start with bullet point;\n\n– \n\nIf more than 30% of sentences in the document end with an ellipsis;\n\n– \n\nIf more than 20% of words in the document doesn’t contain alphabetic characters;\n\n– \n\nIf a document does not contain at least two stop words, such as the, be, to, of, and, that, have, and with.\n\n• \n\nSentence-Level Rules. Sentences that match any of following rules will be removed, if over 5% words of the document is removed by sentence filters the whole document is discarded.\n\n– \n\nIf over 60% of letters in the sentence is uppercase;\n\n– \n\nIf it only contains numerical characters;\n\n– \n\nIf it is a counter (match regex \\̂d+\\s+[a-zA-Z]+$, e.g., 3 likes);\n\n– \n\nIf it only contains one word;\n\n– \n\nIf matches of following regex could be found in the sentence\n\n* \n\n^sign-in\n\n* \n\nread more...$\n\n* \n\nitems in card\n\nModel-based Filtering.\n\nRule-based filtering can only identify fixed patterns within the text and cannot assess its quality in terms of syntax, content, or overall coherence. We employed a transformer-based classifier for data filtering, following a process similar to FineWeb-edu [34]. We first sampled data pre-filtered by rule-based methods and had GPT-4 annotate 2,000 samples for quality, evenly split between positive and negative. We then trained stablelm-1.6b on this annotated data to filter the entire dataset. The filtering criteria focused on grammar, logic, and most importantly, knowledge—defined as data that enhances the model’s real-world understanding. This approach ensures the model is trained on high-quality, informative data, improving its performance on downstream tasks.\n\nDeduplication.\n\nThe presence of substantial duplicate content on the web necessitates deduplication to reduce the required training steps and increase data diversity [11, 39]. Many studies have demonstrated that deduplication can enhance model performance. For deduplication, we utilized MinHash [5], a popular and efficient technique for detecting duplicate content in large datasets. MinHash approximates the Jaccard similarity between sets, making it suitable for identifying near-duplicate texts without the need for exhaustive pairwise comparisons, which can be computationally prohibitive for massive datasets.\n\nWe employ the MinHash-LSH algorithm to extend the deduplication scope across all snapshots. We generate 117 64-bit MinHash signatures (effectively 62-bit due to the limitations of the Meson prime) over 5-gram sequences of documents. These signatures are divided into 9 bands, each containing 13 values, to achieve an estimated 80% Jaccard similarity when an LSH collision occurs.\n\nTo reduce computational complexity and enable a broader processing range and faster iteration speed, we opted against using larger MinHash parameters or connected-graph-based deduplication methods.\n\n2.2RedStone-Code\n\nCode is a significant domain for LLMs, particularly in areas such as code generation, code summarization, and code refinement. Leveraging capabilities across different areas, various downstream applications have been developed to improve code quality, accelerate code writing, and fix code bugs. For example, GitHub Copilot1 has been adopted by more than 50,000 organizations and is favored by approximately 55% of developers.\n\nTo advance the development of code-related capabilities, several initiatives have focused on creating comprehensive code datasets. One notable example is StarCoder [33], which aggregates code data from the BigCode community. However, these datasets typically consist solely of source code and lack the contextual information or explanations accompanying the code snippets. Unlike previous efforts, our goal is to construct an interleaved code dataset using RedStone that includes both source code and the interleaved text providing context or explanations for the code to enhance understanding and usability.\n\nThe original 89 snapshots of Common Crawl contain over 200 billion HTML documents, making efficient processing of these documents a challenging task. We designed the following steps, utilizing the filtering and extraction modules of RedStone. First, we employ the filtering module to exclude pages that are not related to code. Subsequently, we use the extraction module to convert the HTML pages into plain text required for training.\n\nUpon the completion of the aforementioned steps, we have obtained approximately 114.9 million documents, encompassing a total of 250.2 billion tokens, as detailed in Table 1. The detailed steps are as follows.\n\n2.2.1Filtering\n\nSince code snippets in HTML pages are often enclosed in special tags (e.g., <code>), these tags can be used to quickly determine whether a page contains code snippets. Therefore, RedStone-Code uses WARC files as input and employs code tags to filter out pages that do not contain code snippets. Additionally, during the processing of HTML pages, we observed that they often contain a significant amount of noise (e.g., hidden nodes) that could interfere with subsequent content extraction. To address this, we implemented element filtering to remove invisible elements. Moreover, Code snippets sometimes appear within a single element, such as inline or single-line code snippets. Other times, they span multiple elements that share a common parent, such as multi-line code snippets. In these cases, we use rules to determine the overall scope of the code, filter out smaller code elements, and merge the code snippets into a single cohesive unit. The specifics of each step are detailed below.\n\n• \n\nDocument Filtering. To expedite the processing, we implement a pre-filtering operation on the raw HTML content using keyword filters. Specifically, (1) only HTML documents that include the <code or <pre keyword are passed to the next step; (2) To reduce the interweaving situation of two code snippets under the code diff or blame function, we exclude documents whose URLs include blame.php or diff.php.\n\n• \n\nHTML Element Filtering. There are several ways to hide HTML elements, such as using aria-hidden=\"true\", display:none, and visibility:hidden. Some of these are set in HTML, while others are set via JavaScript or CSS. To prevent duplicated or invisible content from remaining after text extraction, we remove all elements with these properties in the HTML.\n\n• \n\nRefined Code Filtering. Code snippets in HTML documents can be represented in various ways. Sometimes, a snippet is enclosed in a single <code> element, such as with inline and single-line code snippets. Other times, it spans several <code> elements that share a common parent, as seen with multi-line code snippets. To filter HTML documents for code snippets and identify the root element of each filtered code snippet, we follow these steps:\n\n– \n\nTraverse each element in the DOM tree of the HTML document and identify all elements of type <code>.\n\n– \n\nIf the parent of each <code> element is either <pre> or <tbody>, treat its parent as a candidate root element for containing the code snippet. Otherwise, the <code> element itself is the candidate root element.\n\n– \n\nApply a regular expression to the extracted text from each candidate root element to determine whether it includes a code snippet. Specifically, the candidate root element is considered to contain a code snippet if it matches one of the four conditions in the designed regular expression: keywords of programming languages, code indicators, function calls, and variable assignments.\n\n– \n\nIf a candidate element is identified as containing a code snippet, its type is changed to <code-encode>. Consequently, the entire HTML document is detected as containing code knowledge.\n\n– \n\nTo avoid the interweaving situation of code lines and line numbers in one code snippet, we remove line numbers by identifying and deleting consecutive single-line or alternating-line increasing numbers within each code snippet.\n\n– \n\nTo merge adjacent code snippets that should belong to one code snippet, we detect and combine them by locating a closing code tag immediately followed by an opening code tag.\n\n2.2.2Extraction\n\nIn this step, we convert the WARC HTML pages to WET plain text format by adopting the WEATGenerator function in the ia-hadoop-tools library [15], which is the WARC-to-WET conversion method officially used by Common Crawl. We do not use Trafilatura to extract the main content from the HTML because Trafilatura’s recall is relatively low, and it is likely to exclude code snippets from the main content.\n\n2.3RedStone-Math\n\nThe ability to reason mathematically is crucial for LLMs. Several studies have aimed to enhance this capability. For instance, the PaLM [13] was fine-tuned on billions of tokens from mathematical documents sourced from arXiv and the web, while OpenWebMath  [51] was trained on 14.7 billion tokens of mathematical webpages from Common Crawl. Both models have shown significant improvements in solving problems that require quantitative reasoning.\n\nMathematical formulas or code snippets on web pages typically have corresponding HTML tags, making the logic for constructing these two types of data quite similar. There are only slight differences in the processing steps. By making appropriate modifications, we utilized the HTML content from Common Crawl’s WARC files to filter pages containing mathematical content on a large scale.\n\nAdditionally, during the construction of RedStone-Math, we discovered a substantial number of mathematical formulas in ASCII format. These formulas exist as plain text on web pages and cannot be identified through HTML tags. Therefore, for these mathematical pages, we designed a new processing workflow specifically to extract these formulas.\n\nFinally, we compiled a comprehensive RedStone-Math totaling 15.9 billion tokens, as illustrated in Table 4. This dataset encompasses mathematical formulas along with their corresponding context, thereby enhancing the model’s ability to understand mathematics. The following sections will describe the construction processes for these two different types of mathematical data.\n\nRedStone-Math \tTokens (B)\tPages (M)\tTokens per Page\nHTML-Math\t11.1\t2.7\t4,041.2\nASCII-Math\t4.8\t3.7\t1,297.9\nTotal\t15.9\t6.4\t2,460.7\nTable 4:RedStone-Math from Common Crawl.\n2.3.1HTML-Math\nFiltering.\n\nThe filtering process encompasses document filtering, HTML element filtering, and fine-grained filtering. The workflow is similar to that used for RedStone-Code. Document filtering removes pages that do not contain mathematical formulas by using a coarse heuristic based on HTML tags typically associated with mathematics. However, content wrapped in mathematical tags is not necessarily a valid formula; it could merely be a few mathematical characters or incorrect expressions. Therefore, more refined mathematical formula filtering rules are required to accurately determine whether a page genuinely contains mathematical formulas. The specifics of each step are detailed below.\n\n• \n\nDocument Filtering. We use keywords such as <math, <annotation, =\"math, athjax, math-container, class=\"tex\", tex.cgi, latex.php, katex.min.css, \\frac, and codecogs to pre-filter the HTML documents associated with mathematics.\n\n• \n\nRefined Math Filtering. To verify the syntactical correctness of formulas, it is essential first to identify their positions for extraction. Consequently, this process is divided into two steps: formula localization and formula syntax verification.\n\n– \n\nTo identify mathematical formulas in HTML documents, we use HTML tags and related attributes. For instance, we utilize tags such as <script> and <math>. For LaTeX formulas, we use specific prefixes like <script type=\"math/tex\", <script type=\"math/latex\", <script type=\"math/asciimath\", <span class=\"math-formula\", and <annotation encoding=\"application/x-tex\" to detect them. For MathML formulas, we apply <math and <script type=\"math/mml\" to identify them.\n\n– \n\nTo ensure the grammatical correctness of each mathematical formula, we apply LatexWalker from the pylatexenc library [22] to check the grammatical correctness of each formula, filtering out documents with errors.\n\nAnother potential issue is the possibility of duplicate representations of formulas. For instance, a LaTeX annotation, typically represented as <annotation encoding=\"application/x-tex\", is included as child node within a MathML formula represented as <math>. This could result in multiple duplicates of the same formula. Therefore, we need to ensure that each mathematical formula is represented only once, eliminating any duplicate representations, such as formula text along with formula annotations, formula images, and alternative text for images.\n\nExtraction.\n\nThe Extraction module follows the same processing procedure as RedStone-Code, which will not be reiterated here.\n\n2.3.2ASCII-Math\n\nDuring the processing, we observed that many mathematical formulas were directly represented in ASCII math format. These formulas were not enclosed within any math-related HTML tags but existed as plain text within the pages. This type of data is also highly valuable. For this type of data, we employed the following pipeline.\n\nExtraction.\n\nSince the mathematical formulas are no longer encapsulated within HTML tags, we can convert HTML pages into plain text format in advance to facilitate subsequent filtering. Unlike the text extraction process described for HTML tags, all HTML documents in this step are converted into text documents using the Trafilatura, excluding menu, header, and footer content, resulting in cleaner text.\n\nFiltering.\n\nTo efficiently extract pages containing mathematical formulas from a vast collection of raw pages, we employ a two-stage filtering approach. Initially, we use rule-based heuristics to quickly and broadly exclude pages without mathematical content. Subsequently, we apply a model for finer-grained filtering on the remaining pages, generating the final dataset.\n\n• \n\nRule-based Filtering. We categorize the keywords into two groups: LaTeX symbols and non-LaTeX symbols. For LaTeX symbols, such as \\frac, \\mu, \\dot, \\log, and \\eq, we have collected over 3,000 keywords in total. For non-LaTeX symbols, such as sqrt, sum, log, +, *, and $, we have gathered more than 20 keywords. Based on these keywords, we filter out text documents that contain fewer than five of them. At this stage, only about 0.1% of the text documents pass the filter and proceed to the next stage for further processing.\n\n• \n\nModel-based Filtering. After the pre-filtering stage by keywords, we further filter out the text documents by employing a self-trained classification model. Firstly, we construct the training dataset by sampling 100k text documents from the pages previously identified as containing mathematical formulas using HTML tags, and also randomly selecting 100k text documents that exclude the math text documents as negative samples, and totally 200k samples. Secondly, we train an n-gram model using fasttext [29] to balance efficacy and efficiency. During the filtering stage, we apply this self-trained model to further filter the text documents, using a threshold of 0.5.\n\n2.4RedStone-QA\n\nQA datasets often contain a wealth of specific knowledge and are highly valuable for advancing understanding in various fields. To answer these questions, LLMs need to have a comprehensive and accurate understanding of the world, requiring critical thinking, analysis, and discussion. We have created two types of QA datasets: open question answering and multiple-choice question answering.\n\nFor the open question answering data, there are various types, such as short answer, fill-in-the-blank, multiple choice, and true-or-false questions. These open questions are often interspersed with other text content in the document, which may include reading passages, explanations, or other informative content. Therefore, the contextual information along with the open questions is essential for a language model to effectively understand and learn.\n\nFor the multiple-choice question answering data, during the construction of open question answering data, we observed a significant number of high-quality, valuable multiple-choice questions within the Common Crawl. A considerable proportion of these multiple-choice questions include the stem, options, answers, and explanations. The presence of answers and explanations imbues this data with a high density of knowledge, facilitating a deeper understanding of the real world for the model. Therefore, we further processed the open question answering data to obtain multiple-choice question answering data.\n\nRedStone-QA \tTokens (B)\tPages (M)\tTokens per Page\nOpen Question Aswering\t51.3\t42.5\t1,206.8\nMulti-choice Question Aswering\t0.1\t1.6\t63.8\nTotal\t51.4\t44.1\t1165.5\nTable 5:RedStone-QA from Common Crawl.\n\nThe final dataset of RedStone-QA consists of 44.1 million documents and 51.4 billion tokens, as shown in Table 5. The following sections will describe the data construction process in detail.\n\n2.4.1Open Question Aswering\n\nIn processing pages that contain open questions, we utilized the extraction and filtering modules. The extraction module is responsible for extracting main content from the pages, while the filtering module is tasked with identifying and selecting the pages that contain open questions. The specific steps are as follows.\n\nExtraction.\n\nThe first step is to extract the main content of the original pages. Due to the low recall when extracting main content from WARC files using Trafilatura, which often results in missing parts of the main content, we use WET files as input for creating the RedStone-OpenQA. To ensure the completeness of the extracted content, we employed CCNet’s paragraph-level exact deduplication method. This extraction procedure is consistent with the one described earlier and can be referenced in Section 2.1.1.\n\nFiltering.\n\nTo quickly and efficiently identify pages containing open-domain questions, our filtering module employs a two-stage approach: rule-based filtering and model-based filtering. Initially, we use rule-based filtering to identify common patterns of open-domain questions, thereby discarding pages that are unlikely to contain such questions. This step significantly reduces the number of pages. Subsequently, we apply model-based filtering to the remaining pages to detect the presence of open-domain questions.\n\n• \n\nRule-based Filtering. Since the answers contain the knowledge for each question, We filter text documents to ensure they include both questions and answers by designing two sets of patterns for identifying each. For the question pattern, we use keywords such as ‘‘what’’, ‘‘where’’, ‘‘why’’, ‘‘when’’, ‘‘who’’, ‘‘whose’’, ‘‘how’’, ‘‘q&a’’, ‘‘q & a’’, ‘‘q:’’, ‘‘que:’’, ‘‘question:’’, ‘‘quiz:’’, ‘‘exam:’’, ‘‘examination:’’, ‘‘probe:’’, ‘‘request:’’, ‘‘challenge:’’, ‘‘test:’’, ‘‘query:’’, and ‘‘survey:’’. The document must contain at least one of these keywords. For the answer pattern, we utilize keywords such as ‘‘q&a’’, ‘‘q & a’’, ‘‘a:’’, ‘‘ans:’’, ‘‘answer:’’, ‘‘solution:’’, ‘‘reply:’’, ‘‘response:’’, ‘‘result:’’, ‘‘outcome:’’, ‘‘explanation:’’, ‘‘conclusion:’’, ‘‘finding:’’, ‘‘assertion:’’, ‘‘statement:’’, and ‘‘clarification:’’. The document must also contain at least one of these keywords. Only text documents that match both patterns are retained for further filtering.\n\n• \n\nModel-based Filtering. Rule-based filtering serves only as an initial pre-processing step. To further refine the text documents and ensure they contain open questions, we employ a self-trained classification model. Specifically, (1) we use GPT-3.5-turbo as an annotator to label whether a page contains open questions and to categorize them (e.g., short answer, fill-in-the-blank, multiple choice, etc.), and then constructed a training set of 200k labeled samples, with an equal split between positive and negative samples. Among the positive samples, we ensured a balanced mix of different types of open-domain questions. (2) We then trained an n-gram-based classification model using fastText [29] to determine whether a page contains open-domain questions. (3) Finally, we used the trained model to filter data at scale, thereby obtaining a large dataset of open-domain questions.\n\n2.4.2Multi-choice Question Aswering\n\nThe open question aswering data contains pages with various types of open questions, which need to be further filtered to obtain pages containing multiple-choice questions. For multiple-choice questions, we only extract content related to the questions, such as the question stems, answers, and explanations, and finally standardize this content into a uniform format.\n\nWe utilize two modules in the RedStone: Filtering and Extraction. The Filtering module identifies pages containing multiple-choice questions, while the Extraction module extracts the multiple-choice questions from these pages.\n\nFiltering.\n\nAmong the components of multiple-choice questions, the list of options is more distinctly characterized and thus easier to locate. Consequently, to efficiently filter out pages containing multiple-choice questions from the vast number of web pages, we employed the following two rules:\n\n• \n\nThe serial number is typically in one of the following formats: a, b, c, d, …, 1, 2, 3, 4, …, and i, ii, iii, iv, ….\n\n• \n\nThe delimiter can be one of the following: ., -, ), \n>\n, ], and \\space.\n\nAdditionally, we ensure that multiple-choice questions have corresponding answers by retaining only those text documents that contain one of the following keywords: answer:, solution:, reply:, response:, ans:, a:, and r:. By applying these rules, we can quickly filter and identify a large number of pages that potentially contain multiple choice questions.\n\nExtraction.\n\nWe aim to obtain a clean dataset containing only multiple choice questions. Therefore, from the pages identified by the filtering module as potentially containing multiple choice questions, we further locate and extract the questions from the pages. It is important to note that multiple choice questions without answers are useless for model training, so we only extract those that include answers, explanations for the answers are optional.\n\n• \n\nThe serial number is typically in one of the following formats: a, b, c, d, …, 1, 2, 3, 4, …, and i, ii, iii, iv, ….\n\n• \n\nThe delimiter can be one of the following: ., -, ), \n>\n, ], and \\space.\n\nTo locate the multiple choice questions on a page, we use different rules to identify each component: the stem, the options, the answer, and the explanation. A multiple choice question is retained only if it contains at least the stem, options, and answer. we employed the following rules:\n\n• \n\nChoice List Identification: Locate the candidate of the choice list for each multiple choice question using the rules concluded in the filtering stage.\n\n• \n\nStem Identification: The stem usually precedes the choice list, so the textline before the options is selected as the question stem.\n\n• \n\nAnswer Identification: If the text line following each candidate in the choice list starts with any of the following keywords: answer:, solution:, reply:, response:, ans:, a:, or r:, we treat this candidate as a genuine choice list. The text line following the choice list is added as the answer, while the text line preceding it is considered the question.\n\n• \n\nExplanation Identification: If the text line following the answer starts with explanation:, it is added as the explanation component as well.\n\nGiven the various formats of multiple choice questions on the web—for instance, the answer might precede the stem—the order of the components is not fixed. Therefore, after extracting the multiple choice questions, we standardize each component to ensure uniformity. For example, we arrange each multiple choice question in the order of question, option list, answer, and explanation. We prefix the answer with Answer: and, if an explanation is present, we prefix it with Explanation:. The steps are as follows:\n\n• \n\nQuestion Formatting: Remove any serial number and delimiter at the start of the question.\n\n• \n\nChoice List Formatting: Convert the serial numbers of the choice list to A, B, C, D, ... in order and use . as the delimiter between the serial number and choice.\n\n• \n\nAnswer and Explanation Formatting: Precede the answer with Answer: and the explanation, if it exists, with Explanation:.\n\n• \n\nComponent Arrangement: Arrange each multiple choice question in the order of the question, choice list, answer, and explanation.\n\n3Experiments\n3.1Settings\n\nModel Setting. For evaluations on general domain datasets, we trained models with 1.3 billion parameters on 50 billion tokens. We utilized the same architecture as the LLaMA model, incorporating SwiGLU [58], RoPE [56], and RMS [66]. For evaluations on the domain-specific dataset, We utilized the same architecture as the StableLM-2-1.6B[6] model. As described in Table 6.\n\nHyperparameters\tGeneral Domain\tDomain-specific\nLayers\t24\t32\nEmbed Dim\t2,048\t2,560\nFFN Dim\t5,760\t6,912\nAttn Head\t32\t32\nVocab Size\t32k\t50k\nTable 6:Hyperparameters of model\n\nTraining Setting. For evaluations on general domain datasets, We follow the GPT-3 paper to set the training parameters. For evaluations on the domain-specific dataset, The learning rate is 2e-5, total training steps is 10k, batch size is 16, max token size is 2k. Further details can be found in Table 7.\n\nHyperparameters\tGeneral Domain\tDomain-Specific\nTraining steps\t300,000\t10,000\nWarmup steps\t1,000\t1,000\nOptimizer\tAdamW\nLearning rate\t2e-4\t2e-5\nLearning rate decay\tLinear\nAdam \n𝛽\n \t(0.9, 0.95)\nWeight decay\t0.1\nDropout\t0\nBatch size of text\t512\t16\n#Token per Batch\t1M\t32k\nTable 7:Hyperparameters of training\n3.2Evaluation Datasets\n\nTo comprehensively evaluate our proposed datasets, we employed a two-pronged approach, utilizing different benchmarks for general domain and domain-specific datasets.\n\nTasks\tType\nHellaSwag [65] \tCommon Sense\nWinogrande [57] \tCommon Sense\nPIQA [7] \tCommon Sense\nARC-E [8] \tQuestion Answering\nARC-C [8] \tQuestion Answering\nOpenBookQA [43] \tQuestion Answering\nTable 8:Evaluation tasks of general domain data\n\nFor the general domain dataset, we leveraged the lm-evaluation-harness pipeline2, which includes a variety of tasks that test reading comprehension, common sense reasoning, and question answering abilities. The benchmarks used for this evaluation are listed in Table 8.\n\nTasks\tType\nHumanEval [16] \tCode Synthesis\nMBPP [2] \tCode Synthesis\nGSM8k [12] \tMathematics\nMinerva Math [19] \tMathematics\nMMLU [27, 26] \tCommon Sense\nWinogrande [57] \tCommon Sense\nARC-C [8] \tQuestion Answering\nARC-E [8] \tQuestion Answering\nOpenBookQA [43] \tQuestion Answering\nTable 9:Evaluation tasks of domain-specific data\n\nFor the domain-specific datasets, we selected benchmarks that are tailored to assess the performance of models in code generation, mathematical reasoning, and question-answering tasks. These benchmarks are detailed in Table 9.\n\n3.3RedStone-Web\nDatasets\tARC-C\tARC-E\tHellaSwag\tOpenBookQA\tPIQA\tWinogrande\tAVERAGE\nRedPajama\t0.2270\t0.4386\t0.3171\t0.1900\t0.5968\t0.5296\t0.3832\nFineWeb\t0.1928\t0.4428\t0.3506\t0.1740\t0.6681\t0.5288\t0.3929\nRefinedWeb\t0.2125\t0.4369\t0.3380\t0.2100\t0.6491\t0.5264\t0.3955\nDCLM\t0.2159\t0.4848\t0.3614\t0.1760\t0.6615\t0.5082\t0.4013\nFineWeb-Edu\t0.2722\t0.5648\t0.3637\t0.1940\t0.6676\t0.5051\t0.4279\nRedStone-Web \t0.2662\t0.5181\t0.3722\t0.2340\t0.6795\t0.5162\t0.4310\nTable 10: Comparison of evaluation tasks across open source datasets.\n\nWe evaluated the performance of our general domain dataset, RedStone-Web, across a variety of common sense reasoning tasks and compared it with several other well-known open-source datasets. The results of these evaluations are summarized in Table 10. The results demonstrate that RedStone-Web performs exceptionally well in most tasks, with particularly strong performance in the ARC-E, HellaSwag, OpenBookQA, and PIQA tasks. Specifically, RedStone-Web achieved the highest scores in HellaSwag (0.3722), OpenBookQA (0.2340), and PIQA (0.6795), indicating that it is highly effective in capturing the nuances required for these tasks.\n\nThe comprehensive evaluation across these tasks reveals that RedStone-Web is a highly competitive dataset for general pre-training. It surpasses the other datasets in most common sense reasoning tasks. This indicates that our approach of leveraging Common Crawl to extract a diverse and high-quality dataset is effective in enhancing the capabilities of LLMs.\n\n3.4RedStone-Code\nDatasets\tHumanEval\npass@1\tHumanEval\npass@10\tMBPP\npass@1\tMBPP\npass@10\nRedStone-Web \t0.0125\t0.0168\t0.0751\t0.1566\n     + RedStone-Code \t0.0555\t0.1035\t0.1311\t0.2458\nTable 11:Evaluation of RedStone-Code. ‘‘+’’ indicates the combination of the current dataset with the previous one. In the experiments, the weight of the RedStone-Web data is set to 0.8, and the weight of the RedStone-Code data is set to 0.2.\n\nThe results in Table 11 demonstrate the significant performance improvement achieved by incorporating the RedStone-Code dataset into the RedStone-Web dataset. For HumanEval pass@1, the combination yields a score of 0.0555, a substantial increase from the 0.0125 achieved by RedStone-Web alone. Similarly, HumanEval pass@10 improves from 0.0168 to 0.1035. This indicates a marked improvement in the model’s ability to generate correct solutions within the first attempt as well as within ten attempts. A similar trend is observed in the MBPP dataset, where the pass rates also exhibit considerable advancements. The pass@1 metric rises from 0.0751 to 0.1311, and the pass@10 metric increases from 0.1566 to 0.2458.\n\nThese experimental results indicate that RedStone-Code can serve as a valuable supplement to LLM pre-training datasets in the domain of code. This integration lays a strong foundation for enhancing the code generation capabilities of LLMs in downstream tasks.\n\n3.5RedStone-Math\nDatasets\tGSM8k\tMATH\nOpenWebMath [51] \t3.2503\t3.1288\nRedStone-Math \t3.1125\t3.0557\nTable 12:Evaluation of RedStone-Math and perplexity measurement on various mathematics benchmarks. Both OpenWebMath and RedStone-Math were trained from scratch.\n\nTable 12 presents a comparative evaluation of RedStone-Math and OpenWebMath on two mathematics benchmarks, GSM8k and MATH. The evaluation metric used is perplexity, which scales more smoothly than accuracies.\n\nFrom the results, it is evident that RedStone-Math outperforms OpenWebMath on both datasets. Specifically, RedStone-Math achieves a perplexity of 3.1125 on the GSM8k dataset, compared to OpenWebMath’s perplexity of 3.2503. This represents a significant improvement, suggesting that RedStone-Math enables faster convergence and enhances modeling capability for the types of problems presented in GSM8k. Similarly, on the MATH dataset, RedStone-Math demonstrates a perplexity of 3.0557, whereas OpenWebMath records a perplexity of 3.1288. This further underscores the effectiveness of RedStone-Math in handling diverse mathematical problems.\n\nRedStone-Math consistently achieves lower perplexity scores across both evaluated datasets, highlighting its potential to enhance model performance in the domain of mathematics. It can serve as a valuable supplement to pre-training datasets in this field.\n\n3.6RedStone-OpenQA\nModel\tMMLU\tARC-Challenge\tARC Easy\tOpenbookqa\tWinogrande\tAVERAGE\nStableLM-2-1.6B\t0.3135\t0.3481\t0.6860\t0.2780\t0.6354\t0.4522\n     + FALN v2\t0.3525\t0.3601\t0.6406\t0.2860\t0.6125\t0.4503\n     + Open Orca\t0.3569\t0.3089\t0.5821\t0.2660\t0.5675\t0.4163\n     + RedStone-QA \t0.4582\t0.3643\t0.6839\t0.2760\t0.6377\t0.4840\nTable 13:Evaluation of RedStone-QA: RedStone-QA comprises two components—open question answering and multiple-choice question answering, with training weights set to 0.8 and 0.2, respectively.\n\nTable 13 presents the performance evaluation of various models across multiple benchmark datasets, including MMLU, ARC Challenge, ARC Easy, OpenbookQA, and Winogrande. The baseline model, StableLM-2-1.6B, demonstrates robust performance with an average accuracy of 0.4522. After fine-tuning with FALN v2, the model shows slight improvements on the MMLU and ARC Challenge tasks, although the overall average performance slightly decreases from 0.4522 to 0.4503. Fine-tuning with Open Orca maintains competitive performance on the MMLU task but leads to significant drops in scores on the ARC Easy and OpenbookQA tasks, from 0.6860 to 0.5821 and from 0.2780 to 0.2660, respectively. The average performance decreases to 0.4163, indicating that this enhancement may not generalize well across different tasks.\n\nThe proposed RedStone-QA dataset achieves the highest scores on most datasets, with an overall average performance significantly improving to 0.4840. Notably, it shows a 14.47% improvement on the MMLU dataset, highlighting the effectiveness of RedStone-QA in enhancing the model’s question-answering capabilities.\n\n4Related Work\n4.1General Domain Data Pipelines\n\nSince the advent of BERT [18], there has been a growing trend to leverage large-scale data from Common Crawl for model training. CCNet [60] extracted a substantial amount of high-quality bilingual text from it to build machine translation models. T5 [53] employed a series of heuristic rules to extract text and create the C4 dataset. With the rapid development of LLMs, recent efforts have increasingly focused on constructing larger and higher-quality datasets from Common Crawl to support the training of these models. The Pile [24] used jusText [21] to extract text from Common Crawl, resulting in Pile-CC. LLaMA [59] utilized the CCNet pipeline with modifications to generate a vast amount of pre-training data, though this data was not made publicly available. Subsequently, RedPajama [14] reproduced the training data used in LLaMA and open-sourced the data. Aiming for even higher data quality, RedPajama v2 [14] introduced 46 quality signals to describe data characteristics from various dimensions. RefinedWeb [50] employed content extraction tools to extract main content from HTML pages provided by Common Crawl, yielding cleaner and higher-quality text, though only a small portion of this data was open-sourced. In response, FineWeb [49] reproduced RefinedWeb and open-sourced the data, while also constructing a new filter to exclude educational content, resulting in the higher-quality pre-training dataset FineWeb-edu. DCLM [38] extracted a large volume of text from Common Crawl and developed a custom filter to obtain a substantial amount of instruction-formatted data, significantly enhancing data quality. We also introduce RedStone-Web, a high-quality dataset derived from our RedStone pipeline, which features simpler processing steps, more data, and improved quality.\n\n4.2Domain Specific Data Pipelines\n\nLLMs are currently being widely utilized to address a variety of problems. Among the most prevalent applications are their roles as powerful assistants for writing and editing code, as well as for solving mathematical problems. Additionally, LLMs are frequently engaged in interactive QA formats, making QA capabilities a critical aspect of their evaluation. Given the extensive range of potential applications for LLMs, this paper focuses on advancements in code, math, and QA, which are also key components of RedStone.\n\nCode. Recent advancements in code generation have been significantly driven by Code LLMs. Prominent models such as CodeGen  [45], ERNIE-Code  [17], StarCoder  [40], CodeT5+  [61], CodeLLaMa  [52], and Deepseek-Coder  [25] focus on enhancing coding capabilities during the pre-training stage. Given the substantial data requirements for pre-training, GitHub has emerged as a crucial resource  [37, 41, 46, 23]. For instance, The Stack utilized GHArchive to download 137.36 million repositories from GitHub. After filtering, cleaning, license detection, and deduplication, a training set of 1450.75 GB of data spanning 30 popular programming languages was obtained  [30]. Subsequently, The Stack v2 expanded its data sources to include other high-quality open datasets, such as GitHub issues, pull requests, Kaggle and Jupyter notebooks, code documentation, and other natural language datasets related to math, coding, and reasoning. This expansion resulted in a dataset four times larger than the initial StarCoder dataset. Additionally, models like Code Alpaca  [10], WizardCoder  [42], OctoPack  [44], and Magicoder  [62] focus on enhancing coding capabilities during the instruction tuning stage. This stage typically involves constructing high-quality instruction datasets to improve model performance.\n\nMath. The application of LLMs in solving mathematical problems has seen significant advances, driven by the necessity to manage complex computations and deliver precise solutions. Models such as GPT-3  [4] and Minerva  [32] have been specifically fine-tuned for mathematical problem-solving tasks. These models utilize extensive datasets comprising mathematical texts, problem sets, and solutions to enhance their capabilities. For instance, the OpenWebMath dataset  [51], which includes 14.7 billion tokens of mathematical web pages from Common Crawl, demonstrates the potential of training models on large-scale mathematical data. The training process often involves step-by-step problem-solving, aiding in the understanding and generation of solutions for complex mathematical queries. Furthermore, incorporating symbolic computation and formal methods into LLMs has enhanced their ability to handle algebraic manipulations, calculus, and other advanced mathematical concepts. The integration of these techniques ensures that the models provide not only accurate answers but also human-readable solutions that are easy to follow and verify.\n\nQuestion and Answer. Interactive Question and Answer (QA) capabilities are a cornerstone of LLM applications. Models like BERT  [18], T5  [53], and GPT-3  [4] have set benchmarks in the QA domain by excelling in tasks such as reading comprehension, open-domain QA, and conversational agents. The performance of these models is often evaluated on standardized datasets like SQuAD  [55], TriviaQA  [28], and Natural Questions  [31]. Additionally, the Massive Multitask Language Understanding (MMLU) dataset  [27] has emerged as a critical benchmark for evaluating the broad knowledge and reasoning abilities of LLMs across diverse subjects, ranging from elementary mathematics to advanced science and humanities. Recent models, such as ChatGPT  [47] and InstructGPT  [48], build on these foundations by incorporating reinforcement learning from human feedback (RLHF) to improve their interactive and conversational abilities. Specialized models like QA-GNN  [64] and DrQA  [9] enhance QA performance by integrating graph neural networks and retrieval-based approaches, respectively. These advancements enable LLMs to provide more accurate, context-aware, and informative answers, making them indispensable tools for customer support, educational tutoring, and information retrieval systems. The continuous refinement of QA datasets and the development of hybrid models that combine retrieval and generative approaches are driving the next generation of QA systems.\n\n5Conclusion and Future Work\n\nThis paper introduces RedStone, a comprehensive data pipeline designed to create specialized large-scale datasets by leveraging the vast and diverse data from Common Crawl. The RedStone pipeline is built around Extraction and Filtering modules. These modules can be flexibly combined to efficiently mine data from various domains. We used RedStone to construct large-scale datasets, totaling 3.48 trillion tokens, for RedStone-Web, RedStone-Code, RedStone-Math, and RedStone-QA. The experimental results demonstrate the effectiveness of RedStone. This dataset not only showcases the pipeline’s ability to extract specific types of data but also significantly surpasses existing open-source datasets in terms of quality and scale. We have detailed the dataset construction process to ensure reproducibility and transparency, providing a valuable resource for developing competitive LLMs. This makes it possible to create high-quality, domain-specific datasets at scale. The datasets and code developed through RedStone will be open-sourced to foster further development and collaborative efforts within the research community.\n\nFor future work, several promising directions could enhance RedStone’s capabilities. First, integrating more advanced filtering techniques, such as leveraging multimodal signals (e.g., visual or contextual cues) and sophisticated machine learning models, could refine data quality and improve domain diversity. Second, extending the pipeline to support multilingual datasets and incorporating multimodal content, such as images, audio, and videos, would enable the development of LLMs that excel in cross-lingual and multimodal tasks. Lastly, implementing real-time data updates, including automated mechanisms to incorporate fresh and relevant web data while maintaining stringent quality standards, would ensure the datasets remain current and applicable to evolving use cases. Together, these efforts will unlock the full potential of large-scale web data, advancing the creation of specialized and high-performance language models across diverse domains.\n\nReferences\nAI@ [24]\tAI@Meta.Llama 3 model card.2024.\nAON+ [21]\tJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton.Program synthesis with large language models, 2021.\nBar [21]\tAdrien Barbaresi.Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction.In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122--131. Association for Computational Linguistics, 2021.\nBMR+ [20]\tTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models are few-shot learners.Advances in neural information processing systems, 33:1877--1901, 2020.\nBro [97]\tAndrei Z Broder.On the resemblance and containment of documents.In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 21--29. IEEE, 1997.\nBTM+ [24]\tMarco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al.Stable lm 2 1.6 b technical report.arXiv preprint arXiv:2402.17834, 2024.\nBZB+ [20]\tYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.Piqa: Reasoning about physical commonsense in natural language.In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\nCCE+ [18]\tPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.Think you have solved question answering? try arc, the AI2 reasoning challenge.CoRR, abs/1803.05457, 2018.\nCFWB [17]\tDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.Reading Wikipedia to answer open-domain questions.In Association for Computational Linguistics (ACL), 2017.\nCha [23]\tSahil Chaudhary.Code alpaca: An instruction-following llama model for code generation.https://github.com/sahil280114/codealpaca, 2023.\nCIJ+ [22]\tNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.Quantifying memorization across neural language models.arXiv preprint arXiv:2202.07646, 2022.\nCKB+ [21]\tKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.Training verifiers to solve math word problems.arXiv preprint arXiv:2110.14168, 2021.\nCND+ [23]\tAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.Palm: Scaling language modeling with pathways.Journal of Machine Learning Research, 24(240):1--113, 2023.\nCom [23]\tTogether Computer.Redpajama: an open dataset for training large language models, 2023.\nCra [23]\tCommon Crawl.Web archiving tools on hadoop, 2023.\nCTJ+ [21]\tMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.Evaluating large language models trained on code.2021.\nCWP+ [22]\tYekun Chai, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, and Hua Wu.Ernie-code: Beyond english-centric cross-lingual pretraining for programming languages.arXiv preprint arXiv:2212.06742, 2022.\nDCLT [18]\tJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.Bert: Pre-training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805, 2018.\nDGA [22]\tEthan Dyer and Guy Gur-Ari.Minerva: Solving quantitative reasoning problems with language models.June, 30:2022, 2022.\nDJP+ [24]\tAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.The llama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024.\nEN [13]\tIstván Endrédy and Attila Novák.More effective boilerplate removal-the goldminer algorithm.Polibits, (48):79--83, 2013.\nFai [23]\tPhilippe Faist.Simple latex parser providing latex-to-unicode and unicode-to-latex conversion, 2023.\nFAL+ [22]\tDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.Incoder: A generative model for code infilling and synthesis.arXiv preprint arXiv:2204.05999, 2022.\nGBB+ [20]\tLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.The pile: An 800gb dataset of diverse text for language modeling.arXiv preprint arXiv:2101.00027, 2020.\nGZY+ [24]\tDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al.Deepseek-coder: When the large language model meets programming--the rise of code intelligence.arXiv preprint arXiv:2401.14196, 2024.\n[26]\tDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.Aligning ai with shared human values.Proceedings of the International Conference on Learning Representations (ICLR), 2021.\n[27]\tDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding.Proceedings of the International Conference on Learning Representations (ICLR), 2021.\nJCWZ [17]\tMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.arXiv preprint arXiv:1705.03551, 2017.\nJGB+ [16]\tArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov.Fasttext.zip: Compressing text classification models.arXiv preprint arXiv:1612.03651, 2016.\nKLA+ [22]\tDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al.The stack: 3 tb of permissively licensed source code.arXiv preprint arXiv:2211.15533, 2022.\nKPR+ [19]\tTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.Natural questions: a benchmark for question answering research.Transactions of the Association for Computational Linguistics, 7:453--466, 2019.\nLAD+ [22]\tAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.Solving quantitative reasoning problems with language models.Advances in Neural Information Processing Systems, 35:3843--3857, 2022.\nLAZ+ [23]\tRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.Starcoder: may the source be with you!arXiv preprint arXiv:2305.06161, 2023.\nLBAvWW [24]\tAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf.Fineweb-edu, May 2024.\n[35]\tYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.Textbooks are all you need ii: phi-1.5 technical report.arXiv preprint arXiv:2309.05463, 2023.\n[36]\tYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.Textbooks are all you need ii: phi-1.5 technical report.arXiv preprint arXiv:2309.05463, 2023.\nLCC+ [22]\tYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al.Competition-level code generation with alphacode.Science, 378(6624):1092--1097, 2022.\nLFS+ [24]\tJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al.Datacomp-lm: In search of the next generation of training sets for language models.arXiv preprint arXiv:2406.11794, 2024.\nLIN+ [21]\tKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini.Deduplicating training data makes language models better.arXiv preprint arXiv:2107.06499, 2021.\nLLA+ [24]\tAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al.Starcoder 2 and the stack v2: The next generation.arXiv preprint arXiv:2402.19173, 2024.\nLSW+ [22]\tHugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al.The bigscience roots corpus: A 1.6 tb composite multilingual dataset.Advances in Neural Information Processing Systems, 35:31809--31826, 2022.\nLXZ+ [23]\tZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.Wizardcoder: Empowering code large language models with evol-instruct.arXiv preprint arXiv:2306.08568, 2023.\nMCKS [18]\tTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.Can a suit of armor conduct electricity? a new dataset for open book question answering.In EMNLP, 2018.\nMLZ+ [23]\tNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, and Shayne Longpre.Octopack: Instruction tuning code large language models.arXiv preprint arXiv:2308.07124, 2023.\n[45]\tErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.Codegen: An open large language model for code with multi-turn program synthesis.arXiv preprint arXiv:2203.13474, 2022.\n[46]\tErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.A conversational paradigm for program synthesis.arXiv preprint arXiv:2203.13474, 30, 2022.\nOpe [23]\tOpenAI.Chatgpt: Chatbot by openai.https://www.openai.com/chatgpt, 2023.\nOWJ+ [22]\tLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.Training language models to follow instructions with human feedback.Advances in neural information processing systems, 35:27730--27744, 2022.\nPKvWW [24]\tGuilherme Penedo, Hynek Kydlíček, Leandro von Werra, and Thomas Wolf.Fineweb, 2024.\nPMH+ [23]\tGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.arXiv preprint arXiv:2306.01116, 2023.\nPSAB [23]\tKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba.Openwebmath: An open dataset of high-quality mathematical web text.arXiv preprint arXiv:2310.06786, 2023.\nRGG+ [23]\tBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al.Code llama: Open foundation models for code.arXiv preprint arXiv:2308.12950, 2023.\nRSR+ [20]\tColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of machine learning research, 21(140):1--67, 2020.\nRWC+ [19]\tAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.Language models are unsupervised multitask learners.OpenAI blog, 1(8):9, 2019.\nRZLL [16]\tPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.Squad: 100,000+ questions for machine comprehension of text.arXiv preprint arXiv:1606.05250, 2016.\nSAL+ [24]\tJianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.Roformer: Enhanced transformer with rotary position embedding.Neurocomputing, 568:127063, 2024.\nSBBC [19]\tKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.Winogrande: An adversarial winograd schema challenge at scale.arXiv preprint arXiv:1907.10641, 2019.\nSha [20]\tNoam Shazeer.Glu variants improve transformer.arXiv preprint arXiv:2002.05202, 2020.\nTLI+ [23]\tHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023.\nWLC+ [19]\tGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave.Ccnet: Extracting high quality monolingual datasets from web crawl data.arXiv preprint arXiv:1911.00359, 2019.\nWWJH [21]\tYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi.Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation.arXiv preprint arXiv:2109.00859, 2021.\nWWL+ [23]\tYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.Magicoder: Source code is all you need.arXiv preprint arXiv:2312.02120, 2023.\nXSZ+ [23]\tCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.Wizardlm: Empowering large language models to follow complex instructions.arXiv preprint arXiv:2304.12244, 2023.\nYRB+ [21]\tMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.Qa-gnn: Reasoning with language models and knowledge graphs for question answering.arXiv preprint arXiv:2104.06378, 2021.\nZHB+ [19]\tRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.HellaSwag: Can a machine really finish your sentence?In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791--4800, Florence, Italy, July 2019. Association for Computational Linguistics.\nZS [19]\tBiao Zhang and Rico Sennrich.Root mean square layer normalization.Advances in Neural Information Processing Systems, 32, 2019.\nAppendix AData Samples\nGenerated on Wed Dec 4 15:25:24 2024 by LaTeXML",
  "usage": {
    "tokens": 17902
  }
}
```
