---
title: LILO: Learning Interpretable Libraries by Compressing and Documenting Code
description: Abstract page for arXiv paper 2310.19791: LILO: Learning Interpretable Libraries by Compressing and Documenting Code
url: https://arxiv.org/abs/2310.19791
timestamp: 2025-01-20T15:54:01.292Z
domain: arxiv.org
path: abs_2310.19791
---

# LILO: Learning Interpretable Libraries by Compressing and Documenting Code


Abstract page for arXiv paper 2310.19791: LILO: Learning Interpretable Libraries by Compressing and Documenting Code


## Content

Skip to main content

In just 3 minutes help us improve arXiv:

Annual Global Survey
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
cs
>
arXiv:2310.19791

Help | Advanced Search

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
Computer Science > Computation and Language
[Submitted on 30 Oct 2023 (v1), last revised 15 Mar 2024 (this version, v4)]
LILO: Learning Interpretable Libraries by Compressing and Documenting Code
Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas
While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.
Comments:	ICLR 2024 camera-ready
Subjects:	Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)
Cite as:	arXiv:2310.19791 [cs.CL]
 	(or arXiv:2310.19791v4 [cs.CL] for this version)
 	
https://doi.org/10.48550/arXiv.2310.19791
Focus to learn more
Submission history
From: Gabriel Grand [view email]
[v1] Mon, 30 Oct 2023 17:55:02 UTC (2,946 KB)
[v2] Mon, 12 Feb 2024 21:06:05 UTC (3,056 KB)
[v3] Thu, 14 Mar 2024 14:54:54 UTC (3,056 KB)
[v4] Fri, 15 Mar 2024 16:55:47 UTC (3,056 KB)

Access Paper:
View PDF
HTML (experimental)
TeX Source
Other Formats
view license
Current browse context:
cs.CL
< prev   |   next >

new | recent | 2023-10
Change to browse by:
cs
cs.AI
cs.LG
cs.PL

References & Citations
NASA ADS
Google Scholar
Semantic Scholar
Export BibTeX Citation
Bookmark
 
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
Connected Papers Toggle
Connected Papers (What is Connected Papers?)
Litmaps Toggle
Litmaps (What is Litmaps?)
scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)
About
Help
Contact
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance

arXiv Operational Status 
Get status notifications via email or slack

## Metadata

```json
{
  "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
  "description": "Abstract page for arXiv paper 2310.19791: LILO: Learning Interpretable Libraries by Compressing and Documenting Code",
  "url": "https://arxiv.org/abs/2310.19791",
  "content": "Skip to main content\n\nIn just 3 minutes help us improve arXiv:\n\nAnnual Global Survey\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\nDonate\n>\ncs\n>\narXiv:2310.19791\n\nHelp | Advanced Search\n\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nComputer Science > Computation and Language\n[Submitted on 30 Oct 2023 (v1), last revised 15 Mar 2024 (this version, v4)]\nLILO: Learning Interpretable Libraries by Compressing and Documenting Code\nGabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, Jacob Andreas\nWhile large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.\nComments:\tICLR 2024 camera-ready\nSubjects:\tComputation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)\nCite as:\tarXiv:2310.19791 [cs.CL]\n \t(or arXiv:2310.19791v4 [cs.CL] for this version)\n \t\nhttps://doi.org/10.48550/arXiv.2310.19791\nFocus to learn more\nSubmission history\nFrom: Gabriel Grand [view email]\n[v1] Mon, 30 Oct 2023 17:55:02 UTC (2,946 KB)\n[v2] Mon, 12 Feb 2024 21:06:05 UTC (3,056 KB)\n[v3] Thu, 14 Mar 2024 14:54:54 UTC (3,056 KB)\n[v4] Fri, 15 Mar 2024 16:55:47 UTC (3,056 KB)\n\nAccess Paper:\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.CL\n< prev   |   next >\n\nnew | recent | 2023-10\nChange to browse by:\ncs\ncs.AI\ncs.LG\ncs.PL\n\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\n \nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\nAbout\nHelp\nContact\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\n\narXiv Operational Status \nGet status notifications via email or slack",
  "usage": {
    "tokens": 864
  }
}
```
