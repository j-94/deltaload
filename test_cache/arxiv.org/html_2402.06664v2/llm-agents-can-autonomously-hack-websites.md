---
title: LLM Agents can Autonomously Hack Websites
description: 
url: https://arxiv.org/html/2402.06664v2#:~:text=For%20example%2C%20recent%20work%20has,has%20not%20explored%20autonomous%20agents.&text=for%20preceding%20element-,In%20this%20work%2C%20we%20show%20that%20LLM%20agents%20can%20autonomously,prior%20knowledge%20of%20the%20vulnerability.
timestamp: 2025-01-20T15:58:16.453Z
domain: arxiv.org
path: html_2402.06664v2
---

# LLM Agents can Autonomously Hack Websites



## Content

###### Abstract

In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.

In this work, we show that LLM agents can _autonomously_ hack websites, performing tasks as complex as blind database schema extraction and SQL injections _without human feedback._ Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities _in websites in the wild_. Our findings raise questions about the widespread deployment of LLMs.

Machine Learning, ICML

1 Introduction
--------------

Large language models (LLMs) have become increasingly capable, with recent advances allowing LLMs to interact with tools via function calls, read documents, and recursively prompt themselves (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Shinn et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib31); Wei et al., [2022b](https://arxiv.org/html/2402.06664v2#bib.bib39)). Collectively, these allow LLMs to function autonomously as _agents_ (Xi et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib41)). For example, LLM agents can aid in scientific discovery (Bran et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib4); Boiko et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib3)).

As these LLM agents become more capable, recent work has speculated on the potential for LLMs and LLM agents to aid in cybersecurity offense and defense (Lohn & Jackson, [2022](https://arxiv.org/html/2402.06664v2#bib.bib19); Handa et al., [2019](https://arxiv.org/html/2402.06664v2#bib.bib10)). Despite this speculation, little is known about the capabilities of LLM agents in cybersecurity. For example, recent work has shown that LLMs can be prompted to generate simple malware (Pa Pa et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib23)), but has not explored autonomous agents.

In this work, we show that LLM agents can _autonomously hack websites_, performing complex tasks _without prior knowledge of the vulnerability_. For example, these agents can perform complex SQL union attacks, which involve a multi-step process (38 actions) of extracting a database schema, extracting information from the database based on this schema, and performing the final hack. Our most capable agent can hack 73.3% (11 out of 15, pass at 5) of the vulnerabilities we tested, showing the capabilities of these agents. Importantly, _our LLM agent is capable of finding vulnerabilities in real-world websites_.

![Image 8: Refer to caption](https://arxiv.org/html/2402.06664v2/extracted/5409695/figures/llm-agent-hacking.png)

Figure 1: Schematic of using autonomous LLM agents to hack websites.

To give these LLM agents the capability to hack websites autonomously, we give the agents the ability to read documents, call functions to manipulate a web browser and retrieve results, and access context from previous actions. We further provide the LLM agent with detailed system instructions. These capabilities are now widely available in standard APIs, such as in the newly released OpenAI Assistants API (OpenAI, [2023](https://arxiv.org/html/2402.06664v2#bib.bib22)). As a result, these capabilities can be implemented in as few as 85 lines of code with standard tooling. We show a schematic of the agent in Figure [1](https://arxiv.org/html/2402.06664v2#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LLM Agents can Autonomously Hack Websites").

We show that these capabilities enable the most capable model at the time of writing (GPT-4) to hack websites autonomously. Incredibly, GPT-4 can perform these hacks without prior knowledge of the specific vulnerability. All components are necessary for high performance, with the success rate dropping to 13% when removing components. We further show that hacking websites have a strong scaling law, with even GPT-3.5’s success rate dropping to 6.7% (1 out of 15 vulnerabilities). This scaling law continues to open-source models, with _every_ open-source model we tested achieving a 0% success rate.

We further perform an analysis of the cost of autonomously hacking websites. When incorporating failures into the total cost, it costs approximately $9.81 to attempt a hack on a website. Although expensive, this cost is likely substantially cheaper than human effort (which could cost as much as $80).

In the remainder of the manuscript, we describe how to use LLM agents to autonomously hack websites and our experimental findings.

2 Overview of LLM Agents and Web Security
-----------------------------------------

We first provide an overview of LLM agents and salient points of web security before discussing our methods to use LLM agents to autonomously hack websites.

### 2.1 LLM Agents

Although there no agreed on formal definition of an LLM agent, they have been described as “a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools” (Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35)). For our purposes, we are especially interested in their task-solving capabilities.

One of the most critical capabilities of an LLM agent is the ability to interact with tools and APIs (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Schick et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib29); Mialon et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib20)). This ability enables the LLM to take actions autonomously. Otherwise, some other actor (e.g., a human) would need to perform the action and feed back the response as context. There are many ways for LLMs to interface with tools, some of which are proprietary (e.g., OpenAI’s).

Another critical component of an LLM agent is the ability to plan and react to outputs of the tools/APIs (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35)). This planning/reacting can be as simple as feeding the outputs of the tools/APIs back to the model as further context. Other more complicated methods of planning have also been proposed.

Finally, one useful component for LLM agents is the ability to read documents (closely related to retrieval-augmented generation) (Lewis et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib18)). This can encourage the agent to focus on relevant topics.

There are many other capabilities of LLM agents, such as memory (Shinn et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib31); Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35); Weng, [2023](https://arxiv.org/html/2402.06664v2#bib.bib40)), but we focus on these three capabilities in this manuscript.

### 2.2 Web Security

Web security is an incredibly complex topic, so we focus on salient details. We refer the reader to surveys for further details (Jang-Jaccard & Nepal, [2014](https://arxiv.org/html/2402.06664v2#bib.bib13); Engebretson, [2013](https://arxiv.org/html/2402.06664v2#bib.bib6); Sikorski & Honig, [2012](https://arxiv.org/html/2402.06664v2#bib.bib32)).

Most websites consist of a _front-end_ that the user interacts with. Requests are sent from the front-end to the _back-end_, generally a remote server(s). The remote server generally contains sensitive information, so it is important to ensure that improper access does not occur.

Vulnerabilities in these websites can occur in the front-end, back-end, or both. Generally, exploits in the front-end operate by taking advantage of insecure settings in the browser (often because of security bugs in the front-end logic). For example, the cross-site scripting (XSS) attack operates by a malicious actor injecting an unwanted script (Grossman, [2007](https://arxiv.org/html/2402.06664v2#bib.bib8)). XSS can be used to steal user data.

Back-end exploits often involve a malicious actor exploiting bugs in server-side logic. For example, nearly all front-ends interface with a back-end database. A SQL injection attack takes advantage of the fact that the user can directly send commands to the database by taking actions in the front-end, such as submitting forms (Halfond et al., [2006](https://arxiv.org/html/2402.06664v2#bib.bib9)). The malicious actor can steal sensitive information in the database this way. For example, suppose the website had code to fetch the username and password based on user input, but was not escaped:

uName \= getRequestString("username");

uPass \= getRequestString("userpassword");

sql \= ’SELECT \* FROM Users WHERE Name \="’ + uName + ’" AND Pass \="’ + uPass + ’"’

In this case, an attacker could pass in " or ""=" as the username and password. Because this condition always evaluates to true, and the text is not escaped, this would return all of the information in the database to the attacker. We emphasize that this is a simple form of a SQL injection attack and that we test more challenging forms of SQL attacks, and other backend attacks, in this work.

In this work, we consider vulnerabilities in websites themselves. This excludes large classes of attacks, such as phishing attacks against the maintainers of the websites.

We now turn to leveraging LLM agents to attack websites autonomously.

3 Leveraging LLM Agents to Hack Websites
----------------------------------------

In order to have LLM agents autonomously hack websites, we must first create these agents. Given an agent, we must then prompt the agent with its goals. We describe these two steps below.

Agent setup. In order to leverage LLM agents to hack websites, we use the features of LLM agents described in the section above: function calling, document reading, and planning. As we describe in our Impact Statement, we have omitted specific details in this manuscript. We will make specific details available to researchers upon request.

First, to enable the LLM agents to interface with websites, we allow the agents to interface with a headless web browser (namely, we do not currently leverage the visual features of a website). We use the Playwright browser testing library (playwright, [2023](https://arxiv.org/html/2402.06664v2#bib.bib24)), which runs a browser in a sandboxed environment and allows programmatic access to functionality within a browser, such as clicking on HTML elements. We further give the LLM agents access to the terminal (to access tools such as curl) and a Python code interpreter.

Second, we give the LLM access to documents about web hacking. These documents are publicly sourced from the wider internet and were not modified by us. We used six documents that broadly cover a wide range of web attacks.

Third, we give the agent the ability to plan. There are many forms of planning. We focus on the Assistants API that OpenAI provides, since it works directly with the most capable LLM, GPT-4.

We implement these capabilities within the OpenAI Assistants API for the GPT series of models. To execute the agent itself, we use the LangChain framework (LangChain, [2023](https://arxiv.org/html/2402.06664v2#bib.bib17)), which contains logic to call the OpenAI APIs and execute the functions.

Documents. As we show in Section [4.3](https://arxiv.org/html/2402.06664v2#S4.SS3 "4.3 Ablation Studies ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites"), document retrieval can improve agents’ performance substantially. In this work, we use six documents. These documents include a document on general web hacking, two documents on SQL injections, two documents on XSS, and a document on SSRF. We ensured that these documents did not contain specific instructions on how to hack the websites we created. All of these documents were sourced for the public web. Due to the sensitivity of this work, we have decided not to reveal the specific documents (please read the impact statement at the end of our manuscript for the rationale).

Prompting the agent. An important part of having an LLM agent _autonomously_ hack websites is the initial prompt. Unlike in a chatbot or human assistant setting, the LLM agents do not receive human feedback.

Empirically, we have found that LLM agents can fail for a number of reasons, including not being creative enough initially, not pursuing successful strategies to completion, and getting “stuck” in pursuing strategies that will not work. The choice of initial prompt is important to mitigate these failure cases.

Our best-performing prompt encourages the model to 1) be creative, 2) try different strategies, 3) pursue promising strategies to completion, and 4) try new strategies upon failure. For ethical reasons, we do not release our full prompt publicly but will release this information to researchers upon request.

4 LLM Agents can Autonomously Hack Websites
-------------------------------------------

We now show that LLM agents can _autonomously_ hack websites. We first describe the experimental setup and our choice of metrics.

### 4.1 Experimental Setup

Vulnerabilities. In order to ensure that we did not disrupt real-world systems or violate laws, we conducted our experiments on sandboxed websites. These websites are real websites, including hosting a database, back-end, and front-end. The only difference is the sandboxing.

Table 1: List of vulnerabilities we consider and our ratings of the difficulty.

We tested 15 vulnerabilities, ranging from simple SQL injection vulnerabilities to complex hacks requiring both cross-site scripting (XSS) and Cross-Site Request Forgery (CSRF). We show the complete list of vulnerabilities, their difficulty level, and a description in Table [1](https://arxiv.org/html/2402.06664v2#S4.T1 "Table 1 ‣ 4.1 Experimental Setup ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites"). Our vulnerabilities span different techniques and difficulty levels.

Metrics. In order to test the success of LLM agents hacking these websites, we defined a goal per vulnerability (e.g., stealing private user information). We considered the attack successful if the LLM agent achieved the goal and failed if it di not after 10 minutes of execution, since this is the limit for OpenAI’s assistant API.

In contrast to traditional ML metrics, such as accuracy, a cybersecurity attack only needs to succeed _once_ for the attack to achieve its goals. As such, we ran 5 trials per vulnerability and considered it successful if the agent succeeded once in the 5 trials. We also record the pass rate to understand costs.

Models. We tested 10 total models:

1.  1.GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib1))
    
2.  2.GPT-3.5 (Brown et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib5))
    
3.  3.OpenHermes-2.5-Mistral-7B (Teknium, [2024](https://arxiv.org/html/2402.06664v2#bib.bib33))
    
4.  4.LLaMA-2 Chat (70B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))
    
5.  5.LLaMA-2 Chat (13B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))
    
6.  6.LLaMA-2 Chat (7B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))
    
7.  7.Mixtral-8x7B Instruct (Jiang et al., [2024](https://arxiv.org/html/2402.06664v2#bib.bib15))
    
8.  8.Mistral (7B) Instruct v0.2 (Jiang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib14))
    
9.  9.Nous Hermes-2 Yi (34B) (Research, [2024](https://arxiv.org/html/2402.06664v2#bib.bib27))
    
10.  10.OpenChat 3.5 (Wang et al., [2023a](https://arxiv.org/html/2402.06664v2#bib.bib36))
    

For GPT-4 and GPT-3.5, we use the OpenAI API. For the remainder of the models, we used the Together AI API. We chose the non-GPT models because they were ranked highly on Chatbot Arena (Zheng et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib45)). We used the LangChain framework for all LLMs to wrap them in an agent framework.

### 4.2 Hacking Websites

Table 2: Pass at 5 and overall success rate (pass at 1) of different agents on autonomously hacking websites.

We first measured the success rate of the different LLM and agent frameworks on our benchmark. We show the overall success rate (pass at 5) in Table [2](https://arxiv.org/html/2402.06664v2#S4.T2 "Table 2 ‣ 4.2 Hacking Websites ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites").

As we can see, the overall success rate is as high as 73.3% for our most capable agent, GPT-4 with document reading, function calling, and the assistant API. Importantly, _we do not tell GPT-4 to try a specific vulnerability_ and simply ask it to autonomously hack the website.

We further show a “scaling law” for hacking: GPT-3.5 has a success rate of 6.7%, but this decreases to 0% for _every_ open-source model. This drop in capability is concordant with prior work on how capabilities scale with LLM size (Wei et al., [2022a](https://arxiv.org/html/2402.06664v2#bib.bib38)). We investigate the capabilities of open-source models in more depth in Section [5](https://arxiv.org/html/2402.06664v2#S5 "5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites").

Our most capable agent succeeds on 11 of the 15 vulnerabilities. One of the complex tasks, the hard SQL union attack, requires multiple rounds of interaction with the websites with little to no feedback. In this attack, the agent must perform a “blind” SQL injection to retrieve the database schema. Given the schema, the agent must then select the appropriate username and password, and perform the final hack. This attack requires the ability to synthesize long context, and perform actions based on previous interactions with the website. These results show the capability of LLM agents.

GPT-4 fails on 3 of the 5 hard tasks and 1 of the 6 medium tasks (authorization bypass, Javascript attacks, hard SQL injection, and XSS + CSRF). These attacks are particularly difficult, showing that LLM agents still have limitations with respect to cybersecurity attacks.

In some cases, GPT-4’s success rate for a given vulnerability is low. For example, in the Webhook XSS attack, if the agent does not start with that attack, it does not attempt it later. This can likely be mitigated by having GPT-4 attempt a specific attack from a list of attacks. We hypothesize that the success rate could be raised with this tactic.

In contrast to GPT-4, GPT-3.5 can only correctly execute a single SQL injection. It fails on every other task, including simple and widely known attacks, like XSS and CSRF attacks.

We now turn to ablation experiments to determine which factors are most important for success in hacking.

### 4.3 Ablation Studies

In order to determine which factors are important for success, we tested a GPT-4 agent with the following conditions:

1.  1.With document reading and a detailed system instruction (i.e., same as above),
    
2.  2.Without document reading but with a detailed system instruction,
    
3.  3.With document reading but without a detailed system instruction,
    
4.  4.Without document reading and without detailed system instructions.
    

Function calling and context management (assistants API) are required to interact with the website, so they are not reasonable to remove from the agent. We measured the pass at 5 and the overall success rate for these four conditions.

![Image 9: Refer to caption](https://arxiv.org/html/2402.06664v2/x1.png)

(a) Pass at 5

![Image 10: Refer to caption](https://arxiv.org/html/2402.06664v2/x2.png)

(b) Overall success rate (pass at 1)

Figure 2: Ablation experiments with our best performing agent. We removed the detailed prompt, the documents, and both.

We show results in Figure [2](https://arxiv.org/html/2402.06664v2#S4.F2 "Figure 2 ‣ 4.3 Ablation Studies ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites"). As we can see, removing document reading, detailed system instructions, and both result in substantially reduced performance. Removal of the documents makes performance drop more compared to a less detailed prompt. Removing either the documents or the detailed prompt results in none of the hard vulnerabilities being exploited and few of the medium vulnerabilities. Finally, as expected, removing both the documents and the detailed prompts results in extremely poor performance. Interestingly, it achieves performance comparable to GPT-3.5.

These results show the necessity of recent advances in LLM agent technology to enable autonomous hacking of websites.

5 Understanding Agent Capabilities
----------------------------------

We now turn to a qualitative analysis of the performance of various LLMs on hacking websites. We first analyze GPT-4’s behaviors in more depth before turning to open-source LLMs.

### 5.1 GPT-4 Case Studies

Complex attacks. To understand GPT-4’s performance, we manually explored several examples. We first consider a difficult SQL injection example. The agent is successfully able to:

1.  1.Navigate between pages to determine which to attack.
    
2.  2.Attempt a default username and password (e.g., admin).
    
3.  3.Determine the default failed and attempt a class SQL injection (e.g., appending OR 1 = 1).
    
4.  4.Read the source code to determine that there is a \_GET parameter in the SQL query.
    
5.  5.Determine that this website is vulnerable to a SQL union attack.
    
6.  6.Perform the SQL union attack.
    

As shown, performing these steps requires extended context and memory. Furthermore, it requires GPT-4 to interact with the environment and _change its actions based on feedback from the website_. As we show below, this capability is missing in most open-source models.

In another example, GPT-4 successfully performs a server-side template injection (SSTI) attack, in which user input is directly concatenated to a template. In some cases, this allows the user to run arbitrary code on the server. To perform this attack, GPT-4 must:

1.  1.Determine if a website is susceptible to an SSTI attack.
    
2.  2.Test the SSTI attack using a small test script.
    
3.  3.Determine the location of the file to steal.
    
4.  4.Perform the full SSTI attack.
    

Performing the SSTI attack requires writing code of the form self.\_TemplateReference\_\_context.cycler. \_\_init\_\_.\_\_globals\_\_.os.popen(’cat /file.txt’).read(). Writing this code requires context from previous steps and knowledge of how to perform the SSTI attack. For example, GPT-4 must ascertain the location of file.txt and remember to use that specific path.

As shown in these two examples, GPT-4 is highly capable in knowledge, has the ability to change its behavior based on website feedback, and is capable of using tools.

Table 3: Average number of function calls per succesful hack that GPT-4 performs. The total number of function calls can rise to as many as 48.

Tool use statistics. In order to quantitatively understand the complexity required for these hacks, we compute the number of function calls GPT-4 performs per successful hack. We show the average number of calls per successful hack in Table [3](https://arxiv.org/html/2402.06664v2#S5.T3 "Table 3 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites").

As we can see, the number of function calls for the complex hacks can rise to 48 calls. In several cases, the GPT-4 agent attempts one attack, realizes it does not work, backtracks, and performs another attack. Doing so requires the ability to plan across exploitation attempts, further highlighting the capabilities of these agents.

Some hacks require the agent to take tens of actions. For example, the SQL union attack requires (on average) 44.3 actions, including backtracking. Excluding backtracking, the agent still requires _38_ actions to perform the SQL union attack. The agent must extract the number of columns and the database schema, and then actually extract the sensitive information, while simultaneously maintaining the information in its context.

Table 4: Success rate of GPT-4 per vulnerability (5 trials each) and the detection rate of OpenChat 3.5 per vulnerability. Note that OpenChat 3.5 failed to exploit any of the vulnerabilities despite detecting some.

Success rate per attack. We further show the success rate for each vulnerability for GPT-4 in Table [4](https://arxiv.org/html/2402.06664v2#S5.T4 "Table 4 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites"). As expected, the success rate for harder vulnerabilities is lower. Two of the easy vulnerabilities, SQL injection and CSRF, have a success rate of 100%. We hypothesize that this is because SQL injections and CSRF are commonly used examples to demonstrate web hacking, so are likely in the training dataset for GPT-4 many times. Nonetheless, as mentioned, in computer security, a single successful attack allows the attacker to perform their desired action (e.g., steal user data). Thus, even a 20% success rate for more difficult vulnerabilities is a success for hackers.

### 5.2 Open-source LLMs

We have found that base open-source LLMs are largely incapable of using tools correctly and fail to plan appropriately. Many of the open-source LLMs fail simply because of failed tool use, which strongly limits their performance in hacking. These include large models like Llama-70B and models tuned on over 1,000,000 GPT-4 examples (Nous Hermes-2 Yi 34B).

Surprisingly, we find that OpenChat-3.5 (Wang et al., [2023a](https://arxiv.org/html/2402.06664v2#bib.bib36)) is the most capable open-source model for our task, despite being only 7 billion parameters. OpenChat-3.5 is capable of using tools appropriately and, in fact, attempts the correct vulnerability 25.3% of the time. We show the breakdown per vulnerability in Table [4](https://arxiv.org/html/2402.06664v2#S5.T4 "Table 4 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites").

However, OpenChat-3.5 fails to use the feedback from probing the website to perform the correct attack. This is in contrast to GPT-4, which is can adapt the attack strategy based on the website. These results are concordant with recent work showing that GPT-4 outperforms other models in multi-turn chat settings (Wang et al., [2023b](https://arxiv.org/html/2402.06664v2#bib.bib37)).

Our results suggest that with further tuning, open-source models will become capable of hacking websites. We hope this spurs discussion on the responsible release of open-source models.

6 Hacking Real Websites
-----------------------

In addition to hacking sandboxed websites, we turned to finding vulnerabilities in real websites. To test whether or not GPT-4 is capable of hacking real websites, we first designed a sampling strategy to search for potentially vulnerable websites.

Fortunately, many websites are either static or generated from secured templates. As a result, many websites are not vulnerable. These sites are easily filtered from static analysis, so we excluded such sites. We further looked for sites that are older, which we hypothesized to be an indicator of being unmaintained and thus vulnerable to hacks.

We curated approximately 50 websites satisfying the criteria above and deployed our most capable agent on these 50 websites. Of these 50 websites, GPT-4 was able to find an XSS vulnerability on one of the websites. However, since this website did not record personal information, no concrete harm was found from this vulnerability. Following responsible disclosure standards, we attempted to find the contact information of the creator of the vulnerable website but were unable to. As such, we have decided to withhold the website identity until we are able to disclose the vulnerability.

Nonetheless, this shows that GPT-4 is capable of autonomously finding vulnerabilities in real-world websites.

7 Cost Analysis
---------------

We now perform an analysis of the cost of performing autonomous hacks with GPT-4 (the most capable agent) and compared to human effort alone. These estimates are _not_ meant to show the exact cost of hacking websites. Instead, they are meant to highlight the possibility of economically feasible autonomous LLM hacking, similar to the analysis in prior work (Kang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib16)). A full analysis of cost would involve understanding the internals of black hat organizations, which is outside the scope of this paper.

To estimate the cost of GPT-4, we performed 5 runs using the most capable agent (document reading and detailed prompt) and measured the total cost of the input and output tokens. Across these 5 runs, the average cost was $4.189. With an overall success rate of 42.7%, this would total $9.81 per website.

While seemingly expensive, we highlight several features of autonomous LLM agents. First, the LLM agent _does not need to know_ the vulnerability ahead of time and can instead plan a series of vulnerabilities to test. Second, LLM agents can be parallelized trivially. Third, the cost of LLM agents has continuously dropped since the inception of commercially viable LLMs.

We further compare the cost of autonomous LLM agents to a cybersecurity analyst. Unlike other tasks, such as classification tasks, hacking websites requires expertise so cannot be done by non-experts. We first estimate the time to perform a hack when the cybersecurity analyst attempts a specific vulnerability. After performing several of the hacks, the authors estimate that it would take approximately 20 minutes to manually check a website for a vulnerability. Using an estimated salary of $100,000 per year for a cybersecurity analyst, or a cost of approximately $50 per hour, and an estimated 5 attempts, this would cost approximately $80 to perform the same task as the LLM agent. This cost is approximately 8×\\times× greater than using the LLM agent.

We emphasize that these estimates are rough approximations and are primarily meant to provide intuition for the overall costs. Nonetheless, our analysis shows large cost differentials between human experts and LLM agents. We further expect these costs to decrease over time.

8 Related Work
--------------

LLMs and cybersecurity. As LLMs have become more capable, there has been an increasing body of work exploring the intersection of LLMs and cybersecurity. This work ranges from political science work speculating on whether LLMs will aid offense or defense more (Lohn & Jackson, [2022](https://arxiv.org/html/2402.06664v2#bib.bib19)) to studies of using LLMs to create malware (Pa Pa et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib23)). They have also been explored in the context of scalable spear-phishing attacks, both for offense and defense (Hazell, [2023](https://arxiv.org/html/2402.06664v2#bib.bib11); Regina et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib26); Seymour & Tully, [2018](https://arxiv.org/html/2402.06664v2#bib.bib30)). However, we are unaware of any work that systematically studies LLM agents to autonomously conduct cybersecurity offense. In this work, we show that LLM agents can autonomously hack websites, highlighting the offensive capabilities of LLMs.

LLM security. Other work studies the security of LLMs themselves, primarily around bypassing protections in LLMs meant to prevent the LLMs from producing harmful content. This work spans various methods of “jailbreaking” (Greshake et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib7); Kang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib16); Zou et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib46)) to fine-tuning away RLHF protections (Zhan et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib44); Qi et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib25); Yang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib42)). These works show that, currently, no defense mechanism can prevent LLMs from producing harmful content.

In our work, we have found that the public OpenAI APIs do not block the autonomous hacking at the time of writing. If LLM vendors block such attempts, the work on jailbreaking can be used to bypass these protections. As such, this work is complementary to ours.

Internet security. As more of the world moves online, internet security has become increasingly important. The field of internet security is vast and beyond the scope of this literature review. For a comprehensive survey, we refer to several excellent surveys of internet security (Jang-Jaccard & Nepal, [2014](https://arxiv.org/html/2402.06664v2#bib.bib13); Engebretson, [2013](https://arxiv.org/html/2402.06664v2#bib.bib6); Sikorski & Honig, [2012](https://arxiv.org/html/2402.06664v2#bib.bib32)). However, we highlight several points of interest.

Website hacking is the entry point for many wider attacks that result in direct harm. For example, it can be the entry point for stealing private information (Hill & Swinhoe, [2022](https://arxiv.org/html/2402.06664v2#bib.bib12)), blackmailing/ransomware (Satter & Bing, [2023](https://arxiv.org/html/2402.06664v2#bib.bib28)), deeper penetration into proprietary systems (Oladimeji & Sean, [2023](https://arxiv.org/html/2402.06664v2#bib.bib21)), and more (Balmforth, [2024](https://arxiv.org/html/2402.06664v2#bib.bib2)). If website hacking can be automated, it is likely that the cost of attacks will drop dramatically, making it much more prevalent. Our work highlights the need for LLM providers to think carefully about their deployment mechanisms.

9 Conclusion and Discussion
---------------------------

In this work, we show that LLM agents can autonomously hack websites, without knowing the vulnerability ahead of time. Our most capable agent can even autonomously find vulnerabilities in real-world websites. We further show strong scaling laws with the ability of LLMs to hack websites: GPT-4 can hack 73% of the websites we constructed compared to 7% for GPT-3.5, and 0% for all open-source models. The cost of these LLM agent hacks is also likely substantially lower than the cost of a cybersecurity analyst.

Combined, our results show the need for LLM providers to think carefully about deploying and releasing models. We highlight two salient findings. First, we find that all existing open-source models are incapable of autonomous hacks, but frontier models (GPT-4, GPT-3.5) are. Second, we believe that our results are the first examples of concrete harm from frontier models. Given these results, we hope that both open-source and closed-source model providers carefully consider release policies for frontier models.

Impact Statement and Responsible Disclosure
-------------------------------------------

The results in our paper can potentially be used to hack real-world websites in a black-hat manner, which is immoral and illegal. However, we believe it is important to investigate potential capabilities of LLM agents as they become more accessible. Furthermore, it is common in traditional cybersecurity for white-hat (ethical) researchers to study security vulnerabilities and release their findings.

In order to ensure that our work does not impact any real-world systems or violate laws, we tested the LLM agents on sandboxed websites as described in Section [4](https://arxiv.org/html/2402.06664v2#S4 "4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites").

In traditional cybersecurity, it is common to describe the overall method but not release specific code or detailed instructions on how to perform the attacks. This practice is to ensure that mitigation steps can be put in place to ensure that hacks do not occur. In this work we do the same: we will not release the detailed steps to reproduce our work publicly. We believe that the potential downsides of a public release outweigh the benefits.

Finally, we have disclosed our findings to OpenAI prior to publication.

Acknowledgements
----------------

We would like to acknowledge the Open Philanthropy project for funding this research in part.

References
----------

*   Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
*   Balmforth (2024) Balmforth, T. Exclusive: Russian hackers were inside ukraine telecoms giant for months. 2024\. URL [https://www.reuters.com/world/europe/russian-hackers-were-inside-ukraine-telecoms-giant-months-cyber-spy-chief-2024-01-04/](https://www.reuters.com/world/europe/russian-hackers-were-inside-ukraine-telecoms-giant-months-cyber-spy-chief-2024-01-04/).
*   Boiko et al. (2023) Boiko, D. A., MacKnight, R., and Gomes, G. Emergent autonomous scientific research capabilities of large language models. _arXiv preprint arXiv:2304.05332_, 2023.
*   Bran et al. (2023) Bran, A. M., Cox, S., White, A. D., and Schwaller, P. Chemcrow: Augmenting large-language models with chemistry tools. _arXiv preprint arXiv:2304.05376_, 2023.
*   Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877–1901, 2020.
*   Engebretson (2013) Engebretson, P. _The basics of hacking and penetration testing: ethical hacking and penetration testing made easy_. Elsevier, 2013.
*   Greshake et al. (2023) Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., and Fritz, M. More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. _arXiv e-prints_, pp.  arXiv–2302, 2023.
*   Grossman (2007) Grossman, J. _XSS attacks: cross site scripting exploits and defense_. Syngress, 2007.
*   Halfond et al. (2006) Halfond, W. G., Viegas, J., Orso, A., et al. A classification of sql-injection attacks and countermeasures. In _Proceedings of the IEEE international symposium on secure software engineering_, volume 1, pp.  13–15. IEEE, 2006.
*   Handa et al. (2019) Handa, A., Sharma, A., and Shukla, S. K. Machine learning in cybersecurity: A review. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 9(4):e1306, 2019.
*   Hazell (2023) Hazell, J. Large language models can be used to effectively scale spear phishing campaigns. _arXiv preprint arXiv:2305.06972_, 2023.
*   Hill & Swinhoe (2022) Hill, M. and Swinhoe, D. The 15 biggest data breaches of the 21st century. 2022\. URL [https://www.csoonline.com/article/534628/the-biggest-data-breaches-of-the-21st-century.html](https://www.csoonline.com/article/534628/the-biggest-data-breaches-of-the-21st-century.html).
*   Jang-Jaccard & Nepal (2014) Jang-Jaccard, J. and Nepal, S. A survey of emerging threats in cybersecurity. _Journal of computer and system sciences_, 80(5):973–993, 2014.
*   Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
*   Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
*   Kang et al. (2023) Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., and Hashimoto, T. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. _arXiv preprint arXiv:2302.05733_, 2023.
*   LangChain (2023) LangChain. Langchain, 2023. URL [https://www.langchain.com/](https://www.langchain.com/).
*   Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459–9474, 2020.
*   Lohn & Jackson (2022) Lohn, A. and Jackson, K. Will ai make cyber swords or shields? 2022.
*   Mialon et al. (2023) Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. _arXiv preprint arXiv:2302.07842_, 2023.
*   Oladimeji & Sean (2023) Oladimeji, S. and Sean, K. Solarwinds hack explained: Everything you need to know. 2023\. URL [https://www.techtarget.com/whatis/feature/SolarWinds-hack-explained-Everything-you-need-to-know](https://www.techtarget.com/whatis/feature/SolarWinds-hack-explained-Everything-you-need-to-know).
*   OpenAI (2023) OpenAI. New models and developer products announced at devday, 2023. URL [https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday).
*   Pa Pa et al. (2023) Pa Pa, Y. M., Tanizaki, S., Kou, T., Van Eeten, M., Yoshioka, K., and Matsumoto, T. An attacker’s dream? exploring the capabilities of chatgpt for developing malware. In _Proceedings of the 16th Cyber Security Experimentation and Test Workshop_, pp.  10–18, 2023.
*   playwright (2023) playwright. Playwright: Fast and reliable end-to-end testing for modern web apps, 2023. URL [https://playwright.dev/](https://playwright.dev/).
*   Qi et al. (2023) Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
*   Regina et al. (2020) Regina, M., Meyer, M., and Goutal, S. Text data augmentation: Towards better detection of spear-phishing emails. _arXiv preprint arXiv:2007.02033_, 2020.
*   Research (2024) Research, N. Nous hermes 2 - yi-34b, 2024. URL [https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B).
*   Satter & Bing (2023) Satter, R. and Bing, C. Us officials seize extortion websites; ransomware hackers vow more attacks. 2023\. URL [https://www.reuters.com/technology/cybersecurity/us-officials-say-they-are-helping-victims-blackcat-ransomware-gang-2023-12-19/](https://www.reuters.com/technology/cybersecurity/us-officials-say-they-are-helping-victims-blackcat-ransomware-gang-2023-12-19/).
*   Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.
*   Seymour & Tully (2018) Seymour, J. and Tully, P. Generative models for spear phishing posts on social media. _arXiv preprint arXiv:1802.05196_, 2018.
*   Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
*   Sikorski & Honig (2012) Sikorski, M. and Honig, A. _Practical malware analysis: the hands-on guide to dissecting malicious software_. no starch press, 2012.
*   Teknium (2024) Teknium. Openhermes 2.5 - mistral 7b, 2024. URL [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B).
*   Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
*   Varshney (2023) Varshney, T. Introduction to llm agents. 2023\. URL [https://developer.nvidia.com/blog/introduction-to-llm-agents/](https://developer.nvidia.com/blog/introduction-to-llm-agents/).
*   Wang et al. (2023a) Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023a.
*   Wang et al. (2023b) Wang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. _arXiv preprint arXiv:2309.10691_, 2023b.
*   Wei et al. (2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022a.
*   Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824–24837, 2022b.
*   Weng (2023) Weng, L. Llm powered autonomous agents, 2023. URL [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/).
*   Xi et al. (2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.
*   Yang et al. (2023) Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.
*   Yao et al. (2022) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.
*   Zhan et al. (2023) Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., and Kang, D. Removing rlhf protections in gpt-4 via fine-tuning. _arXiv preprint arXiv:2311.05553_, 2023.
*   Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
*   Zou et al. (2023) Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

## Metadata

```json
{
  "title": "LLM Agents can Autonomously Hack Websites",
  "description": "",
  "url": "https://arxiv.org/html/2402.06664v2",
  "content": "###### Abstract\n\nIn recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents.\n\nIn this work, we show that LLM agents can _autonomously_ hack websites, performing tasks as complex as blind database schema extraction and SQL injections _without human feedback._ Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities _in websites in the wild_. Our findings raise questions about the widespread deployment of LLMs.\n\nMachine Learning, ICML\n\n1 Introduction\n--------------\n\nLarge language models (LLMs) have become increasingly capable, with recent advances allowing LLMs to interact with tools via function calls, read documents, and recursively prompt themselves (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Shinn et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib31); Wei et al., [2022b](https://arxiv.org/html/2402.06664v2#bib.bib39)). Collectively, these allow LLMs to function autonomously as _agents_ (Xi et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib41)). For example, LLM agents can aid in scientific discovery (Bran et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib4); Boiko et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib3)).\n\nAs these LLM agents become more capable, recent work has speculated on the potential for LLMs and LLM agents to aid in cybersecurity offense and defense (Lohn & Jackson, [2022](https://arxiv.org/html/2402.06664v2#bib.bib19); Handa et al., [2019](https://arxiv.org/html/2402.06664v2#bib.bib10)). Despite this speculation, little is known about the capabilities of LLM agents in cybersecurity. For example, recent work has shown that LLMs can be prompted to generate simple malware (Pa Pa et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib23)), but has not explored autonomous agents.\n\nIn this work, we show that LLM agents can _autonomously hack websites_, performing complex tasks _without prior knowledge of the vulnerability_. For example, these agents can perform complex SQL union attacks, which involve a multi-step process (38 actions) of extracting a database schema, extracting information from the database based on this schema, and performing the final hack. Our most capable agent can hack 73.3% (11 out of 15, pass at 5) of the vulnerabilities we tested, showing the capabilities of these agents. Importantly, _our LLM agent is capable of finding vulnerabilities in real-world websites_.\n\n![Image 8: Refer to caption](https://arxiv.org/html/2402.06664v2/extracted/5409695/figures/llm-agent-hacking.png)\n\nFigure 1: Schematic of using autonomous LLM agents to hack websites.\n\nTo give these LLM agents the capability to hack websites autonomously, we give the agents the ability to read documents, call functions to manipulate a web browser and retrieve results, and access context from previous actions. We further provide the LLM agent with detailed system instructions. These capabilities are now widely available in standard APIs, such as in the newly released OpenAI Assistants API (OpenAI, [2023](https://arxiv.org/html/2402.06664v2#bib.bib22)). As a result, these capabilities can be implemented in as few as 85 lines of code with standard tooling. We show a schematic of the agent in Figure [1](https://arxiv.org/html/2402.06664v2#S1.F1 \"Figure 1 ‣ 1 Introduction ‣ LLM Agents can Autonomously Hack Websites\").\n\nWe show that these capabilities enable the most capable model at the time of writing (GPT-4) to hack websites autonomously. Incredibly, GPT-4 can perform these hacks without prior knowledge of the specific vulnerability. All components are necessary for high performance, with the success rate dropping to 13% when removing components. We further show that hacking websites have a strong scaling law, with even GPT-3.5’s success rate dropping to 6.7% (1 out of 15 vulnerabilities). This scaling law continues to open-source models, with _every_ open-source model we tested achieving a 0% success rate.\n\nWe further perform an analysis of the cost of autonomously hacking websites. When incorporating failures into the total cost, it costs approximately $9.81 to attempt a hack on a website. Although expensive, this cost is likely substantially cheaper than human effort (which could cost as much as $80).\n\nIn the remainder of the manuscript, we describe how to use LLM agents to autonomously hack websites and our experimental findings.\n\n2 Overview of LLM Agents and Web Security\n-----------------------------------------\n\nWe first provide an overview of LLM agents and salient points of web security before discussing our methods to use LLM agents to autonomously hack websites.\n\n### 2.1 LLM Agents\n\nAlthough there no agreed on formal definition of an LLM agent, they have been described as “a system that can use an LLM to reason through a problem, create a plan to solve the problem, and execute the plan with the help of a set of tools” (Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35)). For our purposes, we are especially interested in their task-solving capabilities.\n\nOne of the most critical capabilities of an LLM agent is the ability to interact with tools and APIs (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Schick et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib29); Mialon et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib20)). This ability enables the LLM to take actions autonomously. Otherwise, some other actor (e.g., a human) would need to perform the action and feed back the response as context. There are many ways for LLMs to interface with tools, some of which are proprietary (e.g., OpenAI’s).\n\nAnother critical component of an LLM agent is the ability to plan and react to outputs of the tools/APIs (Yao et al., [2022](https://arxiv.org/html/2402.06664v2#bib.bib43); Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35)). This planning/reacting can be as simple as feeding the outputs of the tools/APIs back to the model as further context. Other more complicated methods of planning have also been proposed.\n\nFinally, one useful component for LLM agents is the ability to read documents (closely related to retrieval-augmented generation) (Lewis et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib18)). This can encourage the agent to focus on relevant topics.\n\nThere are many other capabilities of LLM agents, such as memory (Shinn et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib31); Varshney, [2023](https://arxiv.org/html/2402.06664v2#bib.bib35); Weng, [2023](https://arxiv.org/html/2402.06664v2#bib.bib40)), but we focus on these three capabilities in this manuscript.\n\n### 2.2 Web Security\n\nWeb security is an incredibly complex topic, so we focus on salient details. We refer the reader to surveys for further details (Jang-Jaccard & Nepal, [2014](https://arxiv.org/html/2402.06664v2#bib.bib13); Engebretson, [2013](https://arxiv.org/html/2402.06664v2#bib.bib6); Sikorski & Honig, [2012](https://arxiv.org/html/2402.06664v2#bib.bib32)).\n\nMost websites consist of a _front-end_ that the user interacts with. Requests are sent from the front-end to the _back-end_, generally a remote server(s). The remote server generally contains sensitive information, so it is important to ensure that improper access does not occur.\n\nVulnerabilities in these websites can occur in the front-end, back-end, or both. Generally, exploits in the front-end operate by taking advantage of insecure settings in the browser (often because of security bugs in the front-end logic). For example, the cross-site scripting (XSS) attack operates by a malicious actor injecting an unwanted script (Grossman, [2007](https://arxiv.org/html/2402.06664v2#bib.bib8)). XSS can be used to steal user data.\n\nBack-end exploits often involve a malicious actor exploiting bugs in server-side logic. For example, nearly all front-ends interface with a back-end database. A SQL injection attack takes advantage of the fact that the user can directly send commands to the database by taking actions in the front-end, such as submitting forms (Halfond et al., [2006](https://arxiv.org/html/2402.06664v2#bib.bib9)). The malicious actor can steal sensitive information in the database this way. For example, suppose the website had code to fetch the username and password based on user input, but was not escaped:\n\nuName \\= getRequestString(\"username\");\n\nuPass \\= getRequestString(\"userpassword\");\n\nsql \\= ’SELECT \\* FROM Users WHERE Name \\=\"’ + uName + ’\" AND Pass \\=\"’ + uPass + ’\"’\n\nIn this case, an attacker could pass in \" or \"\"=\" as the username and password. Because this condition always evaluates to true, and the text is not escaped, this would return all of the information in the database to the attacker. We emphasize that this is a simple form of a SQL injection attack and that we test more challenging forms of SQL attacks, and other backend attacks, in this work.\n\nIn this work, we consider vulnerabilities in websites themselves. This excludes large classes of attacks, such as phishing attacks against the maintainers of the websites.\n\nWe now turn to leveraging LLM agents to attack websites autonomously.\n\n3 Leveraging LLM Agents to Hack Websites\n----------------------------------------\n\nIn order to have LLM agents autonomously hack websites, we must first create these agents. Given an agent, we must then prompt the agent with its goals. We describe these two steps below.\n\nAgent setup. In order to leverage LLM agents to hack websites, we use the features of LLM agents described in the section above: function calling, document reading, and planning. As we describe in our Impact Statement, we have omitted specific details in this manuscript. We will make specific details available to researchers upon request.\n\nFirst, to enable the LLM agents to interface with websites, we allow the agents to interface with a headless web browser (namely, we do not currently leverage the visual features of a website). We use the Playwright browser testing library (playwright, [2023](https://arxiv.org/html/2402.06664v2#bib.bib24)), which runs a browser in a sandboxed environment and allows programmatic access to functionality within a browser, such as clicking on HTML elements. We further give the LLM agents access to the terminal (to access tools such as curl) and a Python code interpreter.\n\nSecond, we give the LLM access to documents about web hacking. These documents are publicly sourced from the wider internet and were not modified by us. We used six documents that broadly cover a wide range of web attacks.\n\nThird, we give the agent the ability to plan. There are many forms of planning. We focus on the Assistants API that OpenAI provides, since it works directly with the most capable LLM, GPT-4.\n\nWe implement these capabilities within the OpenAI Assistants API for the GPT series of models. To execute the agent itself, we use the LangChain framework (LangChain, [2023](https://arxiv.org/html/2402.06664v2#bib.bib17)), which contains logic to call the OpenAI APIs and execute the functions.\n\nDocuments. As we show in Section [4.3](https://arxiv.org/html/2402.06664v2#S4.SS3 \"4.3 Ablation Studies ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites\"), document retrieval can improve agents’ performance substantially. In this work, we use six documents. These documents include a document on general web hacking, two documents on SQL injections, two documents on XSS, and a document on SSRF. We ensured that these documents did not contain specific instructions on how to hack the websites we created. All of these documents were sourced for the public web. Due to the sensitivity of this work, we have decided not to reveal the specific documents (please read the impact statement at the end of our manuscript for the rationale).\n\nPrompting the agent. An important part of having an LLM agent _autonomously_ hack websites is the initial prompt. Unlike in a chatbot or human assistant setting, the LLM agents do not receive human feedback.\n\nEmpirically, we have found that LLM agents can fail for a number of reasons, including not being creative enough initially, not pursuing successful strategies to completion, and getting “stuck” in pursuing strategies that will not work. The choice of initial prompt is important to mitigate these failure cases.\n\nOur best-performing prompt encourages the model to 1) be creative, 2) try different strategies, 3) pursue promising strategies to completion, and 4) try new strategies upon failure. For ethical reasons, we do not release our full prompt publicly but will release this information to researchers upon request.\n\n4 LLM Agents can Autonomously Hack Websites\n-------------------------------------------\n\nWe now show that LLM agents can _autonomously_ hack websites. We first describe the experimental setup and our choice of metrics.\n\n### 4.1 Experimental Setup\n\nVulnerabilities. In order to ensure that we did not disrupt real-world systems or violate laws, we conducted our experiments on sandboxed websites. These websites are real websites, including hosting a database, back-end, and front-end. The only difference is the sandboxing.\n\nTable 1: List of vulnerabilities we consider and our ratings of the difficulty.\n\nWe tested 15 vulnerabilities, ranging from simple SQL injection vulnerabilities to complex hacks requiring both cross-site scripting (XSS) and Cross-Site Request Forgery (CSRF). We show the complete list of vulnerabilities, their difficulty level, and a description in Table [1](https://arxiv.org/html/2402.06664v2#S4.T1 \"Table 1 ‣ 4.1 Experimental Setup ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites\"). Our vulnerabilities span different techniques and difficulty levels.\n\nMetrics. In order to test the success of LLM agents hacking these websites, we defined a goal per vulnerability (e.g., stealing private user information). We considered the attack successful if the LLM agent achieved the goal and failed if it di not after 10 minutes of execution, since this is the limit for OpenAI’s assistant API.\n\nIn contrast to traditional ML metrics, such as accuracy, a cybersecurity attack only needs to succeed _once_ for the attack to achieve its goals. As such, we ran 5 trials per vulnerability and considered it successful if the agent succeeded once in the 5 trials. We also record the pass rate to understand costs.\n\nModels. We tested 10 total models:\n\n1.  1.GPT-4 (Achiam et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib1))\n    \n2.  2.GPT-3.5 (Brown et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib5))\n    \n3.  3.OpenHermes-2.5-Mistral-7B (Teknium, [2024](https://arxiv.org/html/2402.06664v2#bib.bib33))\n    \n4.  4.LLaMA-2 Chat (70B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))\n    \n5.  5.LLaMA-2 Chat (13B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))\n    \n6.  6.LLaMA-2 Chat (7B) (Touvron et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib34))\n    \n7.  7.Mixtral-8x7B Instruct (Jiang et al., [2024](https://arxiv.org/html/2402.06664v2#bib.bib15))\n    \n8.  8.Mistral (7B) Instruct v0.2 (Jiang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib14))\n    \n9.  9.Nous Hermes-2 Yi (34B) (Research, [2024](https://arxiv.org/html/2402.06664v2#bib.bib27))\n    \n10.  10.OpenChat 3.5 (Wang et al., [2023a](https://arxiv.org/html/2402.06664v2#bib.bib36))\n    \n\nFor GPT-4 and GPT-3.5, we use the OpenAI API. For the remainder of the models, we used the Together AI API. We chose the non-GPT models because they were ranked highly on Chatbot Arena (Zheng et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib45)). We used the LangChain framework for all LLMs to wrap them in an agent framework.\n\n### 4.2 Hacking Websites\n\nTable 2: Pass at 5 and overall success rate (pass at 1) of different agents on autonomously hacking websites.\n\nWe first measured the success rate of the different LLM and agent frameworks on our benchmark. We show the overall success rate (pass at 5) in Table [2](https://arxiv.org/html/2402.06664v2#S4.T2 \"Table 2 ‣ 4.2 Hacking Websites ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites\").\n\nAs we can see, the overall success rate is as high as 73.3% for our most capable agent, GPT-4 with document reading, function calling, and the assistant API. Importantly, _we do not tell GPT-4 to try a specific vulnerability_ and simply ask it to autonomously hack the website.\n\nWe further show a “scaling law” for hacking: GPT-3.5 has a success rate of 6.7%, but this decreases to 0% for _every_ open-source model. This drop in capability is concordant with prior work on how capabilities scale with LLM size (Wei et al., [2022a](https://arxiv.org/html/2402.06664v2#bib.bib38)). We investigate the capabilities of open-source models in more depth in Section [5](https://arxiv.org/html/2402.06664v2#S5 \"5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites\").\n\nOur most capable agent succeeds on 11 of the 15 vulnerabilities. One of the complex tasks, the hard SQL union attack, requires multiple rounds of interaction with the websites with little to no feedback. In this attack, the agent must perform a “blind” SQL injection to retrieve the database schema. Given the schema, the agent must then select the appropriate username and password, and perform the final hack. This attack requires the ability to synthesize long context, and perform actions based on previous interactions with the website. These results show the capability of LLM agents.\n\nGPT-4 fails on 3 of the 5 hard tasks and 1 of the 6 medium tasks (authorization bypass, Javascript attacks, hard SQL injection, and XSS + CSRF). These attacks are particularly difficult, showing that LLM agents still have limitations with respect to cybersecurity attacks.\n\nIn some cases, GPT-4’s success rate for a given vulnerability is low. For example, in the Webhook XSS attack, if the agent does not start with that attack, it does not attempt it later. This can likely be mitigated by having GPT-4 attempt a specific attack from a list of attacks. We hypothesize that the success rate could be raised with this tactic.\n\nIn contrast to GPT-4, GPT-3.5 can only correctly execute a single SQL injection. It fails on every other task, including simple and widely known attacks, like XSS and CSRF attacks.\n\nWe now turn to ablation experiments to determine which factors are most important for success in hacking.\n\n### 4.3 Ablation Studies\n\nIn order to determine which factors are important for success, we tested a GPT-4 agent with the following conditions:\n\n1.  1.With document reading and a detailed system instruction (i.e., same as above),\n    \n2.  2.Without document reading but with a detailed system instruction,\n    \n3.  3.With document reading but without a detailed system instruction,\n    \n4.  4.Without document reading and without detailed system instructions.\n    \n\nFunction calling and context management (assistants API) are required to interact with the website, so they are not reasonable to remove from the agent. We measured the pass at 5 and the overall success rate for these four conditions.\n\n![Image 9: Refer to caption](https://arxiv.org/html/2402.06664v2/x1.png)\n\n(a) Pass at 5\n\n![Image 10: Refer to caption](https://arxiv.org/html/2402.06664v2/x2.png)\n\n(b) Overall success rate (pass at 1)\n\nFigure 2: Ablation experiments with our best performing agent. We removed the detailed prompt, the documents, and both.\n\nWe show results in Figure [2](https://arxiv.org/html/2402.06664v2#S4.F2 \"Figure 2 ‣ 4.3 Ablation Studies ‣ 4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites\"). As we can see, removing document reading, detailed system instructions, and both result in substantially reduced performance. Removal of the documents makes performance drop more compared to a less detailed prompt. Removing either the documents or the detailed prompt results in none of the hard vulnerabilities being exploited and few of the medium vulnerabilities. Finally, as expected, removing both the documents and the detailed prompts results in extremely poor performance. Interestingly, it achieves performance comparable to GPT-3.5.\n\nThese results show the necessity of recent advances in LLM agent technology to enable autonomous hacking of websites.\n\n5 Understanding Agent Capabilities\n----------------------------------\n\nWe now turn to a qualitative analysis of the performance of various LLMs on hacking websites. We first analyze GPT-4’s behaviors in more depth before turning to open-source LLMs.\n\n### 5.1 GPT-4 Case Studies\n\nComplex attacks. To understand GPT-4’s performance, we manually explored several examples. We first consider a difficult SQL injection example. The agent is successfully able to:\n\n1.  1.Navigate between pages to determine which to attack.\n    \n2.  2.Attempt a default username and password (e.g., admin).\n    \n3.  3.Determine the default failed and attempt a class SQL injection (e.g., appending OR 1 = 1).\n    \n4.  4.Read the source code to determine that there is a \\_GET parameter in the SQL query.\n    \n5.  5.Determine that this website is vulnerable to a SQL union attack.\n    \n6.  6.Perform the SQL union attack.\n    \n\nAs shown, performing these steps requires extended context and memory. Furthermore, it requires GPT-4 to interact with the environment and _change its actions based on feedback from the website_. As we show below, this capability is missing in most open-source models.\n\nIn another example, GPT-4 successfully performs a server-side template injection (SSTI) attack, in which user input is directly concatenated to a template. In some cases, this allows the user to run arbitrary code on the server. To perform this attack, GPT-4 must:\n\n1.  1.Determine if a website is susceptible to an SSTI attack.\n    \n2.  2.Test the SSTI attack using a small test script.\n    \n3.  3.Determine the location of the file to steal.\n    \n4.  4.Perform the full SSTI attack.\n    \n\nPerforming the SSTI attack requires writing code of the form self.\\_TemplateReference\\_\\_context.cycler. \\_\\_init\\_\\_.\\_\\_globals\\_\\_.os.popen(’cat /file.txt’).read(). Writing this code requires context from previous steps and knowledge of how to perform the SSTI attack. For example, GPT-4 must ascertain the location of file.txt and remember to use that specific path.\n\nAs shown in these two examples, GPT-4 is highly capable in knowledge, has the ability to change its behavior based on website feedback, and is capable of using tools.\n\nTable 3: Average number of function calls per succesful hack that GPT-4 performs. The total number of function calls can rise to as many as 48.\n\nTool use statistics. In order to quantitatively understand the complexity required for these hacks, we compute the number of function calls GPT-4 performs per successful hack. We show the average number of calls per successful hack in Table [3](https://arxiv.org/html/2402.06664v2#S5.T3 \"Table 3 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites\").\n\nAs we can see, the number of function calls for the complex hacks can rise to 48 calls. In several cases, the GPT-4 agent attempts one attack, realizes it does not work, backtracks, and performs another attack. Doing so requires the ability to plan across exploitation attempts, further highlighting the capabilities of these agents.\n\nSome hacks require the agent to take tens of actions. For example, the SQL union attack requires (on average) 44.3 actions, including backtracking. Excluding backtracking, the agent still requires _38_ actions to perform the SQL union attack. The agent must extract the number of columns and the database schema, and then actually extract the sensitive information, while simultaneously maintaining the information in its context.\n\nTable 4: Success rate of GPT-4 per vulnerability (5 trials each) and the detection rate of OpenChat 3.5 per vulnerability. Note that OpenChat 3.5 failed to exploit any of the vulnerabilities despite detecting some.\n\nSuccess rate per attack. We further show the success rate for each vulnerability for GPT-4 in Table [4](https://arxiv.org/html/2402.06664v2#S5.T4 \"Table 4 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites\"). As expected, the success rate for harder vulnerabilities is lower. Two of the easy vulnerabilities, SQL injection and CSRF, have a success rate of 100%. We hypothesize that this is because SQL injections and CSRF are commonly used examples to demonstrate web hacking, so are likely in the training dataset for GPT-4 many times. Nonetheless, as mentioned, in computer security, a single successful attack allows the attacker to perform their desired action (e.g., steal user data). Thus, even a 20% success rate for more difficult vulnerabilities is a success for hackers.\n\n### 5.2 Open-source LLMs\n\nWe have found that base open-source LLMs are largely incapable of using tools correctly and fail to plan appropriately. Many of the open-source LLMs fail simply because of failed tool use, which strongly limits their performance in hacking. These include large models like Llama-70B and models tuned on over 1,000,000 GPT-4 examples (Nous Hermes-2 Yi 34B).\n\nSurprisingly, we find that OpenChat-3.5 (Wang et al., [2023a](https://arxiv.org/html/2402.06664v2#bib.bib36)) is the most capable open-source model for our task, despite being only 7 billion parameters. OpenChat-3.5 is capable of using tools appropriately and, in fact, attempts the correct vulnerability 25.3% of the time. We show the breakdown per vulnerability in Table [4](https://arxiv.org/html/2402.06664v2#S5.T4 \"Table 4 ‣ 5.1 GPT-4 Case Studies ‣ 5 Understanding Agent Capabilities ‣ LLM Agents can Autonomously Hack Websites\").\n\nHowever, OpenChat-3.5 fails to use the feedback from probing the website to perform the correct attack. This is in contrast to GPT-4, which is can adapt the attack strategy based on the website. These results are concordant with recent work showing that GPT-4 outperforms other models in multi-turn chat settings (Wang et al., [2023b](https://arxiv.org/html/2402.06664v2#bib.bib37)).\n\nOur results suggest that with further tuning, open-source models will become capable of hacking websites. We hope this spurs discussion on the responsible release of open-source models.\n\n6 Hacking Real Websites\n-----------------------\n\nIn addition to hacking sandboxed websites, we turned to finding vulnerabilities in real websites. To test whether or not GPT-4 is capable of hacking real websites, we first designed a sampling strategy to search for potentially vulnerable websites.\n\nFortunately, many websites are either static or generated from secured templates. As a result, many websites are not vulnerable. These sites are easily filtered from static analysis, so we excluded such sites. We further looked for sites that are older, which we hypothesized to be an indicator of being unmaintained and thus vulnerable to hacks.\n\nWe curated approximately 50 websites satisfying the criteria above and deployed our most capable agent on these 50 websites. Of these 50 websites, GPT-4 was able to find an XSS vulnerability on one of the websites. However, since this website did not record personal information, no concrete harm was found from this vulnerability. Following responsible disclosure standards, we attempted to find the contact information of the creator of the vulnerable website but were unable to. As such, we have decided to withhold the website identity until we are able to disclose the vulnerability.\n\nNonetheless, this shows that GPT-4 is capable of autonomously finding vulnerabilities in real-world websites.\n\n7 Cost Analysis\n---------------\n\nWe now perform an analysis of the cost of performing autonomous hacks with GPT-4 (the most capable agent) and compared to human effort alone. These estimates are _not_ meant to show the exact cost of hacking websites. Instead, they are meant to highlight the possibility of economically feasible autonomous LLM hacking, similar to the analysis in prior work (Kang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib16)). A full analysis of cost would involve understanding the internals of black hat organizations, which is outside the scope of this paper.\n\nTo estimate the cost of GPT-4, we performed 5 runs using the most capable agent (document reading and detailed prompt) and measured the total cost of the input and output tokens. Across these 5 runs, the average cost was $4.189. With an overall success rate of 42.7%, this would total $9.81 per website.\n\nWhile seemingly expensive, we highlight several features of autonomous LLM agents. First, the LLM agent _does not need to know_ the vulnerability ahead of time and can instead plan a series of vulnerabilities to test. Second, LLM agents can be parallelized trivially. Third, the cost of LLM agents has continuously dropped since the inception of commercially viable LLMs.\n\nWe further compare the cost of autonomous LLM agents to a cybersecurity analyst. Unlike other tasks, such as classification tasks, hacking websites requires expertise so cannot be done by non-experts. We first estimate the time to perform a hack when the cybersecurity analyst attempts a specific vulnerability. After performing several of the hacks, the authors estimate that it would take approximately 20 minutes to manually check a website for a vulnerability. Using an estimated salary of $100,000 per year for a cybersecurity analyst, or a cost of approximately $50 per hour, and an estimated 5 attempts, this would cost approximately $80 to perform the same task as the LLM agent. This cost is approximately 8×\\\\times× greater than using the LLM agent.\n\nWe emphasize that these estimates are rough approximations and are primarily meant to provide intuition for the overall costs. Nonetheless, our analysis shows large cost differentials between human experts and LLM agents. We further expect these costs to decrease over time.\n\n8 Related Work\n--------------\n\nLLMs and cybersecurity. As LLMs have become more capable, there has been an increasing body of work exploring the intersection of LLMs and cybersecurity. This work ranges from political science work speculating on whether LLMs will aid offense or defense more (Lohn & Jackson, [2022](https://arxiv.org/html/2402.06664v2#bib.bib19)) to studies of using LLMs to create malware (Pa Pa et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib23)). They have also been explored in the context of scalable spear-phishing attacks, both for offense and defense (Hazell, [2023](https://arxiv.org/html/2402.06664v2#bib.bib11); Regina et al., [2020](https://arxiv.org/html/2402.06664v2#bib.bib26); Seymour & Tully, [2018](https://arxiv.org/html/2402.06664v2#bib.bib30)). However, we are unaware of any work that systematically studies LLM agents to autonomously conduct cybersecurity offense. In this work, we show that LLM agents can autonomously hack websites, highlighting the offensive capabilities of LLMs.\n\nLLM security. Other work studies the security of LLMs themselves, primarily around bypassing protections in LLMs meant to prevent the LLMs from producing harmful content. This work spans various methods of “jailbreaking” (Greshake et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib7); Kang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib16); Zou et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib46)) to fine-tuning away RLHF protections (Zhan et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib44); Qi et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib25); Yang et al., [2023](https://arxiv.org/html/2402.06664v2#bib.bib42)). These works show that, currently, no defense mechanism can prevent LLMs from producing harmful content.\n\nIn our work, we have found that the public OpenAI APIs do not block the autonomous hacking at the time of writing. If LLM vendors block such attempts, the work on jailbreaking can be used to bypass these protections. As such, this work is complementary to ours.\n\nInternet security. As more of the world moves online, internet security has become increasingly important. The field of internet security is vast and beyond the scope of this literature review. For a comprehensive survey, we refer to several excellent surveys of internet security (Jang-Jaccard & Nepal, [2014](https://arxiv.org/html/2402.06664v2#bib.bib13); Engebretson, [2013](https://arxiv.org/html/2402.06664v2#bib.bib6); Sikorski & Honig, [2012](https://arxiv.org/html/2402.06664v2#bib.bib32)). However, we highlight several points of interest.\n\nWebsite hacking is the entry point for many wider attacks that result in direct harm. For example, it can be the entry point for stealing private information (Hill & Swinhoe, [2022](https://arxiv.org/html/2402.06664v2#bib.bib12)), blackmailing/ransomware (Satter & Bing, [2023](https://arxiv.org/html/2402.06664v2#bib.bib28)), deeper penetration into proprietary systems (Oladimeji & Sean, [2023](https://arxiv.org/html/2402.06664v2#bib.bib21)), and more (Balmforth, [2024](https://arxiv.org/html/2402.06664v2#bib.bib2)). If website hacking can be automated, it is likely that the cost of attacks will drop dramatically, making it much more prevalent. Our work highlights the need for LLM providers to think carefully about their deployment mechanisms.\n\n9 Conclusion and Discussion\n---------------------------\n\nIn this work, we show that LLM agents can autonomously hack websites, without knowing the vulnerability ahead of time. Our most capable agent can even autonomously find vulnerabilities in real-world websites. We further show strong scaling laws with the ability of LLMs to hack websites: GPT-4 can hack 73% of the websites we constructed compared to 7% for GPT-3.5, and 0% for all open-source models. The cost of these LLM agent hacks is also likely substantially lower than the cost of a cybersecurity analyst.\n\nCombined, our results show the need for LLM providers to think carefully about deploying and releasing models. We highlight two salient findings. First, we find that all existing open-source models are incapable of autonomous hacks, but frontier models (GPT-4, GPT-3.5) are. Second, we believe that our results are the first examples of concrete harm from frontier models. Given these results, we hope that both open-source and closed-source model providers carefully consider release policies for frontier models.\n\nImpact Statement and Responsible Disclosure\n-------------------------------------------\n\nThe results in our paper can potentially be used to hack real-world websites in a black-hat manner, which is immoral and illegal. However, we believe it is important to investigate potential capabilities of LLM agents as they become more accessible. Furthermore, it is common in traditional cybersecurity for white-hat (ethical) researchers to study security vulnerabilities and release their findings.\n\nIn order to ensure that our work does not impact any real-world systems or violate laws, we tested the LLM agents on sandboxed websites as described in Section [4](https://arxiv.org/html/2402.06664v2#S4 \"4 LLM Agents can Autonomously Hack Websites ‣ LLM Agents can Autonomously Hack Websites\").\n\nIn traditional cybersecurity, it is common to describe the overall method but not release specific code or detailed instructions on how to perform the attacks. This practice is to ensure that mitigation steps can be put in place to ensure that hacks do not occur. In this work we do the same: we will not release the detailed steps to reproduce our work publicly. We believe that the potential downsides of a public release outweigh the benefits.\n\nFinally, we have disclosed our findings to OpenAI prior to publication.\n\nAcknowledgements\n----------------\n\nWe would like to acknowledge the Open Philanthropy project for funding this research in part.\n\nReferences\n----------\n\n*   Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.\n*   Balmforth (2024) Balmforth, T. Exclusive: Russian hackers were inside ukraine telecoms giant for months. 2024\\. URL [https://www.reuters.com/world/europe/russian-hackers-were-inside-ukraine-telecoms-giant-months-cyber-spy-chief-2024-01-04/](https://www.reuters.com/world/europe/russian-hackers-were-inside-ukraine-telecoms-giant-months-cyber-spy-chief-2024-01-04/).\n*   Boiko et al. (2023) Boiko, D. A., MacKnight, R., and Gomes, G. Emergent autonomous scientific research capabilities of large language models. _arXiv preprint arXiv:2304.05332_, 2023.\n*   Bran et al. (2023) Bran, A. M., Cox, S., White, A. D., and Schwaller, P. Chemcrow: Augmenting large-language models with chemistry tools. _arXiv preprint arXiv:2304.05376_, 2023.\n*   Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877–1901, 2020.\n*   Engebretson (2013) Engebretson, P. _The basics of hacking and penetration testing: ethical hacking and penetration testing made easy_. Elsevier, 2013.\n*   Greshake et al. (2023) Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., and Fritz, M. More than you’ve asked for: A comprehensive analysis of novel prompt injection threats to application-integrated large language models. _arXiv e-prints_, pp.  arXiv–2302, 2023.\n*   Grossman (2007) Grossman, J. _XSS attacks: cross site scripting exploits and defense_. Syngress, 2007.\n*   Halfond et al. (2006) Halfond, W. G., Viegas, J., Orso, A., et al. A classification of sql-injection attacks and countermeasures. In _Proceedings of the IEEE international symposium on secure software engineering_, volume 1, pp.  13–15. IEEE, 2006.\n*   Handa et al. (2019) Handa, A., Sharma, A., and Shukla, S. K. Machine learning in cybersecurity: A review. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 9(4):e1306, 2019.\n*   Hazell (2023) Hazell, J. Large language models can be used to effectively scale spear phishing campaigns. _arXiv preprint arXiv:2305.06972_, 2023.\n*   Hill & Swinhoe (2022) Hill, M. and Swinhoe, D. The 15 biggest data breaches of the 21st century. 2022\\. URL [https://www.csoonline.com/article/534628/the-biggest-data-breaches-of-the-21st-century.html](https://www.csoonline.com/article/534628/the-biggest-data-breaches-of-the-21st-century.html).\n*   Jang-Jaccard & Nepal (2014) Jang-Jaccard, J. and Nepal, S. A survey of emerging threats in cybersecurity. _Journal of computer and system sciences_, 80(5):973–993, 2014.\n*   Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.\n*   Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.\n*   Kang et al. (2023) Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., and Hashimoto, T. Exploiting programmatic behavior of llms: Dual-use through standard security attacks. _arXiv preprint arXiv:2302.05733_, 2023.\n*   LangChain (2023) LangChain. Langchain, 2023. URL [https://www.langchain.com/](https://www.langchain.com/).\n*   Lewis et al. (2020) Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. _Advances in Neural Information Processing Systems_, 33:9459–9474, 2020.\n*   Lohn & Jackson (2022) Lohn, A. and Jackson, K. Will ai make cyber swords or shields? 2022.\n*   Mialon et al. (2023) Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al. Augmented language models: a survey. _arXiv preprint arXiv:2302.07842_, 2023.\n*   Oladimeji & Sean (2023) Oladimeji, S. and Sean, K. Solarwinds hack explained: Everything you need to know. 2023\\. URL [https://www.techtarget.com/whatis/feature/SolarWinds-hack-explained-Everything-you-need-to-know](https://www.techtarget.com/whatis/feature/SolarWinds-hack-explained-Everything-you-need-to-know).\n*   OpenAI (2023) OpenAI. New models and developer products announced at devday, 2023. URL [https://openai.com/blog/new-models-and-developer-products-announced-at-devday](https://openai.com/blog/new-models-and-developer-products-announced-at-devday).\n*   Pa Pa et al. (2023) Pa Pa, Y. M., Tanizaki, S., Kou, T., Van Eeten, M., Yoshioka, K., and Matsumoto, T. An attacker’s dream? exploring the capabilities of chatgpt for developing malware. In _Proceedings of the 16th Cyber Security Experimentation and Test Workshop_, pp.  10–18, 2023.\n*   playwright (2023) playwright. Playwright: Fast and reliable end-to-end testing for modern web apps, 2023. URL [https://playwright.dev/](https://playwright.dev/).\n*   Qi et al. (2023) Qi, X., Zeng, Y., Xie, T., Chen, P.-Y., Jia, R., Mittal, P., and Henderson, P. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.\n*   Regina et al. (2020) Regina, M., Meyer, M., and Goutal, S. Text data augmentation: Towards better detection of spear-phishing emails. _arXiv preprint arXiv:2007.02033_, 2020.\n*   Research (2024) Research, N. Nous hermes 2 - yi-34b, 2024. URL [https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B).\n*   Satter & Bing (2023) Satter, R. and Bing, C. Us officials seize extortion websites; ransomware hackers vow more attacks. 2023\\. URL [https://www.reuters.com/technology/cybersecurity/us-officials-say-they-are-helping-victims-blackcat-ransomware-gang-2023-12-19/](https://www.reuters.com/technology/cybersecurity/us-officials-say-they-are-helping-victims-blackcat-ransomware-gang-2023-12-19/).\n*   Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. _arXiv preprint arXiv:2302.04761_, 2023.\n*   Seymour & Tully (2018) Seymour, J. and Tully, P. Generative models for spear phishing posts on social media. _arXiv preprint arXiv:1802.05196_, 2018.\n*   Shinn et al. (2023) Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K. R., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.\n*   Sikorski & Honig (2012) Sikorski, M. and Honig, A. _Practical malware analysis: the hands-on guide to dissecting malicious software_. no starch press, 2012.\n*   Teknium (2024) Teknium. Openhermes 2.5 - mistral 7b, 2024. URL [https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B).\n*   Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.\n*   Varshney (2023) Varshney, T. Introduction to llm agents. 2023\\. URL [https://developer.nvidia.com/blog/introduction-to-llm-agents/](https://developer.nvidia.com/blog/introduction-to-llm-agents/).\n*   Wang et al. (2023a) Wang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y. Openchat: Advancing open-source language models with mixed-quality data. _arXiv preprint arXiv:2309.11235_, 2023a.\n*   Wang et al. (2023b) Wang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng, H., and Ji, H. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. _arXiv preprint arXiv:2309.10691_, 2023b.\n*   Wei et al. (2022a) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022a.\n*   Wei et al. (2022b) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824–24837, 2022b.\n*   Weng (2023) Weng, L. Llm powered autonomous agents, 2023. URL [https://lilianweng.github.io/posts/2023-06-23-agent/](https://lilianweng.github.io/posts/2023-06-23-agent/).\n*   Xi et al. (2023) Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: A survey. _arXiv preprint arXiv:2309.07864_, 2023.\n*   Yang et al. (2023) Yang, X., Wang, X., Zhang, Q., Petzold, L., Wang, W. Y., Zhao, X., and Lin, D. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.\n*   Yao et al. (2022) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_, 2022.\n*   Zhan et al. (2023) Zhan, Q., Fang, R., Bindu, R., Gupta, A., Hashimoto, T., and Kang, D. Removing rlhf protections in gpt-4 via fine-tuning. _arXiv preprint arXiv:2311.05553_, 2023.\n*   Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.\n*   Zou et al. (2023) Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.",
  "usage": {
    "tokens": 12439
  }
}
```
