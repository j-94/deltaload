---
title: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning
description: Abstract page for arXiv paper 2301.13808: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning
url: https://arxiv.org/abs/2301.13808
timestamp: 2025-01-20T15:56:32.882Z
domain: arxiv.org
path: abs_2301.13808
---

# Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning


Abstract page for arXiv paper 2301.13808: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning


## Content

Skip to main content

In just 3 minutes help us improve arXiv:

Annual Global Survey
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
cs
>
arXiv:2301.13808

Help | Advanced Search

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
Computer Science > Computation and Language
[Submitted on 31 Jan 2023 (v1), last revised 27 Apr 2023 (this version, v3)]
Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning
Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li
Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.
Comments:	SIGIR 2023
Subjects:	Computation and Language (cs.CL)
Cite as:	arXiv:2301.13808 [cs.CL]
 	(or arXiv:2301.13808v3 [cs.CL] for this version)
 	
https://doi.org/10.48550/arXiv.2301.13808
Focus to learn more
Submission history
From: Binyuan Hui [view email]
[v1] Tue, 31 Jan 2023 17:51:45 UTC (552 KB)
[v2] Fri, 21 Apr 2023 03:52:32 UTC (550 KB)
[v3] Thu, 27 Apr 2023 11:24:10 UTC (550 KB)

Access Paper:
View PDF
TeX Source
Other Formats
view license
Current browse context:
cs.CL
< prev   |   next >

new | recent | 2023-01
Change to browse by:
cs

References & Citations
NASA ADS
Google Scholar
Semantic Scholar
Export BibTeX Citation
Bookmark
 
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
Connected Papers Toggle
Connected Papers (What is Connected Papers?)
Litmaps Toggle
Litmaps (What is Litmaps?)
scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)
About
Help
Contact
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance

arXiv Operational Status 
Get status notifications via email or slack

## Metadata

```json
{
  "title": "Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning",
  "description": "Abstract page for arXiv paper 2301.13808: Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning",
  "url": "https://arxiv.org/abs/2301.13808",
  "content": "Skip to main content\n\nIn just 3 minutes help us improve arXiv:\n\nAnnual Global Survey\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\nDonate\n>\ncs\n>\narXiv:2301.13808\n\nHelp | Advanced Search\n\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nComputer Science > Computation and Language\n[Submitted on 31 Jan 2023 (v1), last revised 27 Apr 2023 (this version, v3)]\nLarge Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li\nTable-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a \"parsing-execution-filling\" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.\nComments:\tSIGIR 2023\nSubjects:\tComputation and Language (cs.CL)\nCite as:\tarXiv:2301.13808 [cs.CL]\n \t(or arXiv:2301.13808v3 [cs.CL] for this version)\n \t\nhttps://doi.org/10.48550/arXiv.2301.13808\nFocus to learn more\nSubmission history\nFrom: Binyuan Hui [view email]\n[v1] Tue, 31 Jan 2023 17:51:45 UTC (552 KB)\n[v2] Fri, 21 Apr 2023 03:52:32 UTC (550 KB)\n[v3] Thu, 27 Apr 2023 11:24:10 UTC (550 KB)\n\nAccess Paper:\nView PDF\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.CL\n< prev   |   next >\n\nnew | recent | 2023-01\nChange to browse by:\ncs\n\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\n \nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\nAbout\nHelp\nContact\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\n\narXiv Operational Status \nGet status notifications via email or slack",
  "usage": {
    "tokens": 848
  }
}
```
