---
title: FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings
description: Abstract page for arXiv paper 2210.03766: FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings
url: https://arxiv.org/abs/2210.03766
timestamp: 2025-01-20T15:46:40.491Z
domain: arxiv.org
path: abs_2210.03766
---

# FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings


Abstract page for arXiv paper 2210.03766: FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings


## Content

Skip to main content

In just 3 minutes help us improve arXiv:

Annual Global Survey
We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.
Donate
>
cs
>
arXiv:2210.03766

Help | Advanced Search

All fields
Title
Author
Abstract
Comments
Journal reference
ACM classification
MSC classification
Report number
arXiv identifier
DOI
ORCID
arXiv author ID
Help pages
Full text
Search
Computer Science > Computation and Language
[Submitted on 7 Oct 2022]
FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings
Andrew Silva, Pradyumna Tambwekar, Matthew Gombolay
Federated learning is a training paradigm that learns from multiple distributed users without aggregating data on a centralized server. Such a paradigm promises the ability to deploy machine-learning at-scale to a diverse population of end-users without first collecting a large, labeled dataset for all possible tasks. As federated learning typically averages learning updates across a decentralized population, there is a growing need for personalization of federated learning systems (i.e conversational agents must be able to personalize to a specific user's preferences). In this work, we propose a new direction for personalization research within federated learning, leveraging both personal embeddings and shared context embeddings. We also present an approach to predict these ``preference'' embeddings, enabling personalization without backpropagation. Compared to state-of-the-art personalization baselines, our approach achieves a 50\% improvement in test-time perplexity using 0.001\% of the memory required by baseline approaches, and achieving greater sample- and compute-efficiency.
Comments:	Andrew Silva and Pradyumna Tambwekar contributed equally towards this work
Subjects:	Computation and Language (cs.CL); Machine Learning (cs.LG)
Cite as:	arXiv:2210.03766 [cs.CL]
 	(or arXiv:2210.03766v1 [cs.CL] for this version)
 	
https://doi.org/10.48550/arXiv.2210.03766
Focus to learn more
Submission history
From: Pradyumna Tambwekar [view email]
[v1] Fri, 7 Oct 2022 18:01:19 UTC (428 KB)

Access Paper:
View PDF
TeX Source
Other Formats
view license
Current browse context:
cs.CL
< prev   |   next >

new | recent | 2022-10
Change to browse by:
cs
cs.LG

References & Citations
NASA ADS
Google Scholar
Semantic Scholar
Export BibTeX Citation
Bookmark
 
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer (What is the Explorer?)
Connected Papers Toggle
Connected Papers (What is Connected Papers?)
Litmaps Toggle
Litmaps (What is Litmaps?)
scite.ai Toggle
scite Smart Citations (What are Smart Citations?)
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax (What is MathJax?)
About
Help
Contact
Subscribe
Copyright
Privacy Policy
Web Accessibility Assistance

arXiv Operational Status 
Get status notifications via email or slack

## Metadata

```json
{
  "title": "FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings",
  "description": "Abstract page for arXiv paper 2210.03766: FedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings",
  "url": "https://arxiv.org/abs/2210.03766",
  "content": "Skip to main content\n\nIn just 3 minutes help us improve arXiv:\n\nAnnual Global Survey\nWe gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\nDonate\n>\ncs\n>\narXiv:2210.03766\n\nHelp | Advanced Search\n\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nComputer Science > Computation and Language\n[Submitted on 7 Oct 2022]\nFedPC: Federated Learning for Language Generation with Personal and Context Preference Embeddings\nAndrew Silva, Pradyumna Tambwekar, Matthew Gombolay\nFederated learning is a training paradigm that learns from multiple distributed users without aggregating data on a centralized server. Such a paradigm promises the ability to deploy machine-learning at-scale to a diverse population of end-users without first collecting a large, labeled dataset for all possible tasks. As federated learning typically averages learning updates across a decentralized population, there is a growing need for personalization of federated learning systems (i.e conversational agents must be able to personalize to a specific user's preferences). In this work, we propose a new direction for personalization research within federated learning, leveraging both personal embeddings and shared context embeddings. We also present an approach to predict these ``preference'' embeddings, enabling personalization without backpropagation. Compared to state-of-the-art personalization baselines, our approach achieves a 50\\% improvement in test-time perplexity using 0.001\\% of the memory required by baseline approaches, and achieving greater sample- and compute-efficiency.\nComments:\tAndrew Silva and Pradyumna Tambwekar contributed equally towards this work\nSubjects:\tComputation and Language (cs.CL); Machine Learning (cs.LG)\nCite as:\tarXiv:2210.03766 [cs.CL]\n \t(or arXiv:2210.03766v1 [cs.CL] for this version)\n \t\nhttps://doi.org/10.48550/arXiv.2210.03766\nFocus to learn more\nSubmission history\nFrom: Pradyumna Tambwekar [view email]\n[v1] Fri, 7 Oct 2022 18:01:19 UTC (428 KB)\n\nAccess Paper:\nView PDF\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.CL\n< prev   |   next >\n\nnew | recent | 2022-10\nChange to browse by:\ncs\ncs.LG\n\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\nExport BibTeX Citation\nBookmark\n \nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer (What is the Explorer?)\nConnected Papers Toggle\nConnected Papers (What is Connected Papers?)\nLitmaps Toggle\nLitmaps (What is Litmaps?)\nscite.ai Toggle\nscite Smart Citations (What are Smart Citations?)\nCode, Data, Media\nDemos\nRelated Papers\nAbout arXivLabs\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\nAbout\nHelp\nContact\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\n\narXiv Operational Status \nGet status notifications via email or slack",
  "usage": {
    "tokens": 710
  }
}
```
