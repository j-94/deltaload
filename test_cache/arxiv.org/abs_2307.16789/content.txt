Authors:[Yujia Qin](https://arxiv.org/search/cs?searchtype=author&query=Qin,+Y), [Shihao Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+S), [Yining Ye](https://arxiv.org/search/cs?searchtype=author&query=Ye,+Y), [Kunlun Zhu](https://arxiv.org/search/cs?searchtype=author&query=Zhu,+K), [Lan Yan](https://arxiv.org/search/cs?searchtype=author&query=Yan,+L), [Yaxi Lu](https://arxiv.org/search/cs?searchtype=author&query=Lu,+Y), [Yankai Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+Y), [Xin Cong](https://arxiv.org/search/cs?searchtype=author&query=Cong,+X), [Xiangru Tang](https://arxiv.org/search/cs?searchtype=author&query=Tang,+X), [Bill Qian](https://arxiv.org/search/cs?searchtype=author&query=Qian,+B), [Sihan Zhao](https://arxiv.org/search/cs?searchtype=author&query=Zhao,+S), [Lauren Hong](https://arxiv.org/search/cs?searchtype=author&query=Hong,+L), [Runchu Tian](https://arxiv.org/search/cs?searchtype=author&query=Tian,+R), [Ruobing Xie](https://arxiv.org/search/cs?searchtype=author&query=Xie,+R), [Jie Zhou](https://arxiv.org/search/cs?searchtype=author&query=Zhou,+J), [Mark Gerstein](https://arxiv.org/search/cs?searchtype=author&query=Gerstein,+M), [Dahai Li](https://arxiv.org/search/cs?searchtype=author&query=Li,+D), [Zhiyuan Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+Z), [Maosong Sun](https://arxiv.org/search/cs?searchtype=author&query=Sun,+M)

[View PDF](https://arxiv.org/pdf/2307.16789)

> Abstract:Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.

Submission history
------------------

From: Yujia Qin \[[view email](https://arxiv.org/show-email/85e4ecb2/2307.16789)\]  
**[\[v1\]](https://arxiv.org/abs/2307.16789v1)** Mon, 31 Jul 2023 15:56:53 UTC (1,473 KB)  
**\[v2\]** Tue, 3 Oct 2023 14:45:48 UTC (1,477 KB)