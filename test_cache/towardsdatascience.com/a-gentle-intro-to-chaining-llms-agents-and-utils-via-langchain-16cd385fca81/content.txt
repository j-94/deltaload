_#LLM for beginners_
--------------------

Understand the basics of agents, tools, and prompts and some learnings along the way
------------------------------------------------------------------------------------

[![Image 15: Dr. Varshita Sher](https://miro.medium.com/v2/resize:fill:88:88/1*ImYwhIFTYTspDAzmdk7i6w.jpeg)](https://varshitasher.medium.com/?source=post_page---byline--16cd385fca81--------------------------------)

[![Image 16: Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page---byline--16cd385fca81--------------------------------)

> Audience: For those feeling overwhelmed with the giant (yet brilliant) library…

![Image 17](https://miro.medium.com/v2/resize:fit:700/1*WJpvKWSnSV0NHa5_BFt8Qw.png)

Image generated by Author using [DALL.E 2](https://openai.com/product/dall-e-2)

Introduction
------------

I’d be lying if I said I have got the entire LangChain library covered — in fact, I am far from it. But the buzz surrounding it was enough to shake me out of my writing hiatus and give it a go 🚀.

The initial motivation was to see what was it that LangChain was adding (on a practical level) that set it apart from the chatbot I built last month using the `ChatCompletion.create()` function from the `openai` package. Whilst doing so, I realized I needed to understand the building blocks for LangChain first before moving on to the more complex parts.

This is what this article does. Heads-up though, there will be more parts coming as I am truly fascinated by the library and will continue to explore to see what all can be built through it.

Let’s begin by understanding the fundamental building blocks of LangChain — i.e. Chains. If you’d like to follow along, here’s the [GitHub repo](https://github.com/V-Sher/LangChain-Tutorial).

What are chains in LangChain?
-----------------------------

Chains are what you get by connecting one or more large language models (LLMs) in a logical way. (Chains can be built of entities other than LLMs but for now, let’s stick with this definition for simplicity).

OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Bloom, Huggingface, etc.

_Note: Pretty much most of these LLM providers will need you to request an API key in order to use them. So make sure you do that before proceeding with the remainder of this blog. For example:_

import os  
os.environ\["OPENAI\_API\_KEY"\] = "..."

_P.S. I am going to use OpenAI for this tutorial because I have a key with credits that expire in a month’s time, but feel free to replace it with any other LLM. The concepts covered here will be useful regardless._

Chains can be simple (i.e. Generic) or specialized (i.e. Utility).

1.  Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt). Here’s an example:

Let’s build a basic chain — create a prompt and get a prediction
----------------------------------------------------------------

Prompt creation (using `PromptTemplate`) is a bit fancy in Lanchain but this is probably because there are quite a few different ways prompts can be created depending on the use case (we will cover `AIMessagePromptTemplate`,  
`HumanMessagePromptTemplate` etc. in the next blog post). Here’s a simple one for now:

from langchain.prompts import PromptTemplateprompt = PromptTemplate(  
    input\_variables=\["product"\],  
    template="What is a good name for a company that makes {product}?",  
)

print(prompt.format(product="podcast player"))

\# OUTPUT  
\# What is a good name for a company that makes podcast player?

_Note: If you require multiple_ `_input_variables_`_, for instance:_ `_input_variables=["product", "audience"]_` _for a template such as_ `_“What is a good name for a company that makes {product} for {audience}”_`_, you need to do_ `print(prompt.format(product="podcast player", audience="children”)` to get the updated prompt.

Once you have built a prompt, we can call the desired LLM with it. To do so, we create an `LLMChain` instance (in our case, we use `OpenAI`'s large language model `text-davinci-003`). To get the prediction (i.e. AI-generated text), we use `run` function with the name of the `product`.

from langchain.llms import OpenAI  
from langchain.chains import LLMChainllm = OpenAI(  
          model\_name="text-davinci-003", # default model  
          temperature=0.9) #temperature dictates how whacky the output should be  
llmchain = LLMChain(llm=llm, prompt=prompt)  
llmchain.run("podcast player")

\# OUTPUT  
\# PodConneXion

If you had more than one input\_variables, then you won’t be able to use `run`. Instead, you’ll have to pass all the variables as a `dict`. For example, `llmchain({“product”: “podcast player”, “audience”: “children”})`.

_Note 1: According to_ [_OpenAI_](https://openai.com/blog/introducing-chatgpt-and-whisper-apis), `_davinci_` _text-generation models are 10x more expensive than their chat counterparts i.e_ `_gpt-3.5-turbo_`_, so I tried to switch from a text model to a chat model (i.e. from_ `_OpenAI_` _to_ `_ChatOpenAI_`_) and the results are pretty much the same._

_Note 2: You might see some tutorials using_ `_OpenAIChat_`_instead of_ `_ChatOpenAI_`_. The former is_ [_deprecated_](https://github.com/hwchase17/langchain/issues/1556#issuecomment-1463224442) _and will no longer be supported and we are supposed to use_ `_ChatOpenAI_`_._

from langchain.chat\_models import ChatOpenAIchatopenai = ChatOpenAI(  
                model\_name="gpt-3.5-turbo")  
llmchain\_chat = LLMChain(llm=chatopenai, prompt=prompt)  
llmchain\_chat.run("podcast player")

\# OUTPUT  
\# PodcastStream

This concludes our section on simple chains. It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next).

2\. Utility — These are specialized chains, comprised of many LLMs to help solve a specific task. For example, LangChain supports some end-to-end chains (such as `[AnalyzeDocumentChain](https://python.langchain.com/docs/use_cases/question_answering/how_to/analyze_document)` for summarization, QnA, etc) and some specific ones (such as `[GraphQnAChain](https://python.langchain.com/en/latest/modules/chains/index_examples/graph_qa.html#querying-the-graph)` for creating, querying, and saving graphs). We will look at one specific chain called `PalChain` in this tutorial for digging deeper.

PAL stands for [Programme Aided Language Model](https://arxiv.org/pdf/2211.10435.pdf). `PALChain` reads complex math problems (described in natural language) and generates programs (for solving the math problem) as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter.

To confirm this is in fact true, we can inspect the `_call()` in the base code [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py). Under the hood, we can see this chain:

*   [first uses a generic](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58) `[LLMChain](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58)` [to understand the query](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L58) we pass to it and get a prediction. Thus, this chain requires passing an LLM at the time of initializing (we are going to use the same OpenAI LLM as before).
*   s[econd, it uses Python REPL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/base.py#L63-L64) to solve the function/program outputted by the LLM.

_P.S. It is a good practice to inspect_ `__call()_` _in_ `_base.py_` _for any of the chains in LangChain to see how things are working under the hood._

from langchain.chains import PALChain  
palchain = PALChain.from\_math\_prompt(llm=llm, verbose=True)  
palchain.run("If my age is half of my dad's age and he is going to be 60 next year, what is my current age?")\# OUTPUT  
\# \> Entering new PALChain chain...  
\# def solution():  
\#    """If my age is half of my dad's age and he is going to be 60 next year, what is my current age?"""  
\#    dad\_age\_next\_year = 60  
\#    dad\_age\_now = dad\_age\_next\_year - 1  
\#    my\_age\_now = dad\_age\_now / 2  
\#    result = my\_age\_now  
\#    return result  
#  
\# \> Finished chain.  
\# '29.5'

_Note1:_ `_verbose_` _can be set to_ `_False_` _if you do not need to see the intermediate step._

Now some of you may be wondering — _but what about the prompt? We certainly didn’t pass one as we did for the generic_ `_llmchain_` _we built._ The fact is, it is automatically loaded when using `.from_math_prompt()`. You can check the default prompt using `palchain.prompt.template` or you can directly inspect the prompt file [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/pal/math_prompt.py).

print(palchain.prompt.template)  
\# OUTPUT  
\# 'Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """Olivia has $23. She bought five bagels for $3 each. How much money does she have left?"""\\n    money\_initial = 23\\n    bagels = 5\\n    bagel\_cost = 3\\n    money\_spent = bagels \* bagel\_cost\\n    money\_left = money\_initial - money\_spent\\n    result = money\_left\\n    return result\\n\\n\\n\\n\\n\\nQ: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?"""\\n    golf\_balls\_initial = 58\\n    golf\_balls\_lost\_tuesday = 23\\n    golf\_balls\_lost\_wednesday = 2\\n    golf\_balls\_left = golf\_balls\_initial - golf\_balls\_lost\_tuesday - golf\_balls\_lost\_wednesday\\n    result = golf\_balls\_left\\n    return result\\n\\n\\n\\n\\n\\nQ: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?"""\\n    computers\_initial = 9\\n    computers\_per\_day = 5\\n    num\_days = 4  # 4 days between monday and thursday\\n    computers\_added = computers\_per\_day \* num\_days\\n    computers\_total = computers\_initial + computers\_added\\n    result = computers\_total\\n    return result\\n\\n\\n\\n\\n\\nQ: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?"""\\n    toys\_initial = 5\\n    mom\_toys = 2\\n    dad\_toys = 2\\n    total\_received = mom\_toys + dad\_toys\\n    total\_toys = toys\_initial + total\_received\\n    result = total\_toys\\n    return result\\n\\n\\n\\n\\n\\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?"""\\n    jason\_lollipops\_initial = 20\\n    jason\_lollipops\_after = 12\\n    denny\_lollipops = jason\_lollipops\_initial - jason\_lollipops\_after\\n    result = denny\_lollipops\\n    return result\\n\\n\\n\\n\\n\\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?"""\\n    leah\_chocolates = 32\\n    sister\_chocolates = 42\\n    total\_chocolates = leah\_chocolates + sister\_chocolates\\n    chocolates\_eaten = 35\\n    chocolates\_left = total\_chocolates - chocolates\_eaten\\n    result = chocolates\_left\\n    return result\\n\\n\\n\\n\\n\\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?"""\\n    cars\_initial = 3\\n    cars\_arrived = 2\\n    total\_cars = cars\_initial + cars\_arrived\\n    result = total\_cars\\n    return result\\n\\n\\n\\n\\n\\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\n\\n# solution in Python:\\n\\n\\ndef solution():\\n    """There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?"""\\n    trees\_initial = 15\\n    trees\_after = 21\\n    trees\_added = trees\_after - trees\_initial\\n    result = trees\_added\\n    return result\\n\\n\\n\\n\\n\\nQ: {question}\\n\\n# solution in Python:\\n\\n\\n'

_Note: Most of the utility chains will have their prompts pre-defined as part of the library (check them out_ [_here_](https://github.com/hwchase17/langchain/tree/master/langchain/chains)_). They are, at times, quite detailed (read: lots of tokens) so there is definitely a trade-off between cost and the quality of response from the LLM._

Are there any Chains that don’t need LLMs and prompts?
------------------------------------------------------

_Even though PalChain requires an LLM (and a corresponding prompt) to parse the user’s question written in natural language, there are some chains in LangChain that don’t need one. These are mainly transformation chains that preprocess the prompt, such as removing extra spaces, before inputting it into the LLM. You can see another example_ [_here_](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html)_._

Can we get to the good part and start creating chains?
------------------------------------------------------

Of course, we can! We have all the basic building blocks we need to start chaining together LLMs logically such that input from one can be fed to the next. To do so, we will use `SimpleSequentialChain`.

The documentation has some great examples on this, for example, you can see [here](https://python.langchain.com/en/latest/modules/chains/generic/transformation.html) how to have two chains combined where chain#1 is used to clean the prompt (remove extra whitespaces, shorten prompt, etc) and chain#2 is used to call an LLM with this clean prompt. Here’s [another one](https://js.langchain.com/docs/modules/chains/foundational/sequential_chains/#simplesequentialchain) where chain#1 is used to generate a synopsis for a play and chain#2 is used to write a review based on this synopsis.

While these are excellent examples, I want to focus on something else. If you remember before, I mentioned that chains can be composed of entities other than LLMs. More specifically, I am interested in chaining agents and LLMs together. _But first, what are agents?_

Using agents for dynamically calling LLMs
-----------------------------------------

It will be much easier to explain what an agent does vs. what it is.

Say, we want to know the weather forecast for tomorrow. If were to use the simple ChatGPT API and give it a prompt `Show me the weather for tomorrow in London`, it won’t know the answer because it does not have access to real-time data.

![Image 18](https://miro.medium.com/v2/resize:fit:700/1*DtfKzz1HZ9l8l0zpqzRF1g.png)

Wouldn’t it be useful if we had an arrangement where we could utilize an LLM for understanding our query (i.e prompt) in natural language and then call the weather API on our behalf to fetch the data needed? This is exactly what an agent does (amongst other things, of course).

> An agent has access to an LLM and a suite of tools for example Google Search, Python REPL, math calculator, weather APIs, etc.

There are quite a few agents that LangChain supports — see [here](https://python.langchain.com/docs/modules/agents/agent_types/) for the complete list, but quite frankly the most common one I came across in tutorials and YT videos was `zero-shot-react-description`. This agent uses [ReAct](https://arxiv.org/abs/2210.03629) (Reason + Act) framework to pick the most usable tool (from a list of tools), based on what the input query is.

_P.S.:_ [_Here’s_](https://tsmatz.wordpress.com/2023/03/07/react-with-openai-gpt-and-langchain/) _a nice article that goes in-depth into the ReAct framework._

Let’s initialize an agent using `initialize_agent` and pass it the `tools` and `LLM` it needs. There’s a long list of tools available [here](https://python.langchain.com/docs/integrations/tools/) that an agent can use to interact with the outside world. For our example, we are using the same math-solving tool as above, called `pal-math`. This one requires an LLM at the time of initialization, so we pass to it the same OpenAI LLM instance as before.

from langchain.agents import initialize\_agent  
from langchain.agents import AgentType  
from langchain.agents import load\_toolsllm = OpenAI(temperature=0)  
tools = load\_tools(\["pal-math"\], llm=llm)

agent = initialize\_agent(tools,  
                         llm,  
                         agent=AgentType.ZERO\_SHOT\_REACT\_DESCRIPTION,  
                         verbose=True)

Let’s test it out on the same example as above:

agent.run("If my age is half of my dad's age and he is going to be 60 next year, what is my current age?")\# OUTPUT  
\# \> Entering new AgentExecutor chain...  
\# I need to figure out my dad's current age and then divide it by two.  
\# Action: PAL-MATH  
\# Action Input: What is my dad's current age if he is going to be 60 next year?  
\# Observation: 59  
\# Thought: I now know my dad's current age, so I can divide it by two to get my age.  
\# Action: Divide 59 by 2  
\# Action Input: 59/2  
\# Observation: Divide 59 by 2 is not a valid tool, try another one.  
\# Thought: I can use PAL-MATH to divide 59 by 2.  
\# Action: PAL-MATH  
\# Action Input: Divide 59 by 2  
\# Observation: 29.5  
\# Thought: I now know the final answer.  
\# Final Answer: My current age is 29.5 years old.

\# \> Finished chain.  
\# 'My current age is 29.5 years old.'

_Note 1: At each step, you’ll notice that an agent does one of three things — it either has an_ `_observation_`_, a_ `_thought_`_, or it takes an_ `_action_`_. This is mainly due to the ReAct framework and the associated prompt that the agent is using:_

print(agent.agent.llm\_chain.prompt.template)  
\# OUTPUT  
\# Answer the following questions as best you can. You have access to the following tools:  
\# PAL-MATH: A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.\# Use the following format:

\# Question: the input question you must answer  
\# Thought: you should always think about what to do  
\# Action: the action to take, should be one of \[PAL-MATH\]  
\# Action Input: the input to the action  
\# Observation: the result of the action  
\# ... (this Thought/Action/Action Input/Observation can repeat N times)  
\# Thought: I now know the final answer  
\# Final Answer: the final answer to the original input question  
\# Begin!  
\# Question: {input}  
\# Thought:{agent\_scratchpad}

_Note2: You might be wondering what’s the point of getting an agent to do the same thing that an LLM can do. Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input \[_[_Source_](https://python.langchain.com/en/latest/modules/agents.html#agents)_\]. In these types of chains, there is an “agent” which has access to a suite of tools.  
For instance,_ [_here’s_](https://python.langchain.com/en/latest/modules/agents/agent_executors/examples/agent_vectorstore.html#create-the-agent) _an example of an agent that can fetch the correct documents (from the vectorstores) for_ `_RetrievalQAChain_` _depending on whether the question refers to document A or document B._

For fun, I tried making the input question more complex (using Demi Moore’s age as a placeholder for Dad’s actual age).

agent.run("My age is half of my dad's age. Next year he is going to be same age as Demi Moore. What is my current age?")

Unfortunately, the answer was slightly off as the agent was not using the latest age for Demi Moore (since Open AI models were trained on data until 2020). This can be easily fixed by including another tool —  
`tools = load_tools([“pal-math”, **"serpapi"**], llm=llm)`. `serpapi` is useful for answering questions about current events.

_Note: It is important to add as many tools as you think may be relevant to the user query. The problem with using a single tool is that the agent keeps trying to use the same tool even if it’s not the most relevant for a particular observation/action step._

Here’s another example of a tool you can use — `podcast-api`. You need to [get your own API key](https://www.listennotes.com/api/pricing/) and plug it into the code below.

  
tools = load\_tools(\["podcast-api"\], llm=llm, listen\_api\_key="...")  
agent = initialize\_agent(tools,  
                         llm,  
                         agent=AgentType.ZERO\_SHOT\_REACT\_DESCRIPTION,  
                         verbose=True)agent.run("Show me episodes for money saving tips.")

\# OUTPUT  
\# \> Entering new AgentExecutor chain...  
\# I should search for podcasts or episodes related to money saving  
\# Action: Podcast API  
\# Action Input: Money saving tips  
\# Observation:  The API call returned 3 podcasts related to money saving tips: The Money Nerds, The Rachel Cruze Show, and The Martin Lewis Podcast. These podcasts offer valuable money saving tips and advice to help people take control of their finances and create a life they love.  
\# Thought: I now have some options to choose from   
\# Final Answer: The Money Nerds, The Rachel Cruze Show, and The Martin Lewis Podcast are great podcast options for money saving tips.

\# \> Finished chain.

\# 'The Money Nerds, The Rachel Cruze Show, and The Martin Lewis Podcast are great podcast options for money saving tips.'

_Note1: There is a_ [_known error_](https://github.com/hwchase17/langchain/pull/1833) _with using this API where you might see,_ `_openai.error.InvalidRequestError: This model’s maximum context length is 4097 tokens, however you requested XXX tokens (XX in your prompt; XX for the completion). Please reduce your prompt; or completion length._` _This happens when the response returned by the API might be too big. To work around this, the documentation suggests returning fewer search results, for example, by updating the question to_ `"Show me episodes for money saving tips, return only 1 result"`.

_Note2: While tinkering around with this tool, I noticed some inconsistencies. The responses aren’t always complete the first time around, for instance here are the input and responses from two consecutive runs:_

_Input: “Podcasts for getting better at French”_

_Response 1: “The best podcast for learning French is the one with the highest review score.”  
Response 2: ‘The best podcast for learning French is “FrenchPod101”._

Under the hood, the tool is first using an LLMChain for [building the API URL](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L115) based on our input instructions (something along the lines of `[https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29)`[)](https://listen-api.listennotes.com/api/v2/search?q=french&type=podcast&page_size=3%29) and [making the API call](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L116). Upon receiving the response, it uses another LLMChain that [summarizes the response](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/base.py#L117) to get the answer to our original question. You can check out the prompts [here](https://github.com/hwchase17/langchain/blob/master/langchain/chains/api/prompt.py) for both LLMchains which describe the process in more detail.

I am inclined to guess the inconsistent results seen above are resulting from the summarization step because I have separately debugged and tested the API URL (created by LLMChain#1) via Postman and received the right response. To further confirm my doubts, I also stress-tested the summarization chain as a standalone chain with an empty API URL hoping it would throw an error but got the response _“Investing’ podcasts were found, containing 3 results in total.”_ 🤷‍♀ I’d be curious to see if others had better luck than me with this tool!

Use Case 2: Combine chains to create an age-appropriate gift generator
----------------------------------------------------------------------

Let’s put our knowledge of agents and sequential chaining to good use and create our own sequential chain. We will combine:

*   Chain #1 — The `agent` we just created that can solve [age problems](https://www.cliffsnotes.com/study-guides/algebra/algebra-i/word-problems/age-problems) in math.
*   Chain #2 — An LLM that takes the age of a person and suggests an appropriate gift for them.

\# Chain1 - solve math problem, get the age  
chain\_one = agent\# Chain2 - suggest age-appropriate gift  
template = """You are a gift recommender. Given a person's age,\\n  
 it is your job to suggest an appropriate gift for them.

Person Age:  
{age}  
Suggest gift:"""  
prompt\_template = PromptTemplate(input\_variables=\["age"\], template=template)  
chain\_two = LLMChain(llm=llm, prompt=prompt\_template) 

Now that we have both chains ready we can combine them using `SimpleSequentialChain`.

from langchain.chains import SimpleSequentialChainoverall\_chain = SimpleSequentialChain(  
                  chains=\[chain\_one, chain\_two\],  
                  verbose=True)

A couple of things to note:

*   We need not explicitly pass `input_variables` and `output_variables` for `SimpleSequentialChain` as the underlying assumption is that the output from chain 1 is passed as input to chain 2.

Finally, we can run it with the same math problem as before:

question = "If my age is half of my dad's age and he is going to be 60 next year, what is my current age?"  
overall\_chain.run(question)\# OUTPUT  
\# \> Entering new SimpleSequentialChain chain...

\# \> Entering new AgentExecutor chain...  
\# I need to figure out my dad's current age and then divide it by two.  
\# Action: PAL-MATH  
\# Action Input: What is my dad's current age if he is going to be 60 next year?  
\# Observation: 59  
\# Thought: I now know my dad's current age, so I can divide it by two to get my age.  
\# Action: Divide 59 by 2  
\# Action Input: 59/2  
\# Observation: Divide 59 by 2 is not a valid tool, try another one.  
\# Thought: I need to use PAL-MATH to divide 59 by 2.  
\# Action: PAL-MATH  
\# Action Input: Divide 59 by 2  
\# Observation: 29.5  
\# Thought: I now know the final answer.  
\# Final Answer: My current age is 29.5 years old.

\# \> Finished chain.  
\# My current age is 29.5 years old.

\# Given your age, a great gift would be something that you can use and enjoy now like a nice bottle of wine, a luxury watch, a cookbook, or a gift card to a favorite store or restaurant. Or, you could get something that will last for years like a nice piece of jewelry or a quality leather wallet.

\# \> Finished chain.

\# '\\nGiven your age, a great gift would be something that you can use and enjoy now like a nice bottle of wine, a luxury watch, a cookbook, or a gift card to a favorite store or restaurant. Or, you could get something that will last for years like a nice piece of jewelry or a quality leather wallet

There might be times when you need to pass along some additional context to the second chain, in addition to what it is receiving from the first chain. For instance, I want to set a budget for the gift, depending on the age of the person that is returned by the first chain. We can do so using `SimpleMemory`.

First, let’s update the prompt for `chain_two` and pass to it a second variable called `budget` inside `input_variables`.

template = """You are a gift recommender. Given a person's age,\\n  
 it is your job to suggest an appropriate gift for them. If age is under 10,\\n  
 the gift should cost no more than {budget} otherwise it should cost atleast 10 times {budget}.Person Age:  
{output}  
Suggest gift:"""  
prompt\_template = PromptTemplate(input\_variables=\["output", "budget"\], template=template)  
chain\_two = LLMChain(llm=llm, prompt=prompt\_template)

If you compare the `template` we had for `SimpleSequentialChain` with the one above, you’ll notice that I have also updated the first input’s variable name from `age` → `output`. This is a crucial step, failing which an error would be raised at the time of [chain validation](https://github.com/hwchase17/langchain/blob/master/langchain/chains/sequential.py#L41) — `_Missing required input keys: {age}, only had {input, output, budget}_`.  
This is because the output from the first entity in the chain (i.e. `agent`) will be the input for the second entity in the chain (i.e. `chain_two`) and therefore the variable names must match**.** Upon inspecting `agent`’s output keys, we see that the output variable is called `output`, hence the update.

print(agent.agent.llm\_chain.output\_keys)\# OUTPUT  
\["output"\]

Next, let’s update the kind of chain we are making. We can no longer work with `SimpleSequentialChain` because it only works in cases where this is a single input and single output. Since `chain_two` is now taking two `input_variables`, we need to use `SequentialChain` which is tailored to handle multiple inputs and outputs.

overall\_chain = SequentialChain(  
                input\_variables=\["input"\],  
                memory=SimpleMemory(memories={"budget": "100 GBP"}),  
                chains=\[agent, chain\_two\],  
                verbose=True)

A couple of things to note:

*   Unlike `SimpleSequentialChain`, passing `input_variables` parameter is mandatory for `SequentialChain`. It is a list containing the name of the input variables that the first entity in the chain (i.e. `agent` in our case) expects.  
    Now some of you may be wondering how to know the exact name used in the input prompt that the `agent` is going to use. We certainly did not write the prompt for this agent (as we did for `chain_two`)! It's actually pretty straightforward to find it out by inspecting the prompt template of the `llm_chain` that the agent is made up of.

print(agent.agent.llm\_chain.prompt.template)\# OUTPUT  
#Answer the following questions as best you can. You have access to the following tools:

#PAL-MATH: A language model that is really good at solving complex word math problems. Input should be a fully worded hard word math problem.

#Use the following format:

#Question: the input question you must answer  
#Thought: you should always think about what to do  
#Action: the action to take, should be one of \[PAL-MATH\]  
#Action Input: the input to the action  
#Observation: the result of the action  
#... (this Thought/Action/Action Input/Observation can repeat N times)  
#Thought: I now know the final answer  
#Final Answer: the final answer to the original input question

#Begin!

#Question: {input}  
#Thought:{agent\_scratchpad}

As you can see toward the end of the prompt, the questions being asked by the end-user is stored in an input variable by the name `input`. If for some reason you had to manipulate this name in the prompt, make sure you are also updating the `input_variables` at the time of the creation of `SequentialChain`.

Finally, you could have found out the same information without going through the whole prompt:

print(agent.agent.llm\_chain.prompt.input\_variables)\# OUTPUT  
\# \['input', 'agent\_scratchpad'\]

*   `[SimpleMemory](https://github.com/hwchase17/langchain/blob/master/langchain/memory/simple.py#L6)` is an easy way to store context or other bits of information that shouldn’t ever change between prompts. It requires one parameter at the time of initialization — `memories`. You can pass elements to it in `dict` form. For instance, `SimpleMemory(memories={“budget”: “100 GBP”})`.

Finally, let’s run the new chain with the same prompt as before. You will notice, the final output has some luxury gift recommendations such as weekend getaways in accordance with the higher budget in our updated prompt.

overall\_chain.run("If my age is half of my dad's age and he is going to be 60 next year, what is my current age?")\# OUTPUT  
#\> Entering new SequentialChain chain...

#\> Entering new AgentExecutor chain...  
\# I need to figure out my dad's current age and then divide it by two.  
#Action: PAL-MATH  
#Action Input: What is my dad's current age if he is going to be 60 next year?  
#Observation: 59  
#Thought: I now know my dad's current age, so I can divide it by two to get my age.  
#Action: Divide 59 by 2  
#Action Input: 59/2  
#Observation: Divide 59 by 2 is not a valid tool, try another one.  
#Thought: I can use PAL-MATH to divide 59 by 2.  
#Action: PAL-MATH  
#Action Input: Divide 59 by 2  
#Observation: 29.5  
#Thought: I now know the final answer.  
#Final Answer: My current age is 29.5 years old.

#\> Finished chain.

\# For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}

#\> Finished chain.

For someone of your age, a good gift would be something that is both practical and meaningful. Consider something like a nice watch, a piece of jewelry, a nice leather bag, or a gift card to a favorite store or restaurant.\\nIf you have a larger budget, you could consider something like a weekend getaway, a spa package, or a special experience.'}

Conclusion
----------

Hopefully, the learnings I have shared through this post have made you more comfortable in taking a deep dive into the library. This article just scratched the surface, there is so much more to cover. For instance, how to build a QnA chatbot over your own datasets, and how to optimize memory for these chatbots so that you can cherry-pick/summarize conversations to send in the prompt rather than sending all previous chat history as part of the prompt.

As always if there’s an easier way to do/explain some of the things mentioned in this article, do let me know. In general, refrain from unsolicited destructive/trash/hostile comments!

Until next time ✨

_I enjoy writing step-by-step beginner’s guides, how-to tutorials, decoding terminology used in ML/AI, etc. If you want full access to all my articles (and others on Medium), then you can sign up using_ [**_my link_**](https://varshitasher.medium.com/membership) _here_**_._**