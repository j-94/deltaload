---
title: The current state of continual learning in AI: Why is chatGPT only trained up until 2021? | Towards Data Science
description: Why is chatGPT only trained up until 2021? This article attempts to explain the current state of continual learning in deep learning, with a focus on large language models and chatbots.
url: https://towardsdatascience.com/the-current-state-of-continual-learning-in-ai-af4a05c42f3c
timestamp: 2025-01-20T15:49:59.838Z
domain: towardsdatascience.com
path: the-current-state-of-continual-learning-in-ai-af4a05c42f3c
---

# The current state of continual learning in AI: Why is chatGPT only trained up until 2021? | Towards Data Science


Why is chatGPT only trained up until 2021? This article attempts to explain the current state of continual learning in deep learning, with a focus on large language models and chatbots.


## Content

Why is ChatGPT only trained up until 2021?
------------------------------------------

[![Image 25: Jon Flynn](https://miro.medium.com/v2/resize:fill:88:88/1*16AI0ZxosqDanJ22tGkA_Q.jpeg)](https://medium.com/@jon.flynn2?source=post_page---byline--af4a05c42f3c--------------------------------)

[![Image 26: Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page---byline--af4a05c42f3c--------------------------------)

![Image 27](https://miro.medium.com/v2/resize:fit:700/1*vjERthvscIdEQAgIEm5PEg.png)

Image generated by author using DALL-E 3

**Knowledge prerequisites:**
----------------------------

A couple of years ago, I learned the basics of deep learning through StatQuest videos, Lena Voita’s NLP blogs, and books like “Deep Learning for Coders” and “Talking Nets.” I’m now wanting to understand the current state of continual learning in deep learning. I found that there is not much information available that summarises this topic in simpler terms, and it requires sifting through expert research papers. Therefore, this article is intended for readers who have a basic understanding of the topic but find the research difficult to read and may not be experts. It holds a focus on chatbots, so knowing the training stages of chatGPT is also helpful.

**Intro**
---------

![Image 28](https://miro.medium.com/v2/resize:fit:700/1*4UAraN_gKdcYcQ7nZy6vSQ.png)

ChatGPT telling the user it is only trained up until September 2021 (screenshot by author)

If large language models like ChatGPT could be continuously updated with new data, they could accelerate a wide range of tasks, from software development to legal processes to education and learning.

Continual learning is the ability to pause the model training process, save the model’s current state, and then later resume training on new data. The model should be able to generalise well to new data, while still maintaining its ability to generalise to old data. Refer to [this paper](https://arxiv.org/pdf/2302.00487.pdf) for a more formal definition.

Presently, the trend in the industry to augment chatbots with more data is to use RAG, combining queried vectors with prompt engineering to answer questions, rather than continuing to train the LLM with new data. ChatGPT’s zero-shot learning capability, which allows it to answer questions about new, unseen data, makes this approach very appealing. For instance, you could teach it a new programming language and then ask it questions about that language, with just a few prompts, although performance does degrade a bit proportionally to the amount of tokens input. Continually training the model to answer questions based on a new topic like this requires significant computing resources and more importantly, a wide variety of data on the relevant topic. Furthermore, if a topic has very low prevalence in the training set, it will generalise poorly to it. E.g.: take an unpopular public repo and it will know little about it and may hallucinate, despite having seen it at some point during the training process. Context windows (the amount of tokens the model can take as input) are getting increasingly larger very quickly, making RAG even more attractive. Ideally though, do we not want one intelligent all-knowing model, without the need for any external database?

Continual learning is an essential step towards AGI, and some doubt we will even be able to achieve it without significant changes in deep learning network architectures. Jeff Hawkins in his book, [“A Thousand Brains”](https://www.numenta.com/resources/books/a-thousand-brains-by-jeff-hawkins/), stated he does not think current ANN’s are capable of effective continual learning, and believes future models will probably need to be architected more similarly to the human brain using his theory on reference frames in the cortical columns of the neocortex.

Continual Learning in the pre-training vs fine-tuning stages of language models
-------------------------------------------------------------------------------

Earlier this year, a research paper called [“LIMA: Less Is More for Alignment”](https://arxiv.org/abs/2305.11206) was published. It introduced a chatbot that was not trained using Reinforcement Learning from Human Feedback (RLHF), but was instead fine-tuned (via cross-entropy loss) on just 1,000 carefully annotated question-and-answer samples. Surprisingly, the researchers said that in 43% of cases, “the chatbot’s responses were on par with those of GPT-4”. I did not take an in-depth look at how these were evaluated, but nonetheless, it’s widely acknowledged that a substantial amount of the model’s knowledge and capability is acquired during the pre-training phase, and research like this may further prove this.

Models like ChatGPT and Llama-chat have undergone extensive fine-tuning to generate more aligned and effective responses. OpenAI currently offer an API to further [fine-tune a model](https://platform.openai.com/docs/guides/fine-tuning), which takes Q&A data as input to be used for further training. However, this should not be used to teach the model new data, but rather to [customise the tone and steerability](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates). Fine-tuning a model in attempt to teach it new data can cause _catastrophic forgetting_, a problem where the model forgets what it has already learned. This article will go over some techniques that aim to mitigate this problem.

This also leads us to a couple key questions about the feasibility and strategy of continual learning:

*   At which stage of development is it most beneficial and easiest to introduce continual learning?
*   Given that fine-tuning (without LoRA) alters the entire model’s parameters, is it even possible to revert to the pre-training stage for further modification?

_Note: I provide some PyTorch-like pseudocode for some of the papers discussed below. It has not been tested and may not work, it’s used to break the techniques down step by step and translate any confusing math notation to help the reader understand._

The 5 sub-categories of continual learning techniques
-----------------------------------------------------

The [comprehensive overview of continual learning paper](https://arxiv.org/pdf/2302.00487.pdf) states training strategies for continual learning can be divided into 5 sub categories:

1.  Regularisation-based approach: this approach adds constraints or penalties to the learning process during the training process.
2.  Optimisation-based approach: this technique focuses on modifying the optimisation algorithm.
3.  Representation-based approach: this aims to learn a shared feature representation across different tasks, helping the model generalise better to new but related tasks.
4.  Replay-based approach: this involves storing some data or learned features from previous tasks and replaying them during training on new tasks to maintain performance on earlier learned tasks. In other words, mixing both the old and new datasets when training on new tasks.
5.  Architecture-based approach: in this approach, the network architecture is dynamically adjusted, often by growing or partitioning, delegating different parts of the network to different tasks.

1\. Regularisation-based approaches
-----------------------------------

Soft Masking of Parameters
--------------------------

The following soft-masking techniques mask and adjust the gradients of each parameter during the training process. The _optimisation-based approaches_ coming up also manipulate the gradients for continual learning. Remember the gradients aren’t just temporary numbers that appear and disappear during training; they’re signals that guide the evolution of the weights.

SPG
---

This [paper](https://arxiv.org/pdf/2306.14775.pdf) proposes a technique named SPG (Soft-masking of Parameter-level Gradient flow) which aims to:

1.  Train the model on each task until convergence.
2.  After training, calculate the “importance” of each parameter for the task.
3.  Soft-mask parameters based on their accumulated importance, making important parameters less likely to change during the learning of new tasks.

Let’s break the approach down step by step:

1\. Training the First Task
---------------------------

Train the model on the first task’s dataset as normal.

2\. Calculate Parameter Importance for the First Task
-----------------------------------------------------

After the training of the first task is complete, we calculate the importance of each model parameter. The intuition here is simple, we use the gradients of each parameter to compute its importance. A larger gradient implies that a small change in that parameter will result in a larger change in the loss, meaning the model’s performance could vary more significantly, hence that parameter is important.

The gradients are also normalised, because gradients in the first layer could be small, while those in the last layer could be large. If you’re calculating importance based on these raw gradient values, parameters in the last layer would seem more important because of the scale of their gradients, not necessarily because they are genuinely more crucial for the task.

![Image 29](https://miro.medium.com/v2/resize:fit:700/1*nuTWjoen0I34R2BCNeIxRg.png)

Equations for calculating the importance of the model parameters in SPG (section 3.1 of [paper](https://arxiv.org/pdf/2306.14775.pdf))

Let’s translate this calculation to PyTorch-like pseudocode:

import torchdef compute\_final\_importance(model, loss\_function, data\_loader):  
    # Get a single batch from the data loader  
    inputs, labels = next(iter(data\_loader))

# Forward and backward pass to calculate the gradients for all parameters  
    outputs = model(inputs)  
    loss = loss\_function(outputs, labels)  
    loss.backward()

importances = \[\]

# Calculate importance based on the gradients  
    for param in model.parameters():  
        if param.grad is not None:  # Gradients may be None for some unused parameters  
            normalized\_grad = (param.grad - torch.mean(param.grad)) / torch.std(param.grad)  
            importance = torch.tanh(normalized\_grad)  
            importances.append(importance)

return torch.stack(importances).mean(dim=0)

3\. Accumulating Importance Across Tasks
----------------------------------------

The accumulated importance of each parameter across task is simply calculated by taking the max value at any stage.

4\. Training Subsequent Tasks, combined loss and the soft-masking mechanism:
----------------------------------------------------------------------------

When training on new tasks, the researchers use a combined loss function consisting of two parts. One is the standard loss function which is used as normal on the new task and data, and the second is an additional loss function which involves putting the _new_ data through the _old_ model (the converged model checkpoint after the previous task) and summing up the logits produced. In classification networks the logits are usually the raw non normalised predictions generated by the model in one of the last layers before going through something like a softmax function. This sum of logits serves as a form of loss. The rationale is that if the summed logits are significantly affected when the model parameters change, those parameters are crucial for the performance of the previously learned task.

The gradients generated from this additional loss serve as a guide during backpropagation, nudging the shared parameters to change in a direction that is less likely to harm performance on the first task. It therefore acts as a sort of penalty term to enforce that any updates made to the model do not lead to a significant loss of information related to previous tasks.

Train the model on the next task. Use a standard training loop, but modify the gradients during backpropagation based on their accumulated importance. This is the soft-masking mechanism:

import torchaccumulated\_importance = # calculated at the end of each task

for epoch in range(num\_epochs):  
  for x, y in train\_loader:

# Forward Pass: Calculate the loss for the current task using the proper loss function  
    logits = new\_model(x)  
    loss\_current\_task = nn.CrossEntropyLoss()(logits, y)

# Forward Pass: Calculate the additional losses for previous tasks (CHI mechanism)  
    loss\_previous\_tasks = 0  
    for prev\_task\_id in range(task\_id):  
        logits\_prev = old\_model(x, prev\_task\_id)  
        loss\_previous\_tasks += logits\_prev.sum()

# Combine the losses  
    combined\_loss = loss\_current\_task + loss\_previous\_tasks

# Backward Pass  
    optimizer.zero\_grad()  
    combined\_loss.backward()

# Update the accumulated importance  
    for param, acc\_imp in zip(model.parameters(), accumulated\_importance):  
        grad = param.grad  
        acc\_imp = torch.max(acc\_imp, torch.abs(grad))

# Soft-masking the gradients before taking an optimization step  
    for param, imp in zip(model.parameters(), accumulated\_importance):  
        param.grad \*= (1 - importance)

optimizer.step()

5\. Soft-Masking Special Cases
------------------------------

*   Feature Extractor: Gradients of parameters in the shared feature extractor are modified based on their specific accumulated importance.
*   Classification Head: For the classification head, gradients are modified based on the average importance of the feature extractor.

Applying this to LLMs
---------------------

Bear in mind, this paper does not experiment this with a language model, but I assume in a language model you could think of the transformer layers as analogous to the “feature extractor,” and the final classification layer (which predicts the next word or token in the sequence) as the “classification head.”

Soft-masking applied to continual pre-training in a language model
------------------------------------------------------------------

Next we’ll go into a paper which applies similar soft-masking to the pre-training stage in language modelling.

[This paper](https://arxiv.org/pdf/2302.03241.pdf) introduces a technique called DAS (Continual DA-pre-training of LMs with Soft-masking) for continual learning in the pre-training stage of a large language model. It applies a soft-masking technique similar to the one just discussed along with a couple other techniques in attempt to continue pre-training of an LLM without running into catastrophic forgetting.

Let’s break it down step by step:

Initial Pre-training Phase
--------------------------

Pre-train the LLM like normal.

Further Pre-training on A New Domain
------------------------------------

Prepare New Domain Data:
------------------------

A new dataset from a different domain is prepared.

Calculating the importance of each neuron
-----------------------------------------

SPG used gradients to determine the importance of each parameter, and then applied the calculated importance value to mask the gradient adjustments of parameters during training. This paper tries to determine the importance of each unit/neuron, rather than parameter, and then uses this in the same way by masking the gradient during training.

This paper uses two different methods to calculate the importance of neurons, depending on the task at hand. One, a gradient-based importance detection method (originally outlined in [this paper](https://arxiv.org/pdf/1905.10650.pdf)), and two, a custom “proxy loss function”.

The first introduced is _not_ used in the continual learning of the _first_ new domain. Why? It needs data from the training dataset to work and the authors state that users “don’t have access to the massive original pre-training dataset”, which is a fair assumption. The proxy loss function is used instead for the first phase of continual learning and then for each subsequent phase the other method is used.

**The proxy loss function (“Proxy KL-divergence loss”):**

I found this term confusing at first, but it’s called this because the original gradient-based importance detection method is defined as a loss function itself, which you can then use to run the network’s outputs through to get the gradients of each neuron, which can then be used to derive importance, just like the SPG technique. It’s calculated by the following:

*   Take a subset of the new domain we’re wanting to train on and feed it twice through the model to get two different representations. These representations will differ a bit due to the existing dropout masks in the Transformer architecture.
*   Compute the KL-divergence between these two representations.

Modified Backpropagation Flow with Proxy and Combined Loss
----------------------------------------------------------

1.  **Forward Pass:** Data goes through a forward pass in the neural network.
2.  **Backpropagation:**

**Apply Proxy Loss for Gradient Adjustment:** The proxy loss function’s unit-level importance is used to soft-mask the original gradients. This is expressed as:

adjusted\_grad \*= (1 − unit\_level\_importance)

**Calculate Combined Loss (MLM + Contrastive Loss):** Compute the combined loss using both MLM and contrastive loss.

Further Pre-training on More Domains
------------------------------------

1.  **Direct Importance Calculation:** For each new domain, the importance of each unit can now be directly calculated using the data from the new domain via the gradient-based method outlined in equation 3, eliminating the need for the proxy loss function which is only once used after the initial pre-training.
2.  **The importance of neurons is updated incrementally as each new task is learned.** This update is done using element-wise max. “Element-wise maximum (EMax) operation” refers to comparing two vectors element by element, and taking the maximum value for each corresponding element to create a new vector. E.g.: if you have two vectors A and B of the same length, the element-wise maximum will result in a new vector C where each element _C_\[_i_\] is the maximum between _A_\[_i_\] and _B_\[_i_\].

2\. Optimisation-based approaches
---------------------------------

We’ll refer to the two techniques outlined in the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf) in section 3.1

Gradient Direction Preservation
-------------------------------

The paper talks about manipulating the gradient-based optimisation process to make the gradient _directions_ of new training samples close to those from old training samples. The formula

> ⟨ ∇θ Lₖ(θ; Dₖ), ∇θ Lₖ(θ; Mₜ) ⟩ ≥ 0

enforces that learning the new task should not increase the loss for the old tasks. Essentially, the gradients of the new task and the old tasks are encouraged to align.

Breaking down the formula, we take the dot product of the gradient of the loss from the new task (∇θ Lₖ(θ; Dₖ)) and the gradient of the loss from the old task (∇θ Lₖ(θ; Mₜ)) should be non-negative. In this context, a positive dot product implies that the gradients for the old task and the new task are generally pointing in the same direction, with the angle between these two vectors is less than or equal to 90 degrees.

Forward/Backward Passes:
------------------------

Forward Pass:
-------------

You would run your input data _Dₖ_ for the new task and _Mₜ_​ for the old task through the same model to calculate the loss for each.

Backward Pass:
--------------

1.  Compute the gradients of the loss with respect to the network parameters for both the old and new task.
2.  Alignment Check: Compute the dot product of the two gradients. You’d then use this information to modify the gradients for the new task in such a way that the dot product is non-negative.
3.  Update Weights: Update the model parameters using these “aligned” gradients.

  
import torch\# Forward pass for the new task  
output\_k = model(D\_k)  
loss\_k = criterion(output\_k, y\_k)

\# Forward pass for the old task  
output\_t = model(M\_t)  
loss\_t = criterion(output\_t, y\_t)

\# Compute gradients for both tasks  
loss\_k.backward(retain\_graph=True)  # Compute gradients for new task but keep computation graph  
grad\_k = torch.cat(\[p.grad.view(-1) for p in model.parameters()\])

optimizer.zero\_grad()

loss\_t.backward()  # Compute gradients for old task  
grad\_t = torch.cat(\[p.grad.view(-1) for p in model.parameters()\])

\# Compute dot product and modify gradients if they don't align  
dot\_product = torch.dot(grad\_k, grad\_t)  
if dot\_product < 0:  
    # I'm not sure how you modify the gradients here if they don't align, I'm not sure the paper specifies it

\# Use the modified gradient to update model parameters  
index = 0  
for p in model.parameters():  
    num\_params = p.numel()  
    # Update using modified gradients  
    p.grad = grad\_k\[index: index + num\_params\].view(p.shape)  
    index += num\_params

optimizer.step()

Gradient Direction Preservation without needing old training samples
--------------------------------------------------------------------

The text also highlights that gradient projection can be performed even without storing old samples. NCL (Natural continual learning, [paper link](https://proceedings.neurips.cc/paper/2021/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html)) is the technique summarised here. Note, this can be categorised as both a regularisation and optimisation based approach.

Training process step by step:
------------------------------

**Forward Pass:**
-----------------

You would run your new data through the network and calculate the loss as usual.

**Backward Pass:**
------------------

**Objective:** The aim is to minimise the task-specific loss _ℓk(θ)_ while adhering to a distance constraint _d_(_θ_,_θ_+_δ_)≤_r._

**Algorithm step by step**:

1.  As normal, compute the gradient of the loss with respect to the model parameters ∇_θ_​ℓ_k_​(_θ_).
2.  The _δ_ is calculated using the update rule. This gives you the “suggested” changes to the model parameters _θ_ based on the new task’s requirements.
3.  Then, you plug this _δ_ into the distance constraint formula: _d(θ,θ+δ)=squareroot(δ⊤Λ\_k-1​δ)_​. The constraint acts like a boundary around the current parameters _θ_, defined by the distance metric _d_(_θ_,_θ_+_δ_) and the radius _r_. I struggled to see why they called it a “radius”, and not just “constraint number” or something. I think it’s because the researchers are visualising the gradients and training process in a high-dimensional space. When you apply a constraint based on the distance metric, you’re essentially defining a “sphere” around your current parameter values in that high-dimensional space. The “radius” _r_ of this sphere sets a limit on how much the parameter can move while learning a new task.
4.  If the proposed _δ_ would move _θ_ too far according to this distance metric, i.e., beyond this boundary, you scale it down so that it stays within the allowable region defined by the radius _r_.

Let’s look at each bit more in-depth:

**Update Rule:** The update rule provides a direction in which _θ_ should move.

![Image 30](https://miro.medium.com/v2/resize:fit:688/1*AG0E-ZngB2ZAvk1VbDkZ_g.png)

NCL update rule from section 3.1 in the [comprehensive overview of continual learning paper](https://arxiv.org/pdf/2302.00487.pdf)

Breaking it down:

*   _∇θ ℓk(θ)_ represents the gradients for all parameters (_θ)_ calculated by the loss function.
*   Parameter importance calculation (_Λ^(k-1)\_(-1)_): This term represents a _precision matrix_ and it is yet another way to calculate the importance of parameters in the network. _more details below_
*   Regularisation Term (_θ — μ\_(k-1)_): This term pulls the updated parameters closer to the optimal parameters _μ\_(k-1)_​ from the previous task. Like the before techniques, it acts as a regulariser to avoid deviation from what was already learned.
*   Learning Rate (_λ_)

**Distance Constraint:** Before applying this update, you’d usually check whether this change _δ_ would violate the distance constraint _d_(_θ_,_θ_+_δ_)≤_r_. If it does, you’d typically scale down _δ_ so that it satisfies the constraint.

**Precision matrix explanation:** before in the soft-masking methods we saw the calculation of importance via the output of all neurons or their gradients. In this method a precision matrix is used. This is a bit more complex so I’ll attempt to explain it:

We first calculate the _covariance matrix_ for the networks parameters. In the context of neural networks, the columns in the gradient matrix _G_ correspond to the parameters (weights and biases) of the model. Each row in _G_ represents the gradient vector for a single training example, with respect to all of those parameters.

So, if you have a neural network with _P_ parameters (this includes all the weights and biases from all layers), then each gradient vector will have _P_ elements, one for each parameter. Therefore, _G_ will be a matrix of shape _N_ × _P_, _N_ representing each batch and therefore each row representing the average gradient vector across all the training examples in a given batch.

When you calculate the covariance matrix Σ from _G_, the resulting matrix will have dimensions _P_ × _P_. The diagonal entries Σ_ii_​ will indicate the variance of the gradient with respect to the _ith_ parameter, and the off-diagonal entries Σ_ij_​ will indicate the covariance between the gradients with respect to the _ith_ and _jth_ parameters. This gives you an idea of how these parameters interact or co-vary during the training process. The inverse of this matrix is the _precision matrix_, which is what we use to determine importance.

Why the _precision matrix_ over the _covariance matrix_? While the covariance matrix Σ does capture how parameters interact with each other during training, it doesn’t specifically indicate how crucial each parameter is to the task at hand when all other parameters are considered. In contrast, the precision matrix allows us to assess the _conditional independence_ (this is a concept in probability theory, look it up) of parameters. Large values in the precision matrix indicate that knowing one parameter is highly informative about another, given all the other parameters. I’m not going to go into examples of how this works so get ChatGPT to generate some examples using a very small neural network to see how the values can be interpreted.

Previous methods we saw that calculate importance focus on individual neurons or parameters, ignoring the relationships between them. The precision matrix, on the other hand, can capture these relationships. Like everything in deep learning, whether this is a better way to calculate the importance of a network, is going to be empirical and could differ depending on the task and scale of the network.

**Algorithm step by step in PyTorch:**

import torch\# Constraint radius  
radius = 0.1

for epoch in range(num\_epochs):    
    for batch\_idx, (data, target) in enumerate(data\_loader):  
        optimizer.zero\_grad()

# Forward pass  
        output = model(data)  
        loss = loss\_function(output, target)

# Backward pass to get gradients for params  
        loss.backward()  
        model\_grad = torch.cat(\[p.grad.data.view(-1) for p in model.parameters()\])

# Compute δ using the NCL method  
        # δ = Λ^(-1) \* grad - (θ - µ)  
        delta = torch.matmul(torch.inverse(covarianceMatrix), model\_grad) - (torch.cat(\[p.data.view(-1) for p in model.parameters()\]) - parametersForPrevTask)

# Check constraint  
        if torch.norm(delta) \> radius:  
            delta = radius \* delta / torch.norm(delta)

# Update model parameters (θ) using δ  
        idx = 0  
        for p in model.parameters():  
            length = p.data.numel()  
            p.data += delta\[idx: idx + length\].view(p.data.shape)  
            idx += length

# Update Λ and µ for the next task, probably going to be task-specific and non-trivial

3\. Representation-based approach
---------------------------------

Firstly, it’s important to note that the pre-training of LLM’s to be further fine-tuned on a downstream task is an example of continual learning in this sub-category. I think ChatGPT’s ability to reason about never-before-seen data is also an example of this approach. Although we technically call it zero-shot learning, and the term “continual learning” requires updating model parameters, it goes beyond anything we’ve seen before. As discussed in the introduction, prompt engineering could be the future of continual learning, instead of continually updating the parameters.

Below we’ll take a look at using knowledge distillation for continual learning. I’m not really sure which sub-category this falls under, but I’d guess it’s probably a mix between representation, architecture and replay approaches. Even though some of the techniques we’re reviewing may seem random and unproven at large scale, breakthroughs in this field are often unpredictable. Therefore, it’s important to maintain a broad perspective.

Knowledge Distillation for continual learning
---------------------------------------------

You can transfer (or “distill”) the knowledge of one network into another network, and the second network does a reasonable job of approximating the function learned by the original network.

The distilled model (the _student_), is trained to mimic the output of the larger network (the _teacher_), instead of training it on the raw data directly. For example, say you want to train a smaller student model to mimic a large pre-trained language model (the teacher). Run the original pre-training dataset through the teacher model to generate “soft targets.” These are probability distributions over potential outputs, i.e.: next-word predictions. For instance, for a next-word prediction task, instead of predicting “cat,” the teacher might provide probabilities like 90% for “cat”, 5% for “kitten”, 3% for “feline”, etc.

This is usually done to transfer knowledge to much smaller models, and it yields great results despite the smaller model.

Let’s see how some researchers [applied this with success](https://ojs.aaai.org/index.php/AAAI/article/view/17600) to a NER (named entity recognition) model. The training process is fairly straightforward:

Training process step by step
-----------------------------

There are two primary methods outlined in the paper: AddNER and ExtendNER.

AddNER Model
------------

Note, NER models work by taking a sequence of tokens (usually a sentence) as input and then output a probability distribution (for the different types of entities) for each token. IOB tagging is commonly used for NER models, each token can be labeled as ‘O’, or as the beginning (‘B-’) or inside (‘I-’) of an entity of type _X_. ‘O’ stands for ‘Outside’, it just means the current token doesn’t belong to any entity. Therefore, for _n_ entity types, you will have 2_n_ output neurons in the classification layer: _n_ for the ‘B-’ tags (one for each entity type) and _n_ for the ‘I-’ tags (again, one for each entity type). Add to this the ‘O’ label, which signifies that a token doesn’t belong to any entity, and you end up with 2_n_ \+ 1 possible labels for each token. The final dimensions can be written as _h_ × (2_n_ \+ 1), where _h_ is the size of the hidden layer’s output. Bear in mind, this is only for models where tokens can only be one entity. E.g.: “Apple” could be tagged as both “FOOD” and “COMPANY”.

Architecture and teacher-student setup
--------------------------------------

The student model in this case is a copy of the teacher model, with an additional output classification layer for each new entity type that the model should learn. During training, the new output layer learns from the new annotated data, and the older layers are guided by the teacher model’s outputs to minimise forgetting.

After training, the old output layers are not discarded. It then uses the algorithm and heuristics described in the conflict resolver section _(end of section 3.3)_ to combine these outputs into a single, final prediction for each token in the sequence.

![Image 31](https://miro.medium.com/v2/resize:fit:700/1*rlmiNVZXHCJmieYjBucCaQ.png)

Diagram of the AddNER model from section 3.2 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)

Forward Pass
------------

1.  Old Entity Types: The input sentence is passed through the teacher model to obtain probability distributions (the “soft targets” in this context) for the old entity types.
2.  New Entity Types: The same sentence is also passed through the new student model with additional output layers specific to the new entity types​.

Backward Pass
-------------

**Combined loss function:**

1.  KD Loss: calculated by comparing how closely the output probabilities of the old entity types from the new model (student) match those from the old model (teacher). It uses KL-divergence to calculate this. It’s probably calculated token-by-token and then summed or averaged over all tokens in a sentence or batch, but I don’t think the paper goes into this.
2.  Cross-Entropy Loss: This is the usual loss function that compares the model’s predictions for the new entity types against the actual labels from the new dataset.
3.  Combining the two: these two losses are combined into a combined loss by taking a weighted sum of them both. The weights for combining these losses are set by the hyperparameters alpha and beta, which are adjusted like any other hyperparameter to better performance based on experiments.

\# Hyperparameters alpha and beta for weighting the two loss functions  
alpha = 0.5  
beta = 0.5for epoch in range(num\_epochs):  
    for sentence, labels in D\_new:  
        # Forward pass in teacher model for old entity types  
        teacher\_probs\_Ei = teacher\_model(sentence)

# Forward pass in student model for old and new entity types  
        # Note: the new entity types must go through the new output layer (not shown in this pseudocode)  
        student\_probs\_Ei, student\_probs\_Enew = student\_model(sentence)

# Compute KD loss  
        kd\_loss = KL\_divergence(teacher\_probs\_Ei, student\_probs\_Ei)

# Compute CE loss for new entity types  
        ce\_loss = cross\_entropy(labels, student\_probs\_Enew)

# Combined loss  
        total\_loss = alpha \* kd\_loss + beta \* ce\_loss

# Backward pass  
        total\_loss.backward()

# Update student model parameters  
        optimizer.step()

ExtendNER Model
---------------

Architecture and teacher-student setup
--------------------------------------

The ExtendNER model extends the output layer dimensions to accommodate new entity types, instead of adding new output layers. The paper explains quite simply how the dimensions are to be:

“Assuming that _Mi_ was able to recognize _n_ entity types, its final layer can be considered as a matrix with dimension _h×(2n+1)_. The output layer of _Mi+1_ will then be extended to be a matrix with dimension _h × (2n + 2m + 1)_ in order to accommodate the new entity types.”

![Image 32](https://miro.medium.com/v2/resize:fit:700/1*k1hdglsBtvNEpZQYdBqXOA.png)

Diagram of the ExtendNER model from section 3.4 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)

Forward Pass
------------

Same as in AddNER, but with extended dimensions.

Backward Pass
-------------

The loss calculation uses _either_ the KL-divergence loss or the cross-entropy loss, depending on the following:

*   When the NER category label _y_ is “O” (from the IOB tagging schema), the KL divergence loss is used.
*   When the category label _y_ is NOT “O”, the Cross-Entropy loss is used.

Final Prediction
----------------

Viterbi algorithm is applied to decode the final entity types.

Both AddNER and ExtendNER models performed well for continual learning and the results did not differ between them much

4\. Replay-based approach
-------------------------

“Fine-tuned language models are continual learners”
---------------------------------------------------

[paper link](https://arxiv.org/pdf/2205.12393.pdf)

The model in the paper is not a generic, single-task model like GPT trained just for conversational response. Instead, it’s fine-tuned for a sequence of specialised tasks, ranging from text simplification to Haiku generation. Each of these tasks has unique requirements, evaluation metrics, and specialised training datasets.

The researchers mix parts of the old dataset with the new dataset, and achieve great results by mixing in just 1% of the previous task’s dataset when fine-tuning on a new task. This is done sequentially for many tasks (8). The model also performs well in zero-shot learning settings, meaning it can generalise well to tasks it hasn’t been trained on. For instance, it can generate a Haiku with the correct syllable count when given an unseen topic, showing its ability to generalise. The researchers also mention that their approach is task-order invariant, meaning the sequence in which tasks are learned does not affect the model’s performance. The experiments find that the amount of the old dataset mixed in with the new one doesn’t significantly affect the main task’s performance. However, it does affect the zero-shot learning. At 0% rehearsal, the model tends to forget the zero-shot tasks, while at 1% rehearsal, the model maintains its performance in those tasks very well.

This all seems positive, the fact we can just add 1% of the old dataset and continual learning is solved, but of course, applying it to a chatbot like chatGPT, will be empirical and can be completely different. Even if, hypothetically, chatGPT could be continually trained in the fine-tuning stages like this, it would require an immense amount of labeled conversation data.

5\. Architecture-based approach
-------------------------------

I won’t go into any specific paper or implementation in detail here, but I will provide a brief overview of this approach and a couple different techniques. I recommend reading this section (4.5) of the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf). It is also easier to read than the other sections.

1.  Parameter Allocation: Here, a subset of the network parameters is dedicated to each task. This can be done either by masking out irrelevant neurons or by explicitly identifying important ones for the current task.
2.  Modular Network: This involves using separate sub-networks or modules for each task.

Sub-networks can be connected in various ways to form an ensemble or a more complex architecture. Below are a few common methods for connecting sub-networks:

Concatenation of Outputs:
-------------------------

In this approach, the outputs of multiple sub-networks are concatenated into a single tensor, which can then be passed through additional layers to produce the final output.

Voting Mechanism:
-----------------

In some models, each sub-network casts a “vote” on the likely outcome, and the final decision is made by taking the majority vote or a weighted vote. This has biological inspiration as it’s similar to how different cortical columns in the neocortex cast votes.

Skip Connections:
-----------------

Some architectures allow sub-networks to have skip connections to other parts of the model, allowing information to flow across modules.

Sequential:
-----------

In this case, the output of one sub-network serves as the input to the next.

Going back to talking about chatbots, what I find particularly interesting if it were possible to create such an architecture with two sub-networks. The first one is the pre-trained model which holds the general “knowledge”. The second holds knowledge for aligning the model. Once the model is aligned, it would no longer need labeled conversational data. Instead, it could be continually updated by training the pre-trained subnetwork in an unsupervised way.

**Conclusion**
--------------

In conclusion the subfield of continual learning in deep learning is challenging and mostly unknown. This is because we do not fully understand how the neurons in LLMs work, and as outlined in the intro, could also be that current network architectures, or deep learning in general, is just not suited for it.

I noticed last month that ChatGPT (GPT-4 only) had [been updated](https://stackdiary.com/chatgpts-cutoff-date-upgraded-to-january-2022/) as it now says “Since my training cutoff in January 2022”, so I wonder what the folks at OpenAI did to achieve this.

![Image 33](https://miro.medium.com/v2/resize:fit:700/1*T1c2i3dHLZ87O9v9rQMibA.png)

ChatGPT (GPT-4 variant) telling the user it is trained up until January 2022 (screenshot by author)

## Metadata

```json
{
  "title": "The current state of continual learning in AI: Why is chatGPT only trained up until 2021? | Towards Data Science",
  "description": "Why is chatGPT only trained up until 2021? This article attempts to explain the current state of continual learning in deep learning, with a focus on large language models and chatbots.",
  "url": "https://towardsdatascience.com/the-current-state-of-continual-learning-in-ai-af4a05c42f3c",
  "content": "Why is ChatGPT only trained up until 2021?\n------------------------------------------\n\n[![Image 25: Jon Flynn](https://miro.medium.com/v2/resize:fill:88:88/1*16AI0ZxosqDanJ22tGkA_Q.jpeg)](https://medium.com/@jon.flynn2?source=post_page---byline--af4a05c42f3c--------------------------------)\n\n[![Image 26: Towards Data Science](https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg)](https://towardsdatascience.com/?source=post_page---byline--af4a05c42f3c--------------------------------)\n\n![Image 27](https://miro.medium.com/v2/resize:fit:700/1*vjERthvscIdEQAgIEm5PEg.png)\n\nImage generated by author using DALL-E 3\n\n**Knowledge prerequisites:**\n----------------------------\n\nA couple of years ago, I learned the basics of deep learning through StatQuest videos, Lena Voita’s NLP blogs, and books like “Deep Learning for Coders” and “Talking Nets.” I’m now wanting to understand the current state of continual learning in deep learning. I found that there is not much information available that summarises this topic in simpler terms, and it requires sifting through expert research papers. Therefore, this article is intended for readers who have a basic understanding of the topic but find the research difficult to read and may not be experts. It holds a focus on chatbots, so knowing the training stages of chatGPT is also helpful.\n\n**Intro**\n---------\n\n![Image 28](https://miro.medium.com/v2/resize:fit:700/1*4UAraN_gKdcYcQ7nZy6vSQ.png)\n\nChatGPT telling the user it is only trained up until September 2021 (screenshot by author)\n\nIf large language models like ChatGPT could be continuously updated with new data, they could accelerate a wide range of tasks, from software development to legal processes to education and learning.\n\nContinual learning is the ability to pause the model training process, save the model’s current state, and then later resume training on new data. The model should be able to generalise well to new data, while still maintaining its ability to generalise to old data. Refer to [this paper](https://arxiv.org/pdf/2302.00487.pdf) for a more formal definition.\n\nPresently, the trend in the industry to augment chatbots with more data is to use RAG, combining queried vectors with prompt engineering to answer questions, rather than continuing to train the LLM with new data. ChatGPT’s zero-shot learning capability, which allows it to answer questions about new, unseen data, makes this approach very appealing. For instance, you could teach it a new programming language and then ask it questions about that language, with just a few prompts, although performance does degrade a bit proportionally to the amount of tokens input. Continually training the model to answer questions based on a new topic like this requires significant computing resources and more importantly, a wide variety of data on the relevant topic. Furthermore, if a topic has very low prevalence in the training set, it will generalise poorly to it. E.g.: take an unpopular public repo and it will know little about it and may hallucinate, despite having seen it at some point during the training process. Context windows (the amount of tokens the model can take as input) are getting increasingly larger very quickly, making RAG even more attractive. Ideally though, do we not want one intelligent all-knowing model, without the need for any external database?\n\nContinual learning is an essential step towards AGI, and some doubt we will even be able to achieve it without significant changes in deep learning network architectures. Jeff Hawkins in his book, [“A Thousand Brains”](https://www.numenta.com/resources/books/a-thousand-brains-by-jeff-hawkins/), stated he does not think current ANN’s are capable of effective continual learning, and believes future models will probably need to be architected more similarly to the human brain using his theory on reference frames in the cortical columns of the neocortex.\n\nContinual Learning in the pre-training vs fine-tuning stages of language models\n-------------------------------------------------------------------------------\n\nEarlier this year, a research paper called [“LIMA: Less Is More for Alignment”](https://arxiv.org/abs/2305.11206) was published. It introduced a chatbot that was not trained using Reinforcement Learning from Human Feedback (RLHF), but was instead fine-tuned (via cross-entropy loss) on just 1,000 carefully annotated question-and-answer samples. Surprisingly, the researchers said that in 43% of cases, “the chatbot’s responses were on par with those of GPT-4”. I did not take an in-depth look at how these were evaluated, but nonetheless, it’s widely acknowledged that a substantial amount of the model’s knowledge and capability is acquired during the pre-training phase, and research like this may further prove this.\n\nModels like ChatGPT and Llama-chat have undergone extensive fine-tuning to generate more aligned and effective responses. OpenAI currently offer an API to further [fine-tune a model](https://platform.openai.com/docs/guides/fine-tuning), which takes Q&A data as input to be used for further training. However, this should not be used to teach the model new data, but rather to [customise the tone and steerability](https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates). Fine-tuning a model in attempt to teach it new data can cause _catastrophic forgetting_, a problem where the model forgets what it has already learned. This article will go over some techniques that aim to mitigate this problem.\n\nThis also leads us to a couple key questions about the feasibility and strategy of continual learning:\n\n*   At which stage of development is it most beneficial and easiest to introduce continual learning?\n*   Given that fine-tuning (without LoRA) alters the entire model’s parameters, is it even possible to revert to the pre-training stage for further modification?\n\n_Note: I provide some PyTorch-like pseudocode for some of the papers discussed below. It has not been tested and may not work, it’s used to break the techniques down step by step and translate any confusing math notation to help the reader understand._\n\nThe 5 sub-categories of continual learning techniques\n-----------------------------------------------------\n\nThe [comprehensive overview of continual learning paper](https://arxiv.org/pdf/2302.00487.pdf) states training strategies for continual learning can be divided into 5 sub categories:\n\n1.  Regularisation-based approach: this approach adds constraints or penalties to the learning process during the training process.\n2.  Optimisation-based approach: this technique focuses on modifying the optimisation algorithm.\n3.  Representation-based approach: this aims to learn a shared feature representation across different tasks, helping the model generalise better to new but related tasks.\n4.  Replay-based approach: this involves storing some data or learned features from previous tasks and replaying them during training on new tasks to maintain performance on earlier learned tasks. In other words, mixing both the old and new datasets when training on new tasks.\n5.  Architecture-based approach: in this approach, the network architecture is dynamically adjusted, often by growing or partitioning, delegating different parts of the network to different tasks.\n\n1\\. Regularisation-based approaches\n-----------------------------------\n\nSoft Masking of Parameters\n--------------------------\n\nThe following soft-masking techniques mask and adjust the gradients of each parameter during the training process. The _optimisation-based approaches_ coming up also manipulate the gradients for continual learning. Remember the gradients aren’t just temporary numbers that appear and disappear during training; they’re signals that guide the evolution of the weights.\n\nSPG\n---\n\nThis [paper](https://arxiv.org/pdf/2306.14775.pdf) proposes a technique named SPG (Soft-masking of Parameter-level Gradient flow) which aims to:\n\n1.  Train the model on each task until convergence.\n2.  After training, calculate the “importance” of each parameter for the task.\n3.  Soft-mask parameters based on their accumulated importance, making important parameters less likely to change during the learning of new tasks.\n\nLet’s break the approach down step by step:\n\n1\\. Training the First Task\n---------------------------\n\nTrain the model on the first task’s dataset as normal.\n\n2\\. Calculate Parameter Importance for the First Task\n-----------------------------------------------------\n\nAfter the training of the first task is complete, we calculate the importance of each model parameter. The intuition here is simple, we use the gradients of each parameter to compute its importance. A larger gradient implies that a small change in that parameter will result in a larger change in the loss, meaning the model’s performance could vary more significantly, hence that parameter is important.\n\nThe gradients are also normalised, because gradients in the first layer could be small, while those in the last layer could be large. If you’re calculating importance based on these raw gradient values, parameters in the last layer would seem more important because of the scale of their gradients, not necessarily because they are genuinely more crucial for the task.\n\n![Image 29](https://miro.medium.com/v2/resize:fit:700/1*nuTWjoen0I34R2BCNeIxRg.png)\n\nEquations for calculating the importance of the model parameters in SPG (section 3.1 of [paper](https://arxiv.org/pdf/2306.14775.pdf))\n\nLet’s translate this calculation to PyTorch-like pseudocode:\n\nimport torchdef compute\\_final\\_importance(model, loss\\_function, data\\_loader):  \n    # Get a single batch from the data loader  \n    inputs, labels = next(iter(data\\_loader))\n\n# Forward and backward pass to calculate the gradients for all parameters  \n    outputs = model(inputs)  \n    loss = loss\\_function(outputs, labels)  \n    loss.backward()\n\nimportances = \\[\\]\n\n# Calculate importance based on the gradients  \n    for param in model.parameters():  \n        if param.grad is not None:  # Gradients may be None for some unused parameters  \n            normalized\\_grad = (param.grad - torch.mean(param.grad)) / torch.std(param.grad)  \n            importance = torch.tanh(normalized\\_grad)  \n            importances.append(importance)\n\nreturn torch.stack(importances).mean(dim=0)\n\n3\\. Accumulating Importance Across Tasks\n----------------------------------------\n\nThe accumulated importance of each parameter across task is simply calculated by taking the max value at any stage.\n\n4\\. Training Subsequent Tasks, combined loss and the soft-masking mechanism:\n----------------------------------------------------------------------------\n\nWhen training on new tasks, the researchers use a combined loss function consisting of two parts. One is the standard loss function which is used as normal on the new task and data, and the second is an additional loss function which involves putting the _new_ data through the _old_ model (the converged model checkpoint after the previous task) and summing up the logits produced. In classification networks the logits are usually the raw non normalised predictions generated by the model in one of the last layers before going through something like a softmax function. This sum of logits serves as a form of loss. The rationale is that if the summed logits are significantly affected when the model parameters change, those parameters are crucial for the performance of the previously learned task.\n\nThe gradients generated from this additional loss serve as a guide during backpropagation, nudging the shared parameters to change in a direction that is less likely to harm performance on the first task. It therefore acts as a sort of penalty term to enforce that any updates made to the model do not lead to a significant loss of information related to previous tasks.\n\nTrain the model on the next task. Use a standard training loop, but modify the gradients during backpropagation based on their accumulated importance. This is the soft-masking mechanism:\n\nimport torchaccumulated\\_importance = # calculated at the end of each task\n\nfor epoch in range(num\\_epochs):  \n  for x, y in train\\_loader:\n\n# Forward Pass: Calculate the loss for the current task using the proper loss function  \n    logits = new\\_model(x)  \n    loss\\_current\\_task = nn.CrossEntropyLoss()(logits, y)\n\n# Forward Pass: Calculate the additional losses for previous tasks (CHI mechanism)  \n    loss\\_previous\\_tasks = 0  \n    for prev\\_task\\_id in range(task\\_id):  \n        logits\\_prev = old\\_model(x, prev\\_task\\_id)  \n        loss\\_previous\\_tasks += logits\\_prev.sum()\n\n# Combine the losses  \n    combined\\_loss = loss\\_current\\_task + loss\\_previous\\_tasks\n\n# Backward Pass  \n    optimizer.zero\\_grad()  \n    combined\\_loss.backward()\n\n# Update the accumulated importance  \n    for param, acc\\_imp in zip(model.parameters(), accumulated\\_importance):  \n        grad = param.grad  \n        acc\\_imp = torch.max(acc\\_imp, torch.abs(grad))\n\n# Soft-masking the gradients before taking an optimization step  \n    for param, imp in zip(model.parameters(), accumulated\\_importance):  \n        param.grad \\*= (1 - importance)\n\noptimizer.step()\n\n5\\. Soft-Masking Special Cases\n------------------------------\n\n*   Feature Extractor: Gradients of parameters in the shared feature extractor are modified based on their specific accumulated importance.\n*   Classification Head: For the classification head, gradients are modified based on the average importance of the feature extractor.\n\nApplying this to LLMs\n---------------------\n\nBear in mind, this paper does not experiment this with a language model, but I assume in a language model you could think of the transformer layers as analogous to the “feature extractor,” and the final classification layer (which predicts the next word or token in the sequence) as the “classification head.”\n\nSoft-masking applied to continual pre-training in a language model\n------------------------------------------------------------------\n\nNext we’ll go into a paper which applies similar soft-masking to the pre-training stage in language modelling.\n\n[This paper](https://arxiv.org/pdf/2302.03241.pdf) introduces a technique called DAS (Continual DA-pre-training of LMs with Soft-masking) for continual learning in the pre-training stage of a large language model. It applies a soft-masking technique similar to the one just discussed along with a couple other techniques in attempt to continue pre-training of an LLM without running into catastrophic forgetting.\n\nLet’s break it down step by step:\n\nInitial Pre-training Phase\n--------------------------\n\nPre-train the LLM like normal.\n\nFurther Pre-training on A New Domain\n------------------------------------\n\nPrepare New Domain Data:\n------------------------\n\nA new dataset from a different domain is prepared.\n\nCalculating the importance of each neuron\n-----------------------------------------\n\nSPG used gradients to determine the importance of each parameter, and then applied the calculated importance value to mask the gradient adjustments of parameters during training. This paper tries to determine the importance of each unit/neuron, rather than parameter, and then uses this in the same way by masking the gradient during training.\n\nThis paper uses two different methods to calculate the importance of neurons, depending on the task at hand. One, a gradient-based importance detection method (originally outlined in [this paper](https://arxiv.org/pdf/1905.10650.pdf)), and two, a custom “proxy loss function”.\n\nThe first introduced is _not_ used in the continual learning of the _first_ new domain. Why? It needs data from the training dataset to work and the authors state that users “don’t have access to the massive original pre-training dataset”, which is a fair assumption. The proxy loss function is used instead for the first phase of continual learning and then for each subsequent phase the other method is used.\n\n**The proxy loss function (“Proxy KL-divergence loss”):**\n\nI found this term confusing at first, but it’s called this because the original gradient-based importance detection method is defined as a loss function itself, which you can then use to run the network’s outputs through to get the gradients of each neuron, which can then be used to derive importance, just like the SPG technique. It’s calculated by the following:\n\n*   Take a subset of the new domain we’re wanting to train on and feed it twice through the model to get two different representations. These representations will differ a bit due to the existing dropout masks in the Transformer architecture.\n*   Compute the KL-divergence between these two representations.\n\nModified Backpropagation Flow with Proxy and Combined Loss\n----------------------------------------------------------\n\n1.  **Forward Pass:** Data goes through a forward pass in the neural network.\n2.  **Backpropagation:**\n\n**Apply Proxy Loss for Gradient Adjustment:** The proxy loss function’s unit-level importance is used to soft-mask the original gradients. This is expressed as:\n\nadjusted\\_grad \\*= (1 − unit\\_level\\_importance)\n\n**Calculate Combined Loss (MLM + Contrastive Loss):** Compute the combined loss using both MLM and contrastive loss.\n\nFurther Pre-training on More Domains\n------------------------------------\n\n1.  **Direct Importance Calculation:** For each new domain, the importance of each unit can now be directly calculated using the data from the new domain via the gradient-based method outlined in equation 3, eliminating the need for the proxy loss function which is only once used after the initial pre-training.\n2.  **The importance of neurons is updated incrementally as each new task is learned.** This update is done using element-wise max. “Element-wise maximum (EMax) operation” refers to comparing two vectors element by element, and taking the maximum value for each corresponding element to create a new vector. E.g.: if you have two vectors A and B of the same length, the element-wise maximum will result in a new vector C where each element _C_\\[_i_\\] is the maximum between _A_\\[_i_\\] and _B_\\[_i_\\].\n\n2\\. Optimisation-based approaches\n---------------------------------\n\nWe’ll refer to the two techniques outlined in the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf) in section 3.1\n\nGradient Direction Preservation\n-------------------------------\n\nThe paper talks about manipulating the gradient-based optimisation process to make the gradient _directions_ of new training samples close to those from old training samples. The formula\n\n> ⟨ ∇θ Lₖ(θ; Dₖ), ∇θ Lₖ(θ; Mₜ) ⟩ ≥ 0\n\nenforces that learning the new task should not increase the loss for the old tasks. Essentially, the gradients of the new task and the old tasks are encouraged to align.\n\nBreaking down the formula, we take the dot product of the gradient of the loss from the new task (∇θ Lₖ(θ; Dₖ)) and the gradient of the loss from the old task (∇θ Lₖ(θ; Mₜ)) should be non-negative. In this context, a positive dot product implies that the gradients for the old task and the new task are generally pointing in the same direction, with the angle between these two vectors is less than or equal to 90 degrees.\n\nForward/Backward Passes:\n------------------------\n\nForward Pass:\n-------------\n\nYou would run your input data _Dₖ_ for the new task and _Mₜ_​ for the old task through the same model to calculate the loss for each.\n\nBackward Pass:\n--------------\n\n1.  Compute the gradients of the loss with respect to the network parameters for both the old and new task.\n2.  Alignment Check: Compute the dot product of the two gradients. You’d then use this information to modify the gradients for the new task in such a way that the dot product is non-negative.\n3.  Update Weights: Update the model parameters using these “aligned” gradients.\n\n  \nimport torch\\# Forward pass for the new task  \noutput\\_k = model(D\\_k)  \nloss\\_k = criterion(output\\_k, y\\_k)\n\n\\# Forward pass for the old task  \noutput\\_t = model(M\\_t)  \nloss\\_t = criterion(output\\_t, y\\_t)\n\n\\# Compute gradients for both tasks  \nloss\\_k.backward(retain\\_graph=True)  # Compute gradients for new task but keep computation graph  \ngrad\\_k = torch.cat(\\[p.grad.view(-1) for p in model.parameters()\\])\n\noptimizer.zero\\_grad()\n\nloss\\_t.backward()  # Compute gradients for old task  \ngrad\\_t = torch.cat(\\[p.grad.view(-1) for p in model.parameters()\\])\n\n\\# Compute dot product and modify gradients if they don't align  \ndot\\_product = torch.dot(grad\\_k, grad\\_t)  \nif dot\\_product < 0:  \n    # I'm not sure how you modify the gradients here if they don't align, I'm not sure the paper specifies it\n\n\\# Use the modified gradient to update model parameters  \nindex = 0  \nfor p in model.parameters():  \n    num\\_params = p.numel()  \n    # Update using modified gradients  \n    p.grad = grad\\_k\\[index: index + num\\_params\\].view(p.shape)  \n    index += num\\_params\n\noptimizer.step()\n\nGradient Direction Preservation without needing old training samples\n--------------------------------------------------------------------\n\nThe text also highlights that gradient projection can be performed even without storing old samples. NCL (Natural continual learning, [paper link](https://proceedings.neurips.cc/paper/2021/hash/ec5aa0b7846082a2415f0902f0da88f2-Abstract.html)) is the technique summarised here. Note, this can be categorised as both a regularisation and optimisation based approach.\n\nTraining process step by step:\n------------------------------\n\n**Forward Pass:**\n-----------------\n\nYou would run your new data through the network and calculate the loss as usual.\n\n**Backward Pass:**\n------------------\n\n**Objective:** The aim is to minimise the task-specific loss _ℓk(θ)_ while adhering to a distance constraint _d_(_θ_,_θ_+_δ_)≤_r._\n\n**Algorithm step by step**:\n\n1.  As normal, compute the gradient of the loss with respect to the model parameters ∇_θ_​ℓ_k_​(_θ_).\n2.  The _δ_ is calculated using the update rule. This gives you the “suggested” changes to the model parameters _θ_ based on the new task’s requirements.\n3.  Then, you plug this _δ_ into the distance constraint formula: _d(θ,θ+δ)=squareroot(δ⊤Λ\\_k-1​δ)_​. The constraint acts like a boundary around the current parameters _θ_, defined by the distance metric _d_(_θ_,_θ_+_δ_) and the radius _r_. I struggled to see why they called it a “radius”, and not just “constraint number” or something. I think it’s because the researchers are visualising the gradients and training process in a high-dimensional space. When you apply a constraint based on the distance metric, you’re essentially defining a “sphere” around your current parameter values in that high-dimensional space. The “radius” _r_ of this sphere sets a limit on how much the parameter can move while learning a new task.\n4.  If the proposed _δ_ would move _θ_ too far according to this distance metric, i.e., beyond this boundary, you scale it down so that it stays within the allowable region defined by the radius _r_.\n\nLet’s look at each bit more in-depth:\n\n**Update Rule:** The update rule provides a direction in which _θ_ should move.\n\n![Image 30](https://miro.medium.com/v2/resize:fit:688/1*AG0E-ZngB2ZAvk1VbDkZ_g.png)\n\nNCL update rule from section 3.1 in the [comprehensive overview of continual learning paper](https://arxiv.org/pdf/2302.00487.pdf)\n\nBreaking it down:\n\n*   _∇θ ℓk(θ)_ represents the gradients for all parameters (_θ)_ calculated by the loss function.\n*   Parameter importance calculation (_Λ^(k-1)\\_(-1)_): This term represents a _precision matrix_ and it is yet another way to calculate the importance of parameters in the network. _more details below_\n*   Regularisation Term (_θ — μ\\_(k-1)_): This term pulls the updated parameters closer to the optimal parameters _μ\\_(k-1)_​ from the previous task. Like the before techniques, it acts as a regulariser to avoid deviation from what was already learned.\n*   Learning Rate (_λ_)\n\n**Distance Constraint:** Before applying this update, you’d usually check whether this change _δ_ would violate the distance constraint _d_(_θ_,_θ_+_δ_)≤_r_. If it does, you’d typically scale down _δ_ so that it satisfies the constraint.\n\n**Precision matrix explanation:** before in the soft-masking methods we saw the calculation of importance via the output of all neurons or their gradients. In this method a precision matrix is used. This is a bit more complex so I’ll attempt to explain it:\n\nWe first calculate the _covariance matrix_ for the networks parameters. In the context of neural networks, the columns in the gradient matrix _G_ correspond to the parameters (weights and biases) of the model. Each row in _G_ represents the gradient vector for a single training example, with respect to all of those parameters.\n\nSo, if you have a neural network with _P_ parameters (this includes all the weights and biases from all layers), then each gradient vector will have _P_ elements, one for each parameter. Therefore, _G_ will be a matrix of shape _N_ × _P_, _N_ representing each batch and therefore each row representing the average gradient vector across all the training examples in a given batch.\n\nWhen you calculate the covariance matrix Σ from _G_, the resulting matrix will have dimensions _P_ × _P_. The diagonal entries Σ_ii_​ will indicate the variance of the gradient with respect to the _ith_ parameter, and the off-diagonal entries Σ_ij_​ will indicate the covariance between the gradients with respect to the _ith_ and _jth_ parameters. This gives you an idea of how these parameters interact or co-vary during the training process. The inverse of this matrix is the _precision matrix_, which is what we use to determine importance.\n\nWhy the _precision matrix_ over the _covariance matrix_? While the covariance matrix Σ does capture how parameters interact with each other during training, it doesn’t specifically indicate how crucial each parameter is to the task at hand when all other parameters are considered. In contrast, the precision matrix allows us to assess the _conditional independence_ (this is a concept in probability theory, look it up) of parameters. Large values in the precision matrix indicate that knowing one parameter is highly informative about another, given all the other parameters. I’m not going to go into examples of how this works so get ChatGPT to generate some examples using a very small neural network to see how the values can be interpreted.\n\nPrevious methods we saw that calculate importance focus on individual neurons or parameters, ignoring the relationships between them. The precision matrix, on the other hand, can capture these relationships. Like everything in deep learning, whether this is a better way to calculate the importance of a network, is going to be empirical and could differ depending on the task and scale of the network.\n\n**Algorithm step by step in PyTorch:**\n\nimport torch\\# Constraint radius  \nradius = 0.1\n\nfor epoch in range(num\\_epochs):    \n    for batch\\_idx, (data, target) in enumerate(data\\_loader):  \n        optimizer.zero\\_grad()\n\n# Forward pass  \n        output = model(data)  \n        loss = loss\\_function(output, target)\n\n# Backward pass to get gradients for params  \n        loss.backward()  \n        model\\_grad = torch.cat(\\[p.grad.data.view(-1) for p in model.parameters()\\])\n\n# Compute δ using the NCL method  \n        # δ = Λ^(-1) \\* grad - (θ - µ)  \n        delta = torch.matmul(torch.inverse(covarianceMatrix), model\\_grad) - (torch.cat(\\[p.data.view(-1) for p in model.parameters()\\]) - parametersForPrevTask)\n\n# Check constraint  \n        if torch.norm(delta) \\> radius:  \n            delta = radius \\* delta / torch.norm(delta)\n\n# Update model parameters (θ) using δ  \n        idx = 0  \n        for p in model.parameters():  \n            length = p.data.numel()  \n            p.data += delta\\[idx: idx + length\\].view(p.data.shape)  \n            idx += length\n\n# Update Λ and µ for the next task, probably going to be task-specific and non-trivial\n\n3\\. Representation-based approach\n---------------------------------\n\nFirstly, it’s important to note that the pre-training of LLM’s to be further fine-tuned on a downstream task is an example of continual learning in this sub-category. I think ChatGPT’s ability to reason about never-before-seen data is also an example of this approach. Although we technically call it zero-shot learning, and the term “continual learning” requires updating model parameters, it goes beyond anything we’ve seen before. As discussed in the introduction, prompt engineering could be the future of continual learning, instead of continually updating the parameters.\n\nBelow we’ll take a look at using knowledge distillation for continual learning. I’m not really sure which sub-category this falls under, but I’d guess it’s probably a mix between representation, architecture and replay approaches. Even though some of the techniques we’re reviewing may seem random and unproven at large scale, breakthroughs in this field are often unpredictable. Therefore, it’s important to maintain a broad perspective.\n\nKnowledge Distillation for continual learning\n---------------------------------------------\n\nYou can transfer (or “distill”) the knowledge of one network into another network, and the second network does a reasonable job of approximating the function learned by the original network.\n\nThe distilled model (the _student_), is trained to mimic the output of the larger network (the _teacher_), instead of training it on the raw data directly. For example, say you want to train a smaller student model to mimic a large pre-trained language model (the teacher). Run the original pre-training dataset through the teacher model to generate “soft targets.” These are probability distributions over potential outputs, i.e.: next-word predictions. For instance, for a next-word prediction task, instead of predicting “cat,” the teacher might provide probabilities like 90% for “cat”, 5% for “kitten”, 3% for “feline”, etc.\n\nThis is usually done to transfer knowledge to much smaller models, and it yields great results despite the smaller model.\n\nLet’s see how some researchers [applied this with success](https://ojs.aaai.org/index.php/AAAI/article/view/17600) to a NER (named entity recognition) model. The training process is fairly straightforward:\n\nTraining process step by step\n-----------------------------\n\nThere are two primary methods outlined in the paper: AddNER and ExtendNER.\n\nAddNER Model\n------------\n\nNote, NER models work by taking a sequence of tokens (usually a sentence) as input and then output a probability distribution (for the different types of entities) for each token. IOB tagging is commonly used for NER models, each token can be labeled as ‘O’, or as the beginning (‘B-’) or inside (‘I-’) of an entity of type _X_. ‘O’ stands for ‘Outside’, it just means the current token doesn’t belong to any entity. Therefore, for _n_ entity types, you will have 2_n_ output neurons in the classification layer: _n_ for the ‘B-’ tags (one for each entity type) and _n_ for the ‘I-’ tags (again, one for each entity type). Add to this the ‘O’ label, which signifies that a token doesn’t belong to any entity, and you end up with 2_n_ \\+ 1 possible labels for each token. The final dimensions can be written as _h_ × (2_n_ \\+ 1), where _h_ is the size of the hidden layer’s output. Bear in mind, this is only for models where tokens can only be one entity. E.g.: “Apple” could be tagged as both “FOOD” and “COMPANY”.\n\nArchitecture and teacher-student setup\n--------------------------------------\n\nThe student model in this case is a copy of the teacher model, with an additional output classification layer for each new entity type that the model should learn. During training, the new output layer learns from the new annotated data, and the older layers are guided by the teacher model’s outputs to minimise forgetting.\n\nAfter training, the old output layers are not discarded. It then uses the algorithm and heuristics described in the conflict resolver section _(end of section 3.3)_ to combine these outputs into a single, final prediction for each token in the sequence.\n\n![Image 31](https://miro.medium.com/v2/resize:fit:700/1*rlmiNVZXHCJmieYjBucCaQ.png)\n\nDiagram of the AddNER model from section 3.2 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)\n\nForward Pass\n------------\n\n1.  Old Entity Types: The input sentence is passed through the teacher model to obtain probability distributions (the “soft targets” in this context) for the old entity types.\n2.  New Entity Types: The same sentence is also passed through the new student model with additional output layers specific to the new entity types​.\n\nBackward Pass\n-------------\n\n**Combined loss function:**\n\n1.  KD Loss: calculated by comparing how closely the output probabilities of the old entity types from the new model (student) match those from the old model (teacher). It uses KL-divergence to calculate this. It’s probably calculated token-by-token and then summed or averaged over all tokens in a sentence or batch, but I don’t think the paper goes into this.\n2.  Cross-Entropy Loss: This is the usual loss function that compares the model’s predictions for the new entity types against the actual labels from the new dataset.\n3.  Combining the two: these two losses are combined into a combined loss by taking a weighted sum of them both. The weights for combining these losses are set by the hyperparameters alpha and beta, which are adjusted like any other hyperparameter to better performance based on experiments.\n\n\\# Hyperparameters alpha and beta for weighting the two loss functions  \nalpha = 0.5  \nbeta = 0.5for epoch in range(num\\_epochs):  \n    for sentence, labels in D\\_new:  \n        # Forward pass in teacher model for old entity types  \n        teacher\\_probs\\_Ei = teacher\\_model(sentence)\n\n# Forward pass in student model for old and new entity types  \n        # Note: the new entity types must go through the new output layer (not shown in this pseudocode)  \n        student\\_probs\\_Ei, student\\_probs\\_Enew = student\\_model(sentence)\n\n# Compute KD loss  \n        kd\\_loss = KL\\_divergence(teacher\\_probs\\_Ei, student\\_probs\\_Ei)\n\n# Compute CE loss for new entity types  \n        ce\\_loss = cross\\_entropy(labels, student\\_probs\\_Enew)\n\n# Combined loss  \n        total\\_loss = alpha \\* kd\\_loss + beta \\* ce\\_loss\n\n# Backward pass  \n        total\\_loss.backward()\n\n# Update student model parameters  \n        optimizer.step()\n\nExtendNER Model\n---------------\n\nArchitecture and teacher-student setup\n--------------------------------------\n\nThe ExtendNER model extends the output layer dimensions to accommodate new entity types, instead of adding new output layers. The paper explains quite simply how the dimensions are to be:\n\n“Assuming that _Mi_ was able to recognize _n_ entity types, its final layer can be considered as a matrix with dimension _h×(2n+1)_. The output layer of _Mi+1_ will then be extended to be a matrix with dimension _h × (2n + 2m + 1)_ in order to accommodate the new entity types.”\n\n![Image 32](https://miro.medium.com/v2/resize:fit:700/1*k1hdglsBtvNEpZQYdBqXOA.png)\n\nDiagram of the ExtendNER model from section 3.4 of the [paper](https://ojs.aaai.org/index.php/AAAI/article/view/17600)\n\nForward Pass\n------------\n\nSame as in AddNER, but with extended dimensions.\n\nBackward Pass\n-------------\n\nThe loss calculation uses _either_ the KL-divergence loss or the cross-entropy loss, depending on the following:\n\n*   When the NER category label _y_ is “O” (from the IOB tagging schema), the KL divergence loss is used.\n*   When the category label _y_ is NOT “O”, the Cross-Entropy loss is used.\n\nFinal Prediction\n----------------\n\nViterbi algorithm is applied to decode the final entity types.\n\nBoth AddNER and ExtendNER models performed well for continual learning and the results did not differ between them much\n\n4\\. Replay-based approach\n-------------------------\n\n“Fine-tuned language models are continual learners”\n---------------------------------------------------\n\n[paper link](https://arxiv.org/pdf/2205.12393.pdf)\n\nThe model in the paper is not a generic, single-task model like GPT trained just for conversational response. Instead, it’s fine-tuned for a sequence of specialised tasks, ranging from text simplification to Haiku generation. Each of these tasks has unique requirements, evaluation metrics, and specialised training datasets.\n\nThe researchers mix parts of the old dataset with the new dataset, and achieve great results by mixing in just 1% of the previous task’s dataset when fine-tuning on a new task. This is done sequentially for many tasks (8). The model also performs well in zero-shot learning settings, meaning it can generalise well to tasks it hasn’t been trained on. For instance, it can generate a Haiku with the correct syllable count when given an unseen topic, showing its ability to generalise. The researchers also mention that their approach is task-order invariant, meaning the sequence in which tasks are learned does not affect the model’s performance. The experiments find that the amount of the old dataset mixed in with the new one doesn’t significantly affect the main task’s performance. However, it does affect the zero-shot learning. At 0% rehearsal, the model tends to forget the zero-shot tasks, while at 1% rehearsal, the model maintains its performance in those tasks very well.\n\nThis all seems positive, the fact we can just add 1% of the old dataset and continual learning is solved, but of course, applying it to a chatbot like chatGPT, will be empirical and can be completely different. Even if, hypothetically, chatGPT could be continually trained in the fine-tuning stages like this, it would require an immense amount of labeled conversation data.\n\n5\\. Architecture-based approach\n-------------------------------\n\nI won’t go into any specific paper or implementation in detail here, but I will provide a brief overview of this approach and a couple different techniques. I recommend reading this section (4.5) of the [comprehensive survey paper](https://arxiv.org/pdf/2302.00487.pdf). It is also easier to read than the other sections.\n\n1.  Parameter Allocation: Here, a subset of the network parameters is dedicated to each task. This can be done either by masking out irrelevant neurons or by explicitly identifying important ones for the current task.\n2.  Modular Network: This involves using separate sub-networks or modules for each task.\n\nSub-networks can be connected in various ways to form an ensemble or a more complex architecture. Below are a few common methods for connecting sub-networks:\n\nConcatenation of Outputs:\n-------------------------\n\nIn this approach, the outputs of multiple sub-networks are concatenated into a single tensor, which can then be passed through additional layers to produce the final output.\n\nVoting Mechanism:\n-----------------\n\nIn some models, each sub-network casts a “vote” on the likely outcome, and the final decision is made by taking the majority vote or a weighted vote. This has biological inspiration as it’s similar to how different cortical columns in the neocortex cast votes.\n\nSkip Connections:\n-----------------\n\nSome architectures allow sub-networks to have skip connections to other parts of the model, allowing information to flow across modules.\n\nSequential:\n-----------\n\nIn this case, the output of one sub-network serves as the input to the next.\n\nGoing back to talking about chatbots, what I find particularly interesting if it were possible to create such an architecture with two sub-networks. The first one is the pre-trained model which holds the general “knowledge”. The second holds knowledge for aligning the model. Once the model is aligned, it would no longer need labeled conversational data. Instead, it could be continually updated by training the pre-trained subnetwork in an unsupervised way.\n\n**Conclusion**\n--------------\n\nIn conclusion the subfield of continual learning in deep learning is challenging and mostly unknown. This is because we do not fully understand how the neurons in LLMs work, and as outlined in the intro, could also be that current network architectures, or deep learning in general, is just not suited for it.\n\nI noticed last month that ChatGPT (GPT-4 only) had [been updated](https://stackdiary.com/chatgpts-cutoff-date-upgraded-to-january-2022/) as it now says “Since my training cutoff in January 2022”, so I wonder what the folks at OpenAI did to achieve this.\n\n![Image 33](https://miro.medium.com/v2/resize:fit:700/1*T1c2i3dHLZ87O9v9rQMibA.png)\n\nChatGPT (GPT-4 variant) telling the user it is trained up until January 2022 (screenshot by author)",
  "publishedTime": "2023-10-18T17:33:27.604Z",
  "usage": {
    "tokens": 8857
  }
}
```
