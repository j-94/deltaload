---
title: Advancing AI for Humanity
description: 
url: https://thegenerality.com/agi/
timestamp: 2025-01-20T16:18:34.450Z
domain: thegenerality.com
path: agi
---

# Advancing AI for Humanity



## Content

[Star](https://github.com/microsoft/unilm)[...](https://github.com/microsoft/unilm)

#### [A New Paradigm of Generative AI](https://thegenerality.com/agi/#)

_coming_, Jan 1, 2025

#### [The Next Recipe / Dec 15, 2024](https://thegenerality.com/agi/about.html)

[#### The Second Curve of Scaling Law / Jan 15, 2024](https://thegenerality.com/agi/assets/doc/The%20Second%20Curve%20of%20Scaling%20Law_Furu%20Wei_public.pdf)[#### Scaling Factors / Jun 15, 2024](https://thegenerality.com/agi/assets/doc/Scaling%20Factors_Furu%20Wei_public.pdf)

[![Image 93: BitNet](https://thegenerality.com/agi/assets/img/latentlm.png) #### A New Paradigm of Multimodality: Multimodal Latent Language Modeling with Next-Token Diffusion Dec 12, 2024](https://arxiv.org/abs/2412.08635)

[![Image 94: BitNet](https://thegenerality.com/agi/assets/img/diff.png) #### Differential Transformer Oct 7, 2024](https://arxiv.org/abs/2410.05258)

[![Image 95: BitNet](https://thegenerality.com/agi/assets/img/bitnet_b158.png) #### The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits Feb 28, 2024](https://arxiv.org/abs/2402.17764)

[![Image 96: YOCO](https://thegenerality.com/agi/assets/img/YOCO.png) #### You Only Cache Once: Decoder-Decoder Architectures for Large Language Models // Gated RetNet (RetNet-3) May 9, 2024](https://arxiv.org/abs/2405.05254)

[![Image 97: BitNet](https://thegenerality.com/agi/assets/img/bitnet-a4-8.png) #### BitNet a4.8: 4-bit Activations for 1-bit LLMs Nov 8, 2024](https://arxiv.org/abs/2411.04965)

[![Image 98: MoE](https://thegenerality.com/agi/assets/img/mvot.png) #### Imagine while Reasoning in Space: Multimodal Visualization-of-Thought Jan 13, 2025](https://arxiv.org/abs/2501.07542)

[![Image 99: MoE](https://thegenerality.com/agi/assets/img/byocl.png) #### Bootstrap Your Own Context Length Dec 25, 2024](https://arxiv.org/abs/2412.18860)

[![Image 100: MoE](https://thegenerality.com/agi/assets/img/mhmoe.png) #### MH-MoE (v2): Multi-Head Mixture-of-Experts Nov 26, 2024](https://arxiv.org/abs/2411.16205)

[![Image 101: Fully Sparsely-Activated LLMs](https://thegenerality.com/agi/assets/img/bitnet-cpp.png) #### 1-bit AI Infra / bitnet.cpp: Running LLMs on CPUs Oct 17, 2024](https://arxiv.org/abs/2410.16144)

[![Image 102: Fully Sparsely-Activated LLMs](https://thegenerality.com/agi/assets/img/qsparse.png) #### Q-Sparse / Block Q-Sparse: Fully Sparsely-Activated LLMs Jul 15, 2024](https://arxiv.org/abs/2407.10969)

[![Image 103: BitNet](https://thegenerality.com/agi/assets/img/bitnet_s_shape.png) #### The Era of 1-bit LLMs: Training Tips, Code and FAQ Mar 20, 2024](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)

[![Image 104: MoE](https://thegenerality.com/agi/assets/img/melle.png) #### MELLE: Autoregressive Speech Synthesis without Vector Quantization Jul 11, 2024](https://arxiv.org/abs/2407.08551)

[![Image 105: MoE](https://thegenerality.com/agi/assets/img/valle2.png) #### VALL-E 2: Human Parity Zero-Shot Text to Speech Synthesis Jun 8, 2024](https://arxiv.org/abs/2406.05370)

[![Image 106: MoE](https://thegenerality.com/agi/assets/img/mhmoe.png) #### Multi-Head Mixture-of-Experts Apr 23, 2024](https://arxiv.org/abs/2404.15045)

[![Image 107: Learning Law](https://thegenerality.com/agi/assets/img/learning_law.png) #### The Learning Law: Towards Optimal Learning of Language Models Feb 28, 2024](https://arxiv.org/abs/2402.17759)

[![Image 108](https://thegenerality.com/agi/assets/img/vot.png) #### The Mind's Eye of (M)LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models Apr 4, 2024](https://arxiv.org/abs/2404.03622)

[![Image 109](https://thegenerality.com/agi/assets/img/glan.png) #### Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models Feb 20, 2024](https://arxiv.org/abs/2402.13064)

[#### Multilingual E5 Text Embeddings Feb 8, 2024](https://arxiv.org/abs/2402.05672)

[![Image 110](https://thegenerality.com/agi/assets/img/bitnet.png) #### BitNet: 1-bit Transformers and LLMs Oct 18, 2023](https://arxiv.org/abs/2310.11453)

[![Image 111: RetNet](https://thegenerality.com/agi/assets/img/retnet.png) #### Retentive Network: Revolutionizing Transformers for Large Language Models Jul 18, 2023](https://arxiv.org/abs/2307.08621)

[![Image 112: LongViT](https://thegenerality.com/agi/assets/img/longvit.png) #### LongViT (LongNet for Vision): When an Image is Worth 1,024 × 1,024 Words Dec 7, 2023](https://arxiv.org/abs/2312.03558)

[![Image 113: Kosmos](https://thegenerality.com/agi/assets/img/kosmos-g.png) #### Kosmos-G: Generating Images in Context with Multimodal Large Language Models Oct 4, 2023](https://arxiv.org/abs/2310.02992)

[![Image 114: Kosmos 2.5](https://thegenerality.com/agi/assets/img/kosmos-25.png) #### Kosmos-2.5: A Multimodal Literate Model Sep 20, 2023](https://arxiv.org/abs/2309.11419)

[![Image 115: LLM4Science](https://thegenerality.com/agi/assets/img/llm4s.png) #### Large Language Model for Science: A Study on P vs. NP Sep 13, 2023](https://arxiv.org/abs/2309.05689)

[![Image 116: LongNet](https://thegenerality.com/agi/assets/img/longnet.png) #### LongNet: Scaling Transformers to 1,000,000,000 Tokens Jul 6, 2023](https://arxiv.org/abs/2307.02486)

[![Image 117: KOSMOS-2](https://thegenerality.com/agi/assets/img/kosmos2.png) #### Kosmos-2: Grounding Multimodal Large Language Models (MLLMs) to the World Jun 26, 2023](https://arxiv.org/abs/2306.14824)

[![Image 118: KOSMOS-1](https://thegenerality.com/agi/assets/img/kosmos1.png) #### Kosmos-1: A Multimodal Large Language Model (MLLM) Feb 28, 2023](https://arxiv.org/abs/2302.14045)

[![Image 119: VALL-E](https://thegenerality.com/agi/assets/img/valle-watermark.png) #### WavMark: Watermarking for Audio Generation Aug 24, 2023](https://arxiv.org/abs/2308.12770)

[![Image 120: VALL-E](https://thegenerality.com/agi/assets/img/valle.png) #### VALL-E (X): Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers Jan 6, 2023](https://arxiv.org/abs/2301.02111)

[![Image 121: LLMA](https://thegenerality.com/agi/assets/img/PoSE.png) #### PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training Sep 19, 2023](https://arxiv.org/abs/2309.10400)

[![Image 122: LLMA](https://thegenerality.com/agi/assets/img/adallm.png) #### AdaLLM: Adapting Large Language Models via Reading Comprehension Sep 18, 2023](https://arxiv.org/abs/2309.09530)

[![Image 123: MiniLLM](https://thegenerality.com/agi/assets/img/minillm.png) #### MiniLLM: Knowledge Distillation of Large Language Models Jun 14, 2023](https://arxiv.org/abs/2306.08543)

[![Image 124: LongMem](https://thegenerality.com/agi/assets/img/LongMem.png) #### Large Language Models with Long-Term Memory Jun 12, 2023](https://arxiv.org/abs//2306.07174)

[![Image 125: LLMA](https://thegenerality.com/agi/assets/img/llma.png) #### LLM Accelerator: Lossless Acceleration of Large Language Models Apr 11, 2023](https://arxiv.org/abs/2304.04487)

[![Image 126: XPos](https://thegenerality.com/agi/assets/img/sope.png) #### A Length-Extrapolatable Transformer Dec 20, 2022](https://arxiv.org/abs/2212.10554)

[![Image 127: ICL](https://thegenerality.com/agi/assets/img/meta_opt.png) #### Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers Dec 20, 2022](https://arxiv.org/abs/2212.10559)

[![Image 128: Promptist](https://thegenerality.com/agi/assets/img/promptist.png) #### Promptist: Optimizing Prompts for Text-to-Image Generation Dec 19, 2022](https://arxiv.org/abs/2212.09611)

[![Image 129: Structured Prompting](https://thegenerality.com/agi/assets/img/structured_prompt.png) #### Structured Prompting: Scaling In-Context Learning to 1,000 Examples Dec 12, 2022](https://arxiv.org/abs/2212.06713)

[![Image 130: TorchScale](https://thegenerality.com/agi/assets/img/torchscale.png) #### TorchScale: Transformers at (Any) Scale Nov 24, 2022](https://github.com/microsoft/torchscale)

[![Image 131](https://thegenerality.com/agi/assets/img/magnet.png) #### Magneto: A Foundation Transformer October 13, 2022](https://arxiv.org/abs/2210.06423)

[![Image 132: BEiT-3](https://thegenerality.com/agi/assets/img/beit3.png) #### BEiT-3: A General-Purpose Multimodal Foundation Model Aug 30, 2022](https://arxiv.org/abs/2208.10442)

[![Image 133](https://thegenerality.com/agi/assets/img/metalm.png) #### Language Models are General-Purpose Interfaces June 13, 2022](https://arxiv.org/abs/2206.06336)

[![Image 134](https://thegenerality.com/agi/assets/img/deepnet.png) #### DeepNet: Scaling Transformers to 1,000 Layers Mar 1, 2022](https://arxiv.org/abs/2203.00555)

[![Image 135](https://thegenerality.com/agi/assets/img/beit.png) #### BEiT: BERT Pre-Training of Image Transformers June 15, 2021](https://arxiv.org/abs/2106.08254)

[![Image 136](https://thegenerality.com/agi/assets/img/minilm2.png) #### MiniLM: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers Feb 25, 2021](https://arxiv.org/abs/2012.15828)

[![Image 137](https://thegenerality.com/agi/assets/img/xlme.png) #### XLM-E: Efficient Multilingual Language Model Pre-training June 30, 2021](https://arxiv.org/abs/2106.16138)

[![Image 138](https://thegenerality.com/agi/assets/img/unilm.png) #### UniLM: Unified Language Model Pre-training May 8, 2019](https://arxiv.org/abs/1905.03197)

## Metadata

```json
{
  "title": "Advancing AI for Humanity",
  "description": "",
  "url": "https://thegenerality.com/agi/",
  "content": "[Star](https://github.com/microsoft/unilm)[...](https://github.com/microsoft/unilm)\n\n#### [A New Paradigm of Generative AI](https://thegenerality.com/agi/#)\n\n_coming_, Jan 1, 2025\n\n#### [The Next Recipe / Dec 15, 2024](https://thegenerality.com/agi/about.html)\n\n[#### The Second Curve of Scaling Law / Jan 15, 2024](https://thegenerality.com/agi/assets/doc/The%20Second%20Curve%20of%20Scaling%20Law_Furu%20Wei_public.pdf)[#### Scaling Factors / Jun 15, 2024](https://thegenerality.com/agi/assets/doc/Scaling%20Factors_Furu%20Wei_public.pdf)\n\n[![Image 93: BitNet](https://thegenerality.com/agi/assets/img/latentlm.png) #### A New Paradigm of Multimodality: Multimodal Latent Language Modeling with Next-Token Diffusion Dec 12, 2024](https://arxiv.org/abs/2412.08635)\n\n[![Image 94: BitNet](https://thegenerality.com/agi/assets/img/diff.png) #### Differential Transformer Oct 7, 2024](https://arxiv.org/abs/2410.05258)\n\n[![Image 95: BitNet](https://thegenerality.com/agi/assets/img/bitnet_b158.png) #### The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits Feb 28, 2024](https://arxiv.org/abs/2402.17764)\n\n[![Image 96: YOCO](https://thegenerality.com/agi/assets/img/YOCO.png) #### You Only Cache Once: Decoder-Decoder Architectures for Large Language Models // Gated RetNet (RetNet-3) May 9, 2024](https://arxiv.org/abs/2405.05254)\n\n[![Image 97: BitNet](https://thegenerality.com/agi/assets/img/bitnet-a4-8.png) #### BitNet a4.8: 4-bit Activations for 1-bit LLMs Nov 8, 2024](https://arxiv.org/abs/2411.04965)\n\n[![Image 98: MoE](https://thegenerality.com/agi/assets/img/mvot.png) #### Imagine while Reasoning in Space: Multimodal Visualization-of-Thought Jan 13, 2025](https://arxiv.org/abs/2501.07542)\n\n[![Image 99: MoE](https://thegenerality.com/agi/assets/img/byocl.png) #### Bootstrap Your Own Context Length Dec 25, 2024](https://arxiv.org/abs/2412.18860)\n\n[![Image 100: MoE](https://thegenerality.com/agi/assets/img/mhmoe.png) #### MH-MoE (v2): Multi-Head Mixture-of-Experts Nov 26, 2024](https://arxiv.org/abs/2411.16205)\n\n[![Image 101: Fully Sparsely-Activated LLMs](https://thegenerality.com/agi/assets/img/bitnet-cpp.png) #### 1-bit AI Infra / bitnet.cpp: Running LLMs on CPUs Oct 17, 2024](https://arxiv.org/abs/2410.16144)\n\n[![Image 102: Fully Sparsely-Activated LLMs](https://thegenerality.com/agi/assets/img/qsparse.png) #### Q-Sparse / Block Q-Sparse: Fully Sparsely-Activated LLMs Jul 15, 2024](https://arxiv.org/abs/2407.10969)\n\n[![Image 103: BitNet](https://thegenerality.com/agi/assets/img/bitnet_s_shape.png) #### The Era of 1-bit LLMs: Training Tips, Code and FAQ Mar 20, 2024](https://github.com/microsoft/unilm/blob/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf)\n\n[![Image 104: MoE](https://thegenerality.com/agi/assets/img/melle.png) #### MELLE: Autoregressive Speech Synthesis without Vector Quantization Jul 11, 2024](https://arxiv.org/abs/2407.08551)\n\n[![Image 105: MoE](https://thegenerality.com/agi/assets/img/valle2.png) #### VALL-E 2: Human Parity Zero-Shot Text to Speech Synthesis Jun 8, 2024](https://arxiv.org/abs/2406.05370)\n\n[![Image 106: MoE](https://thegenerality.com/agi/assets/img/mhmoe.png) #### Multi-Head Mixture-of-Experts Apr 23, 2024](https://arxiv.org/abs/2404.15045)\n\n[![Image 107: Learning Law](https://thegenerality.com/agi/assets/img/learning_law.png) #### The Learning Law: Towards Optimal Learning of Language Models Feb 28, 2024](https://arxiv.org/abs/2402.17759)\n\n[![Image 108](https://thegenerality.com/agi/assets/img/vot.png) #### The Mind's Eye of (M)LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models Apr 4, 2024](https://arxiv.org/abs/2404.03622)\n\n[![Image 109](https://thegenerality.com/agi/assets/img/glan.png) #### Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models Feb 20, 2024](https://arxiv.org/abs/2402.13064)\n\n[#### Multilingual E5 Text Embeddings Feb 8, 2024](https://arxiv.org/abs/2402.05672)\n\n[![Image 110](https://thegenerality.com/agi/assets/img/bitnet.png) #### BitNet: 1-bit Transformers and LLMs Oct 18, 2023](https://arxiv.org/abs/2310.11453)\n\n[![Image 111: RetNet](https://thegenerality.com/agi/assets/img/retnet.png) #### Retentive Network: Revolutionizing Transformers for Large Language Models Jul 18, 2023](https://arxiv.org/abs/2307.08621)\n\n[![Image 112: LongViT](https://thegenerality.com/agi/assets/img/longvit.png) #### LongViT (LongNet for Vision): When an Image is Worth 1,024 × 1,024 Words Dec 7, 2023](https://arxiv.org/abs/2312.03558)\n\n[![Image 113: Kosmos](https://thegenerality.com/agi/assets/img/kosmos-g.png) #### Kosmos-G: Generating Images in Context with Multimodal Large Language Models Oct 4, 2023](https://arxiv.org/abs/2310.02992)\n\n[![Image 114: Kosmos 2.5](https://thegenerality.com/agi/assets/img/kosmos-25.png) #### Kosmos-2.5: A Multimodal Literate Model Sep 20, 2023](https://arxiv.org/abs/2309.11419)\n\n[![Image 115: LLM4Science](https://thegenerality.com/agi/assets/img/llm4s.png) #### Large Language Model for Science: A Study on P vs. NP Sep 13, 2023](https://arxiv.org/abs/2309.05689)\n\n[![Image 116: LongNet](https://thegenerality.com/agi/assets/img/longnet.png) #### LongNet: Scaling Transformers to 1,000,000,000 Tokens Jul 6, 2023](https://arxiv.org/abs/2307.02486)\n\n[![Image 117: KOSMOS-2](https://thegenerality.com/agi/assets/img/kosmos2.png) #### Kosmos-2: Grounding Multimodal Large Language Models (MLLMs) to the World Jun 26, 2023](https://arxiv.org/abs/2306.14824)\n\n[![Image 118: KOSMOS-1](https://thegenerality.com/agi/assets/img/kosmos1.png) #### Kosmos-1: A Multimodal Large Language Model (MLLM) Feb 28, 2023](https://arxiv.org/abs/2302.14045)\n\n[![Image 119: VALL-E](https://thegenerality.com/agi/assets/img/valle-watermark.png) #### WavMark: Watermarking for Audio Generation Aug 24, 2023](https://arxiv.org/abs/2308.12770)\n\n[![Image 120: VALL-E](https://thegenerality.com/agi/assets/img/valle.png) #### VALL-E (X): Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers Jan 6, 2023](https://arxiv.org/abs/2301.02111)\n\n[![Image 121: LLMA](https://thegenerality.com/agi/assets/img/PoSE.png) #### PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training Sep 19, 2023](https://arxiv.org/abs/2309.10400)\n\n[![Image 122: LLMA](https://thegenerality.com/agi/assets/img/adallm.png) #### AdaLLM: Adapting Large Language Models via Reading Comprehension Sep 18, 2023](https://arxiv.org/abs/2309.09530)\n\n[![Image 123: MiniLLM](https://thegenerality.com/agi/assets/img/minillm.png) #### MiniLLM: Knowledge Distillation of Large Language Models Jun 14, 2023](https://arxiv.org/abs/2306.08543)\n\n[![Image 124: LongMem](https://thegenerality.com/agi/assets/img/LongMem.png) #### Large Language Models with Long-Term Memory Jun 12, 2023](https://arxiv.org/abs//2306.07174)\n\n[![Image 125: LLMA](https://thegenerality.com/agi/assets/img/llma.png) #### LLM Accelerator: Lossless Acceleration of Large Language Models Apr 11, 2023](https://arxiv.org/abs/2304.04487)\n\n[![Image 126: XPos](https://thegenerality.com/agi/assets/img/sope.png) #### A Length-Extrapolatable Transformer Dec 20, 2022](https://arxiv.org/abs/2212.10554)\n\n[![Image 127: ICL](https://thegenerality.com/agi/assets/img/meta_opt.png) #### Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta Optimizers Dec 20, 2022](https://arxiv.org/abs/2212.10559)\n\n[![Image 128: Promptist](https://thegenerality.com/agi/assets/img/promptist.png) #### Promptist: Optimizing Prompts for Text-to-Image Generation Dec 19, 2022](https://arxiv.org/abs/2212.09611)\n\n[![Image 129: Structured Prompting](https://thegenerality.com/agi/assets/img/structured_prompt.png) #### Structured Prompting: Scaling In-Context Learning to 1,000 Examples Dec 12, 2022](https://arxiv.org/abs/2212.06713)\n\n[![Image 130: TorchScale](https://thegenerality.com/agi/assets/img/torchscale.png) #### TorchScale: Transformers at (Any) Scale Nov 24, 2022](https://github.com/microsoft/torchscale)\n\n[![Image 131](https://thegenerality.com/agi/assets/img/magnet.png) #### Magneto: A Foundation Transformer October 13, 2022](https://arxiv.org/abs/2210.06423)\n\n[![Image 132: BEiT-3](https://thegenerality.com/agi/assets/img/beit3.png) #### BEiT-3: A General-Purpose Multimodal Foundation Model Aug 30, 2022](https://arxiv.org/abs/2208.10442)\n\n[![Image 133](https://thegenerality.com/agi/assets/img/metalm.png) #### Language Models are General-Purpose Interfaces June 13, 2022](https://arxiv.org/abs/2206.06336)\n\n[![Image 134](https://thegenerality.com/agi/assets/img/deepnet.png) #### DeepNet: Scaling Transformers to 1,000 Layers Mar 1, 2022](https://arxiv.org/abs/2203.00555)\n\n[![Image 135](https://thegenerality.com/agi/assets/img/beit.png) #### BEiT: BERT Pre-Training of Image Transformers June 15, 2021](https://arxiv.org/abs/2106.08254)\n\n[![Image 136](https://thegenerality.com/agi/assets/img/minilm2.png) #### MiniLM: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers Feb 25, 2021](https://arxiv.org/abs/2012.15828)\n\n[![Image 137](https://thegenerality.com/agi/assets/img/xlme.png) #### XLM-E: Efficient Multilingual Language Model Pre-training June 30, 2021](https://arxiv.org/abs/2106.16138)\n\n[![Image 138](https://thegenerality.com/agi/assets/img/unilm.png) #### UniLM: Unified Language Model Pre-training May 8, 2019](https://arxiv.org/abs/1905.03197)",
  "usage": {
    "tokens": 3090
  }
}
```
