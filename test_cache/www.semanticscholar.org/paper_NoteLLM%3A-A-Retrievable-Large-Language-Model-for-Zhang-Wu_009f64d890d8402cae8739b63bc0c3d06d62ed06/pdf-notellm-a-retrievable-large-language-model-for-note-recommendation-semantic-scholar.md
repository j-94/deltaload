---
title: [PDF] NoteLLM: A Retrievable Large Language Model for Note Recommendation | Semantic Scholar
description: A novel unified framework called NoteLLM is proposed, which leverages LLMs to address the item-to-item (I2I) note recommendation and uses Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.
url: https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06
timestamp: 2025-01-20T15:59:22.013Z
domain: www.semanticscholar.org
path: paper_NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu_009f64d890d8402cae8739b63bc0c3d06d62ed06
---

# [PDF] NoteLLM: A Retrievable Large Language Model for Note Recommendation | Semantic Scholar


A novel unified framework called NoteLLM is proposed, which leverages LLMs to address the item-to-item (I2I) note recommendation and uses Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. People enjoy sharing "notes" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.


## Content

\[PDF\] NoteLLM: A Retrievable Large Language Model for Note Recommendation | Semantic Scholar
===============
                                                              

[Skip to search form](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#search-form)[Skip to main content](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#account-menu)

[](https://www.semanticscholar.org/)

Search 223,685,258 papers from all fields of science

Search

Sign InCreate Free Account

*   DOI:[10.1145/3589335.3648314](https://doi.org/10.1145/3589335.3648314)
    
*   Corpus ID: 268247665

NoteLLM: A Retrievable Large Language Model for Note Recommendation
===================================================================

@article{Zhang2024NoteLLMAR,
  title={NoteLLM: A Retrievable Large Language Model for Note Recommendation},
  author={Chao Zhang and Shiwei Wu and Haoxin Zhang and Tong Xu and Yan Gao and Yao Hu and Enhong Chen},
  journal={Companion Proceedings of the ACM on Web Conference 2024},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:268247665}
}

*   [Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374), [Shiwei Wu](https://www.semanticscholar.org/author/Shiwei-Wu/2142349315), +4 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)
*   Published in [The Web Conference](https://www.semanticscholar.org/venue?name=The%20Web%20Conference) 4 March 2024
*   Computer Science

TLDR

A novel unified framework called NoteLLM is proposed, which leverages LLMs to address the item-to-item (I2I) note recommendation and uses Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach.Expand

[View on ACM](http://dl.acm.org/citation.cfm?id=3648314 "http://dl.acm.org/citation.cfm?id=3648314")

[](https://www.semanticscholar.org/reader/009f64d890d8402cae8739b63bc0c3d06d62ed06)\[PDF\] Semantic Reader

Save to LibrarySave

Create AlertAlert

Cite

Share

15 Citations

[Highly Influential Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)

1

[Background Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)

6

[Methods Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)

2

[View All](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)

Figures and Tables from this paper
----------------------------------

*   [![Image 10: figure 1](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/1-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/0)
*   [![Image 11: table 1](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/5-Table1-1.png) table 1](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/1)
*   [![Image 12: figure 2](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/4-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/2)
*   [![Image 13: table 2](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/6-Table2-1.png) table 2](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/3)
*   [![Image 14: table 3](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/6-Table3-1.png) table 3](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/4)
*   [![Image 15: figure 3](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/8-Figure3-1.png) figure 3](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/5)
*   [![Image 16: table 4](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table4-1.png) table 4](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/6)
*   [![Image 17: table 5](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table5-1.png) table 5](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/7)
*   [![Image 18: table 6](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table6-1.png) table 6](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/8)

View All 9 Figures & Tables

Topics
------

AI-Generated

[Large Language Models (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/66772037317?corpusId=268247665)[Recommendation Systems (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/1361345639?corpusId=268247665)[Instruction Tuning (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/60575166671?corpusId=268247665)[Online Community (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/51199142423?corpusId=268247665)

15 Citations
------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### NoteLLM-2: Multimodal Large Representation Models for Recommendation](https://www.semanticscholar.org/paper/NoteLLM-2%3A-Multimodal-Large-Representation-Models-Zhang-Zhang/aac09b738b2f35c03394d01730b3d764783ec41f)

[Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374)[Haoxin Zhang](https://www.semanticscholar.org/author/Haoxin-Zhang/2290148641)+5 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)

Computer Science

[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)

*   2024

TLDR

The potential of LLMs to enhance multimodal representation in multimodal item-to-item (I2I) recommendations is investigated and a novel training framework, NoteLLM-2, specifically designed for multimodal representation is proposed.Expand

*   [7](https://www.semanticscholar.org/paper/aac09b738b2f35c03394d01730b3d764783ec41f#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/aac09b738b2f35c03394d01730b3d764783ec41f?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/aac09b738b2f35c03394d01730b3d764783ec41f)\[PDF\]

*   7 Excerpts

Save

[### Language Representations Can be What Recommenders Need: Findings and Potentials](https://www.semanticscholar.org/paper/Language-Representations-Can-be-What-Recommenders-Sheng-Zhang/cb085ce72c1647d23da2514dc45e74fbf88f9224)

[Leheng Sheng](https://www.semanticscholar.org/author/Leheng-Sheng/2258731846)[An Zhang](https://www.semanticscholar.org/author/An-Zhang/2153659066)[Yi Zhang](https://www.semanticscholar.org/author/Yi-Zhang/2305963111)[Yuxin Chen](https://www.semanticscholar.org/author/Yuxin-Chen/2258784735)[Xiang Wang](https://www.semanticscholar.org/author/Xiang-Wang/2257436645)[Tat-Seng Chua](https://www.semanticscholar.org/author/Tat-Seng-Chua/2257036129)

Computer Science

*   2024

TLDR

These findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance and suggest the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Expand

[](https://www.semanticscholar.org/reader/cb085ce72c1647d23da2514dc45e74fbf88f9224)\[PDF\]

*   1 Excerpt

Save

[### A Survey on Large Language Models for Recommendation](https://www.semanticscholar.org/paper/A-Survey-on-Large-Language-Models-for-Wu-Zheng/b486982fa7c68a8a08df1111ba9607119419c488)

[Likang Wu](https://www.semanticscholar.org/author/Likang-Wu/12892739)[Zhilan Zheng](https://www.semanticscholar.org/author/Zhilan-Zheng/2115548818)+9 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2173129111)

Computer Science

[World Wide Web](https://www.semanticscholar.org/venue?name=World%20Wide%20Web)

*   2024

TLDR

This survey presents a taxonomy that categorizes existing LLM-based recommendation systems into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLL4Rec (GLLM 4Rec), with the latter being systematically sorted out for the first time.Expand

*   [230](https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488#citing-papers)
[](https://www.semanticscholar.org/reader/b486982fa7c68a8a08df1111ba9607119419c488)\[PDF\]

Save

[### Semantic Convergence: Harmonizing Recommender Systems via Two-Stage Alignment and Behavioral Semantic Tokenization](https://www.semanticscholar.org/paper/Semantic-Convergence%3A-Harmonizing-Recommender-via-Li-Zhang/4116a4fe3f6f8d1b72f7621cfd60b26f0780cba4)

[Guanghan Li](https://www.semanticscholar.org/author/Guanghan-Li/2336084275)[Xun Zhang](https://www.semanticscholar.org/author/Xun-Zhang/2336017879)[Yufei Zhang](https://www.semanticscholar.org/author/Yufei-Zhang/2336012903)[Yifan Yin](https://www.semanticscholar.org/author/Yifan-Yin/2336305855)[Guojun Yin](https://www.semanticscholar.org/author/Guojun-Yin/2335868502)[Wei Lin](https://www.semanticscholar.org/author/Wei-Lin/2336243476)

Computer Science

*   2024

TLDR

A novel framework that harmoniously merges traditional recommendation models with the prowess of LLMs is proposed, and extensive experimental evidence indicates that this model markedly improves recall metrics and displays remarkable scalability of recommendation systems.Expand

[](https://www.semanticscholar.org/reader/4116a4fe3f6f8d1b72f7621cfd60b26f0780cba4)\[PDF\]

Save

[### Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://www.semanticscholar.org/paper/Molar%3A-Multimodal-LLMs-with-Collaborative-Filtering-Luo-Qin/3cc1f63eab45d9f1b610176aac2ff8fb39d8c27e)

[Yucong Luo](https://www.semanticscholar.org/author/Yucong-Luo/2208917508)[Qitao Qin](https://www.semanticscholar.org/author/Qitao-Qin/2320834305)+4 authors [Ouyang Jie](https://www.semanticscholar.org/author/Ouyang-Jie/2322501286)

Computer Science

*   2024

TLDR

Molar is proposed, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively and captures both user interests and contextual semantics, leading to superior recommendation accuracy.Expand

[](https://www.semanticscholar.org/reader/3cc1f63eab45d9f1b610176aac2ff8fb39d8c27e)\[PDF\]

*   1 Excerpt

Save

[### CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation](https://www.semanticscholar.org/paper/CoST%3A-Contrastive-Quantization-based-Semantic-for-Zhu-Jin/6d9c4f406caff94da5846a9be6749ffe3505bda2)

[Jieming Zhu](https://www.semanticscholar.org/author/Jieming-Zhu/2290237904)[Mengqun Jin](https://www.semanticscholar.org/author/Mengqun-Jin/2298723722)[Qijiong Liu](https://www.semanticscholar.org/author/Qijiong-Liu/150270469)[Zexuan Qiu](https://www.semanticscholar.org/author/Zexuan-Qiu/2298269415)[Zhenhua Dong](https://www.semanticscholar.org/author/Zhenhua-Dong/2274021958)[Xiu Li](https://www.semanticscholar.org/author/Xiu-Li/2297866189)

Computer Science

[RecSys](https://www.semanticscholar.org/venue?name=RecSys)

*   2024

TLDR

A contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens and achieves significant improvement in Recall@5 and NDCG@5 on the MIND dataset over previous baselines.Expand

*   [2](https://www.semanticscholar.org/paper/6d9c4f406caff94da5846a9be6749ffe3505bda2#citing-papers)
[](https://www.semanticscholar.org/reader/6d9c4f406caff94da5846a9be6749ffe3505bda2)\[PDF\]

*   1 Excerpt

Save

[### RUIE: Retrieval-based Unified Information Extraction using Large Language Model](https://www.semanticscholar.org/paper/RUIE%3A-Retrieval-based-Unified-Information-using-Liao-Duan/091062b1e3be7d3aa291abd9314404482dd9b3a3)

[Xincheng Liao](https://www.semanticscholar.org/author/Xincheng-Liao/2291001583)[Junwen Duan](https://www.semanticscholar.org/author/Junwen-Duan/2259993319)[Yixi Huang](https://www.semanticscholar.org/author/Yixi-Huang/2321656257)[Jianxin Wang](https://www.semanticscholar.org/author/Jianxin-Wang/2199815721)

Computer Science

[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)

*   2024

TLDR

RUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning to enable rapid generalization while reducing computational costs, is proposed and is confirmed as the first trainable retrieval framework for UIE.Expand

[](https://www.semanticscholar.org/reader/091062b1e3be7d3aa291abd9314404482dd9b3a3)\[PDF\]

*   1 Excerpt

Save

[### Large Language Models for Generative Information Extraction: A Survey](https://www.semanticscholar.org/paper/Large-Language-Models-for-Generative-Information-A-Xu-Chen/351a2d50b4aff8e9754dc7074dd589b10a7465d4)

[Derong Xu](https://www.semanticscholar.org/author/Derong-Xu/2262514619)[Wei Chen](https://www.semanticscholar.org/author/Wei-Chen/2260766940)+6 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)

Computer Science, Linguistics

Frontiers Comput. Sci.

*   2024

TLDR

This study surveys the most recent advancements in generative Large Language Models efforts for IE tasks and identifies several insights in technique and promising research directions that deserve further exploration in future studies.Expand

*   [87](https://www.semanticscholar.org/paper/351a2d50b4aff8e9754dc7074dd589b10a7465d4#citing-papers)
[](https://www.semanticscholar.org/reader/351a2d50b4aff8e9754dc7074dd589b10a7465d4)\[PDF\]

*   1 Excerpt

Save

[### Large Language Model Enhanced Recommender Systems: Taxonomy, Trend, Application and Future](https://www.semanticscholar.org/paper/Large-Language-Model-Enhanced-Recommender-Systems%3A-Liu-Zhao/c6dac310a7df61a238f626d89da42baa851d6372)

[Qidong Liu](https://www.semanticscholar.org/author/Qidong-Liu/2240559309)[Xiangyu Zhao](https://www.semanticscholar.org/author/Xiangyu-Zhao/2238104000)+9 authors [Feng Tian](https://www.semanticscholar.org/author/Feng-Tian/2244621655)

Computer Science

*   2024

TLDR

A critical shift in the field is identified with the move towards incorporating LLM into the online system, notably by avoiding their use during inference, and several promising research directions are highlighted that could further advance the field of LLMERS.Expand

*   [Highly Influenced](https://www.semanticscholar.org/paper/c6dac310a7df61a238f626d89da42baa851d6372?sort=is-influential#citing-papers)
    
*   [PDF](https://www.semanticscholar.org/paper/c6dac310a7df61a238f626d89da42baa851d6372)
    

*   3 Excerpts

Save

[### ScalingNote: Scaling up Retrievers with Large Language Models for Real-World Dense Retrieval](https://www.semanticscholar.org/paper/ScalingNote%3A-Scaling-up-Retrievers-with-Large-for-Huang-Zhang/3be8efd48e056e63a933d5a42b844d5713a6d3ff)

[Suyuan Huang](https://www.semanticscholar.org/author/Suyuan-Huang/2166592873)[Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374)+12 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)

Computer Science

*   2024

TLDR

This work proposes ScalingNote, a two-stage method to exploit the scaling potential of LLMs for retrieval while maintaining online query latency, and verifies the scaling law of dense retrieval with LLMs in industrial scenarios, enabling cost-effective scaling of dense retrieval systems.Expand

[](https://www.semanticscholar.org/reader/3be8efd48e056e63a933d5a42b844d5713a6d3ff)\[PDF\]

Save

...

1

2

...

61 References
-------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://www.semanticscholar.org/paper/A-Bi-Step-Grounding-Paradigm-for-Large-Language-in-Bao-Zhang/aae1d88c70cf18ef6aa23693a4dce8204e22d087)

[Keqin Bao](https://www.semanticscholar.org/author/Keqin-Bao/2188063534)[Jizhi Zhang](https://www.semanticscholar.org/author/Jizhi-Zhang/2116265843)+6 authors [Qi Tian](https://www.semanticscholar.org/author/Qi-Tian/2056267912)

Computer Science

[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)

*   2023

TLDR

This paper investigates the comprehensive ranking capacity of LLMs and proposes a two-step grounding framework known as BIGRec (Bi-step Grounding Paradigm for Recommendation), which initially grounds LLMs to the recommendation space by fine-tuning them to generate meaningful tokens for items and subsequently identifies appropriate actual items that correspond to the generated tokens.Expand

*   [51](https://www.semanticscholar.org/paper/aae1d88c70cf18ef6aa23693a4dce8204e22d087#citing-papers)
[](https://www.semanticscholar.org/reader/aae1d88c70cf18ef6aa23693a4dce8204e22d087)\[PDF\]

*   2 Excerpts

Save

[### TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation](https://www.semanticscholar.org/paper/TALLRec%3A-An-Effective-and-Efficient-Tuning-to-Align-Bao-Zhang/3487c12512fa41d3a4d64f00cb842525a8590ad3)

[Keqin Bao](https://www.semanticscholar.org/author/Keqin-Bao/2188063534)[Jizhi Zhang](https://www.semanticscholar.org/author/Jizhi-Zhang/2116265843)[Yang Zhang](https://www.semanticscholar.org/author/Yang-Zhang/2145957648)[Wenjie Wang](https://www.semanticscholar.org/author/Wenjie-Wang/2117833732)[Fuli Feng](https://www.semanticscholar.org/author/Fuli-Feng/2163400298)[Xiangnan He](https://www.semanticscholar.org/author/Xiangnan-He/7792071)

Computer Science

[RecSys](https://www.semanticscholar.org/venue?name=RecSys)

*   2023

TLDR

It is demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples.Expand

*   [232](https://www.semanticscholar.org/paper/3487c12512fa41d3a4d64f00cb842525a8590ad3#citing-papers)
[](https://www.semanticscholar.org/reader/3487c12512fa41d3a4d64f00cb842525a8590ad3)\[PDF\]

Save

[### Large Language Models are Zero-Shot Rankers for Recommender Systems](https://www.semanticscholar.org/paper/Large-Language-Models-are-Zero-Shot-Rankers-for-Hou-Zhang/f4e723958a93762befb4d4a039b44a7d752f9917)

[Yupeng Hou](https://www.semanticscholar.org/author/Yupeng-Hou/151472453)[Junjie Zhang](https://www.semanticscholar.org/author/Junjie-Zhang/2120518257)+4 authors [Wayne Xin Zhao](https://www.semanticscholar.org/author/Wayne-Xin-Zhao/2542603)

Computer Science

[ECIR](https://www.semanticscholar.org/venue?name=ECIR)

*   2024

TLDR

This work investigates the capacity of LLMs that act as the ranking model for recommender systems, and shows that LLMs have promising zero-shot ranking abilities but struggle to perceive the order of historical interactions, and can be biased by popularity or item positions in the prompts.Expand

*   [230](https://www.semanticscholar.org/paper/f4e723958a93762befb4d4a039b44a7d752f9917#citing-papers)
[](https://www.semanticscholar.org/reader/f4e723958a93762befb4d4a039b44a7d752f9917)\[PDF\]

*   1 Excerpt

Save

[### Training Large-Scale News Recommenders with Pretrained Language Models in the Loop](https://www.semanticscholar.org/paper/Training-Large-Scale-News-Recommenders-with-Models-Xiao-Liu/62842f5a587485e59068e216425b10362b1113e1)

[Shitao Xiao](https://www.semanticscholar.org/author/Shitao-Xiao/2051175765)[Zheng Liu](https://www.semanticscholar.org/author/Zheng-Liu/2145976175)+4 authors [Xing Xie](https://www.semanticscholar.org/author/Xing-Xie/2110972323)

Computer Science

[KDD](https://www.semanticscholar.org/venue?name=KDD)

*   2022

TLDR

A novel framework, SpeedyFeed, which efficiently trains PLMs-based news recommenders of superior quality and is applied to Microsoft News to empower the training of large-scale production models, which demonstrate highly competitive online performances.Expand

*   [36](https://www.semanticscholar.org/paper/62842f5a587485e59068e216425b10362b1113e1#citing-papers)
[](https://www.semanticscholar.org/reader/62842f5a587485e59068e216425b10362b1113e1)\[PDF\]

Save

[### A Survey on Large Language Models for Recommendation](https://www.semanticscholar.org/paper/A-Survey-on-Large-Language-Models-for-Wu-Zheng/b486982fa7c68a8a08df1111ba9607119419c488)

[Likang Wu](https://www.semanticscholar.org/author/Likang-Wu/12892739)[Zhilan Zheng](https://www.semanticscholar.org/author/Zhilan-Zheng/2115548818)+9 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2173129111)

Computer Science

[World Wide Web](https://www.semanticscholar.org/venue?name=World%20Wide%20Web)

*   2024

TLDR

This survey presents a taxonomy that categorizes existing LLM-based recommendation systems into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLL4Rec (GLLM 4Rec), with the latter being systematically sorted out for the first time.Expand

*   [230](https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488#citing-papers)
[](https://www.semanticscholar.org/reader/b486982fa7c68a8a08df1111ba9607119419c488)\[PDF\]

*   2 Excerpts

Save

[### Microblog Hashtag Generation via Encoding Conversation Contexts](https://www.semanticscholar.org/paper/Microblog-Hashtag-Generation-via-Encoding-Contexts-Wang-Li/99526e14a0954217f5261bed6a56c548f4e1ef97)

[Yue Wang](https://www.semanticscholar.org/author/Yue-Wang/49416727)[Jing Li](https://www.semanticscholar.org/author/Jing-Li/49297986)[Irwin King](https://www.semanticscholar.org/author/Irwin-King/145310663)[M. Lyu](https://www.semanticscholar.org/author/M.-Lyu/145609003)[Shuming Shi](https://www.semanticscholar.org/author/Shuming-Shi/34720053)

Computer Science

[NAACL](https://www.semanticscholar.org/venue?name=NAACL)

*   2019

TLDR

This work is the first effort to annotate hashtags with a novel sequence generation framework via viewing the hashtag as a short sequence of words, and proposes to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention.Expand

*   [27](https://www.semanticscholar.org/paper/99526e14a0954217f5261bed6a56c548f4e1ef97#citing-papers)
[](https://www.semanticscholar.org/reader/99526e14a0954217f5261bed6a56c548f4e1ef97)\[PDF\]

*   1 Excerpt

Save

[### One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://www.semanticscholar.org/paper/One-Embedder%2C-Any-Task%3A-Instruction-Finetuned-Text-Su-Shi/63c0dbe2426f9c92f469151d1773e5265ae6580e)

[Hongjin Su](https://www.semanticscholar.org/author/Hongjin-Su/2152173042)[Weijia Shi](https://www.semanticscholar.org/author/Weijia-Shi/3040379)+7 authors [Tao Yu](https://www.semanticscholar.org/author/Tao-Yu/48881008)

Computer Science

[ACL](https://www.semanticscholar.org/venue?name=ACL)

*   2023

TLDR

INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training, and achieves state-of-the-art performance.Expand

*   [232](https://www.semanticscholar.org/paper/63c0dbe2426f9c92f469151d1773e5265ae6580e#citing-papers)
[](https://www.semanticscholar.org/reader/63c0dbe2426f9c92f469151d1773e5265ae6580e)\[PDF\]

*   1 Excerpt

Save

[### A First Look at LLM-Powered Generative News Recommendation](https://www.semanticscholar.org/paper/A-First-Look-at-LLM-Powered-Generative-News-Liu-Chen/0f4d00d01d43d3967ee92b58481b5ad530a944d1)

[Qijiong Liu](https://www.semanticscholar.org/author/Qijiong-Liu/150270469)[Nuo Chen](https://www.semanticscholar.org/author/Nuo-Chen/2257286538)[Tetsuya Sakai](https://www.semanticscholar.org/author/Tetsuya-Sakai/2257233277)[Xiao-Ming Wu](https://www.semanticscholar.org/author/Xiao-Ming-Wu/2187512110)

Computer Science

[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)

*   2023

TLDR

GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data is introduced, to provide a flexible and unified solution for news recommendation.Expand

*   [40](https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1)
    

*   1 Excerpt

Save

[### Is ChatGPT a Good Recommender? A Preliminary Study](https://www.semanticscholar.org/paper/Is-ChatGPT-a-Good-Recommender-A-Preliminary-Study-Liu-Liu/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3)

[Junling Liu](https://www.semanticscholar.org/author/Junling-Liu/2109104637)[Chaoyong Liu](https://www.semanticscholar.org/author/Chaoyong-Liu/50557623)[Renjie Lv](https://www.semanticscholar.org/author/Renjie-Lv/2214809142)[Kangdi Zhou](https://www.semanticscholar.org/author/Kangdi-Zhou/96746184)[Y. Zhang](https://www.semanticscholar.org/author/Y.-Zhang/2152822535)

Computer Science, Linguistics

[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)

*   2023

TLDR

This paper employs ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios and explores the use of few-shot prompting to inject interaction information that contains user potential interest to helpChatGPT better understand user needs and interests.Expand

*   [207](https://www.semanticscholar.org/paper/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3#citing-papers)
[](https://www.semanticscholar.org/reader/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3)\[PDF\]

*   3 Excerpts

Save

[### Large Language Model based Long-tail Query Rewriting in Taobao Search](https://www.semanticscholar.org/paper/Large-Language-Model-based-Long-tail-Query-in-Peng-Li/abf1ddd4f48b2cee7e5c71ee4609f07209189c75)

[Wenjun Peng](https://www.semanticscholar.org/author/Wenjun-Peng/1765218)[Guiyang Li](https://www.semanticscholar.org/author/Guiyang-Li/2187871782)+5 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)

Computer Science

[WWW](https://www.semanticscholar.org/venue?name=WWW)

*   2024

TLDR

BEQUE is presented, a comprehensive framework that Bridges the semantic gap for long-tail QUE for e-commerce search and can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries.Expand

*   [21](https://www.semanticscholar.org/paper/abf1ddd4f48b2cee7e5c71ee4609f07209189c75#citing-papers)
[](https://www.semanticscholar.org/reader/abf1ddd4f48b2cee7e5c71ee4609f07209189c75)\[PDF\]

*   1 Excerpt

Save

...

1

2

3

4

5

...

Related Papers
--------------

Showing 1 through 3 of 0 Related Papers

*   [Figures and Tables](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#extracted)
*   [Topics](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#paper-topics)
*   [15 Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)
*   [61 References](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#cited-papers)
*   [Related Papers](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#related-papers)

Stay Connected With Semantic Scholar

Sign Up

What Is Semantic Scholar?
-------------------------

Semantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.

[Learn More](https://www.semanticscholar.org/about)

### About

[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)

### Product

[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)

### API

[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)

### Research

[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)

### Help

[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact

Proudly built by [Ai2 (opens in a new tab)](http://allenai.org/)

Collaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)

[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)

By clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)

ACCEPT & CONTINUE

## Metadata

```json
{
  "title": "[PDF] NoteLLM: A Retrievable Large Language Model for Note Recommendation | Semantic Scholar",
  "description": "A novel unified framework called NoteLLM is proposed, which leverages LLMs to address the item-to-item (I2I) note recommendation and uses Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. People enjoy sharing \"notes\" including their experiences within online communities. Therefore, recommending notes aligned with user interests has become a crucial task. Existing online methods only input notes into BERT-based models to generate note embeddings for assessing similarity. However, they may underutilize some important cues, e.g., hashtags or categories, which represent the key concepts of notes. Indeed, learning to generate hashtags/categories can potentially enhance note embeddings, both of which compress key note information into limited content. Besides, Large Language Models (LLMs) have significantly outperformed BERT in understanding natural languages. It is promising to introduce LLMs into note recommendation. In this paper, we propose a novel unified framework called NoteLLM, which leverages LLMs to address the item-to-item (I2I) note recommendation. Specifically, we utilize Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach. Moreover, we use NoteLLM to summarize the note and generate the hashtag/category automatically through instruction tuning. Extensive validations on real scenarios demonstrate the effectiveness of our proposed method compared with the online baseline and show major improvements in the recommendation system of Xiaohongshu.",
  "url": "https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06",
  "content": "\\[PDF\\] NoteLLM: A Retrievable Large Language Model for Note Recommendation | Semantic Scholar\n===============\n                                                              \n\n[Skip to search form](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#search-form)[Skip to main content](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#account-menu)\n\n[](https://www.semanticscholar.org/)\n\nSearch 223,685,258 papers from all fields of science\n\nSearch\n\nSign InCreate Free Account\n\n*   DOI:[10.1145/3589335.3648314](https://doi.org/10.1145/3589335.3648314)\n    \n*   Corpus ID: 268247665\n\nNoteLLM: A Retrievable Large Language Model for Note Recommendation\n===================================================================\n\n@article{Zhang2024NoteLLMAR,\n  title={NoteLLM: A Retrievable Large Language Model for Note Recommendation},\n  author={Chao Zhang and Shiwei Wu and Haoxin Zhang and Tong Xu and Yan Gao and Yao Hu and Enhong Chen},\n  journal={Companion Proceedings of the ACM on Web Conference 2024},\n  year={2024},\n  url={https://api.semanticscholar.org/CorpusID:268247665}\n}\n\n*   [Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374), [Shiwei Wu](https://www.semanticscholar.org/author/Shiwei-Wu/2142349315), +4 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)\n*   Published in [The Web Conference](https://www.semanticscholar.org/venue?name=The%20Web%20Conference) 4 March 2024\n*   Computer Science\n\nTLDR\n\nA novel unified framework called NoteLLM is proposed, which leverages LLMs to address the item-to-item (I2I) note recommendation and uses Note Compression Prompt to compress a note into a single special token, and further learn the potentially related notes' embeddings via a contrastive learning approach.Expand\n\n[View on ACM](http://dl.acm.org/citation.cfm?id=3648314 \"http://dl.acm.org/citation.cfm?id=3648314\")\n\n[](https://www.semanticscholar.org/reader/009f64d890d8402cae8739b63bc0c3d06d62ed06)\\[PDF\\] Semantic Reader\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n15 Citations\n\n[Highly Influential Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n\n1\n\n[Background Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)\n\n6\n\n[Methods Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)\n\n2\n\n[View All](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)\n\nFigures and Tables from this paper\n----------------------------------\n\n*   [![Image 10: figure 1](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/1-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/0)\n*   [![Image 11: table 1](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/5-Table1-1.png) table 1](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/1)\n*   [![Image 12: figure 2](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/4-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/2)\n*   [![Image 13: table 2](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/6-Table2-1.png) table 2](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/3)\n*   [![Image 14: table 3](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/6-Table3-1.png) table 3](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/4)\n*   [![Image 15: figure 3](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/8-Figure3-1.png) figure 3](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/5)\n*   [![Image 16: table 4](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table4-1.png) table 4](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/6)\n*   [![Image 17: table 5](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table5-1.png) table 5](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/7)\n*   [![Image 18: table 6](https://figures.semanticscholar.org/009f64d890d8402cae8739b63bc0c3d06d62ed06/7-Table6-1.png) table 6](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06/figure/8)\n\nView All 9 Figures & Tables\n\nTopics\n------\n\nAI-Generated\n\n[Large Language Models (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/66772037317?corpusId=268247665)[Recommendation Systems (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/1361345639?corpusId=268247665)[Instruction Tuning (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/60575166671?corpusId=268247665)[Online Community (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/51199142423?corpusId=268247665)\n\n15 Citations\n------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### NoteLLM-2: Multimodal Large Representation Models for Recommendation](https://www.semanticscholar.org/paper/NoteLLM-2%3A-Multimodal-Large-Representation-Models-Zhang-Zhang/aac09b738b2f35c03394d01730b3d764783ec41f)\n\n[Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374)[Haoxin Zhang](https://www.semanticscholar.org/author/Haoxin-Zhang/2290148641)+5 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n*   2024\n\nTLDR\n\nThe potential of LLMs to enhance multimodal representation in multimodal item-to-item (I2I) recommendations is investigated and a novel training framework, NoteLLM-2, specifically designed for multimodal representation is proposed.Expand\n\n*   [7](https://www.semanticscholar.org/paper/aac09b738b2f35c03394d01730b3d764783ec41f#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/aac09b738b2f35c03394d01730b3d764783ec41f?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/aac09b738b2f35c03394d01730b3d764783ec41f)\\[PDF\\]\n\n*   7 Excerpts\n\nSave\n\n[### Language Representations Can be What Recommenders Need: Findings and Potentials](https://www.semanticscholar.org/paper/Language-Representations-Can-be-What-Recommenders-Sheng-Zhang/cb085ce72c1647d23da2514dc45e74fbf88f9224)\n\n[Leheng Sheng](https://www.semanticscholar.org/author/Leheng-Sheng/2258731846)[An Zhang](https://www.semanticscholar.org/author/An-Zhang/2153659066)[Yi Zhang](https://www.semanticscholar.org/author/Yi-Zhang/2305963111)[Yuxin Chen](https://www.semanticscholar.org/author/Yuxin-Chen/2258784735)[Xiang Wang](https://www.semanticscholar.org/author/Xiang-Wang/2257436645)[Tat-Seng Chua](https://www.semanticscholar.org/author/Tat-Seng-Chua/2257036129)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nThese findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance and suggest the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Expand\n\n[](https://www.semanticscholar.org/reader/cb085ce72c1647d23da2514dc45e74fbf88f9224)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### A Survey on Large Language Models for Recommendation](https://www.semanticscholar.org/paper/A-Survey-on-Large-Language-Models-for-Wu-Zheng/b486982fa7c68a8a08df1111ba9607119419c488)\n\n[Likang Wu](https://www.semanticscholar.org/author/Likang-Wu/12892739)[Zhilan Zheng](https://www.semanticscholar.org/author/Zhilan-Zheng/2115548818)+9 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2173129111)\n\nComputer Science\n\n[World Wide Web](https://www.semanticscholar.org/venue?name=World%20Wide%20Web)\n\n*   2024\n\nTLDR\n\nThis survey presents a taxonomy that categorizes existing LLM-based recommendation systems into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLL4Rec (GLLM 4Rec), with the latter being systematically sorted out for the first time.Expand\n\n*   [230](https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488#citing-papers)\n[](https://www.semanticscholar.org/reader/b486982fa7c68a8a08df1111ba9607119419c488)\\[PDF\\]\n\nSave\n\n[### Semantic Convergence: Harmonizing Recommender Systems via Two-Stage Alignment and Behavioral Semantic Tokenization](https://www.semanticscholar.org/paper/Semantic-Convergence%3A-Harmonizing-Recommender-via-Li-Zhang/4116a4fe3f6f8d1b72f7621cfd60b26f0780cba4)\n\n[Guanghan Li](https://www.semanticscholar.org/author/Guanghan-Li/2336084275)[Xun Zhang](https://www.semanticscholar.org/author/Xun-Zhang/2336017879)[Yufei Zhang](https://www.semanticscholar.org/author/Yufei-Zhang/2336012903)[Yifan Yin](https://www.semanticscholar.org/author/Yifan-Yin/2336305855)[Guojun Yin](https://www.semanticscholar.org/author/Guojun-Yin/2335868502)[Wei Lin](https://www.semanticscholar.org/author/Wei-Lin/2336243476)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nA novel framework that harmoniously merges traditional recommendation models with the prowess of LLMs is proposed, and extensive experimental evidence indicates that this model markedly improves recall metrics and displays remarkable scalability of recommendation systems.Expand\n\n[](https://www.semanticscholar.org/reader/4116a4fe3f6f8d1b72f7621cfd60b26f0780cba4)\\[PDF\\]\n\nSave\n\n[### Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://www.semanticscholar.org/paper/Molar%3A-Multimodal-LLMs-with-Collaborative-Filtering-Luo-Qin/3cc1f63eab45d9f1b610176aac2ff8fb39d8c27e)\n\n[Yucong Luo](https://www.semanticscholar.org/author/Yucong-Luo/2208917508)[Qitao Qin](https://www.semanticscholar.org/author/Qitao-Qin/2320834305)+4 authors [Ouyang Jie](https://www.semanticscholar.org/author/Ouyang-Jie/2322501286)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nMolar is proposed, a Multimodal large language sequential recommendation framework that integrates multiple content modalities with ID information to capture collaborative signals effectively and captures both user interests and contextual semantics, leading to superior recommendation accuracy.Expand\n\n[](https://www.semanticscholar.org/reader/3cc1f63eab45d9f1b610176aac2ff8fb39d8c27e)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation](https://www.semanticscholar.org/paper/CoST%3A-Contrastive-Quantization-based-Semantic-for-Zhu-Jin/6d9c4f406caff94da5846a9be6749ffe3505bda2)\n\n[Jieming Zhu](https://www.semanticscholar.org/author/Jieming-Zhu/2290237904)[Mengqun Jin](https://www.semanticscholar.org/author/Mengqun-Jin/2298723722)[Qijiong Liu](https://www.semanticscholar.org/author/Qijiong-Liu/150270469)[Zexuan Qiu](https://www.semanticscholar.org/author/Zexuan-Qiu/2298269415)[Zhenhua Dong](https://www.semanticscholar.org/author/Zhenhua-Dong/2274021958)[Xiu Li](https://www.semanticscholar.org/author/Xiu-Li/2297866189)\n\nComputer Science\n\n[RecSys](https://www.semanticscholar.org/venue?name=RecSys)\n\n*   2024\n\nTLDR\n\nA contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens and achieves significant improvement in Recall@5 and NDCG@5 on the MIND dataset over previous baselines.Expand\n\n*   [2](https://www.semanticscholar.org/paper/6d9c4f406caff94da5846a9be6749ffe3505bda2#citing-papers)\n[](https://www.semanticscholar.org/reader/6d9c4f406caff94da5846a9be6749ffe3505bda2)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### RUIE: Retrieval-based Unified Information Extraction using Large Language Model](https://www.semanticscholar.org/paper/RUIE%3A-Retrieval-based-Unified-Information-using-Liao-Duan/091062b1e3be7d3aa291abd9314404482dd9b3a3)\n\n[Xincheng Liao](https://www.semanticscholar.org/author/Xincheng-Liao/2291001583)[Junwen Duan](https://www.semanticscholar.org/author/Junwen-Duan/2259993319)[Yixi Huang](https://www.semanticscholar.org/author/Yixi-Huang/2321656257)[Jianxin Wang](https://www.semanticscholar.org/author/Jianxin-Wang/2199815721)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n*   2024\n\nTLDR\n\nRUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning to enable rapid generalization while reducing computational costs, is proposed and is confirmed as the first trainable retrieval framework for UIE.Expand\n\n[](https://www.semanticscholar.org/reader/091062b1e3be7d3aa291abd9314404482dd9b3a3)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Large Language Models for Generative Information Extraction: A Survey](https://www.semanticscholar.org/paper/Large-Language-Models-for-Generative-Information-A-Xu-Chen/351a2d50b4aff8e9754dc7074dd589b10a7465d4)\n\n[Derong Xu](https://www.semanticscholar.org/author/Derong-Xu/2262514619)[Wei Chen](https://www.semanticscholar.org/author/Wei-Chen/2260766940)+6 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)\n\nComputer Science, Linguistics\n\nFrontiers Comput. Sci.\n\n*   2024\n\nTLDR\n\nThis study surveys the most recent advancements in generative Large Language Models efforts for IE tasks and identifies several insights in technique and promising research directions that deserve further exploration in future studies.Expand\n\n*   [87](https://www.semanticscholar.org/paper/351a2d50b4aff8e9754dc7074dd589b10a7465d4#citing-papers)\n[](https://www.semanticscholar.org/reader/351a2d50b4aff8e9754dc7074dd589b10a7465d4)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Large Language Model Enhanced Recommender Systems: Taxonomy, Trend, Application and Future](https://www.semanticscholar.org/paper/Large-Language-Model-Enhanced-Recommender-Systems%3A-Liu-Zhao/c6dac310a7df61a238f626d89da42baa851d6372)\n\n[Qidong Liu](https://www.semanticscholar.org/author/Qidong-Liu/2240559309)[Xiangyu Zhao](https://www.semanticscholar.org/author/Xiangyu-Zhao/2238104000)+9 authors [Feng Tian](https://www.semanticscholar.org/author/Feng-Tian/2244621655)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nA critical shift in the field is identified with the move towards incorporating LLM into the online system, notably by avoiding their use during inference, and several promising research directions are highlighted that could further advance the field of LLMERS.Expand\n\n*   [Highly Influenced](https://www.semanticscholar.org/paper/c6dac310a7df61a238f626d89da42baa851d6372?sort=is-influential#citing-papers)\n    \n*   [PDF](https://www.semanticscholar.org/paper/c6dac310a7df61a238f626d89da42baa851d6372)\n    \n\n*   3 Excerpts\n\nSave\n\n[### ScalingNote: Scaling up Retrievers with Large Language Models for Real-World Dense Retrieval](https://www.semanticscholar.org/paper/ScalingNote%3A-Scaling-up-Retrievers-with-Large-for-Huang-Zhang/3be8efd48e056e63a933d5a42b844d5713a6d3ff)\n\n[Suyuan Huang](https://www.semanticscholar.org/author/Suyuan-Huang/2166592873)[Chao Zhang](https://www.semanticscholar.org/author/Chao-Zhang/2260850374)+12 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nThis work proposes ScalingNote, a two-stage method to exploit the scaling potential of LLMs for retrieval while maintaining online query latency, and verifies the scaling law of dense retrieval with LLMs in industrial scenarios, enabling cost-effective scaling of dense retrieval systems.Expand\n\n[](https://www.semanticscholar.org/reader/3be8efd48e056e63a933d5a42b844d5713a6d3ff)\\[PDF\\]\n\nSave\n\n...\n\n1\n\n2\n\n...\n\n61 References\n-------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems](https://www.semanticscholar.org/paper/A-Bi-Step-Grounding-Paradigm-for-Large-Language-in-Bao-Zhang/aae1d88c70cf18ef6aa23693a4dce8204e22d087)\n\n[Keqin Bao](https://www.semanticscholar.org/author/Keqin-Bao/2188063534)[Jizhi Zhang](https://www.semanticscholar.org/author/Jizhi-Zhang/2116265843)+6 authors [Qi Tian](https://www.semanticscholar.org/author/Qi-Tian/2056267912)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n*   2023\n\nTLDR\n\nThis paper investigates the comprehensive ranking capacity of LLMs and proposes a two-step grounding framework known as BIGRec (Bi-step Grounding Paradigm for Recommendation), which initially grounds LLMs to the recommendation space by fine-tuning them to generate meaningful tokens for items and subsequently identifies appropriate actual items that correspond to the generated tokens.Expand\n\n*   [51](https://www.semanticscholar.org/paper/aae1d88c70cf18ef6aa23693a4dce8204e22d087#citing-papers)\n[](https://www.semanticscholar.org/reader/aae1d88c70cf18ef6aa23693a4dce8204e22d087)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation](https://www.semanticscholar.org/paper/TALLRec%3A-An-Effective-and-Efficient-Tuning-to-Align-Bao-Zhang/3487c12512fa41d3a4d64f00cb842525a8590ad3)\n\n[Keqin Bao](https://www.semanticscholar.org/author/Keqin-Bao/2188063534)[Jizhi Zhang](https://www.semanticscholar.org/author/Jizhi-Zhang/2116265843)[Yang Zhang](https://www.semanticscholar.org/author/Yang-Zhang/2145957648)[Wenjie Wang](https://www.semanticscholar.org/author/Wenjie-Wang/2117833732)[Fuli Feng](https://www.semanticscholar.org/author/Fuli-Feng/2163400298)[Xiangnan He](https://www.semanticscholar.org/author/Xiangnan-He/7792071)\n\nComputer Science\n\n[RecSys](https://www.semanticscholar.org/venue?name=RecSys)\n\n*   2023\n\nTLDR\n\nIt is demonstrated that the proposed TALLRec framework can significantly enhance the recommendation capabilities of LLMs in the movie and book domains, even with a limited dataset of fewer than 100 samples.Expand\n\n*   [232](https://www.semanticscholar.org/paper/3487c12512fa41d3a4d64f00cb842525a8590ad3#citing-papers)\n[](https://www.semanticscholar.org/reader/3487c12512fa41d3a4d64f00cb842525a8590ad3)\\[PDF\\]\n\nSave\n\n[### Large Language Models are Zero-Shot Rankers for Recommender Systems](https://www.semanticscholar.org/paper/Large-Language-Models-are-Zero-Shot-Rankers-for-Hou-Zhang/f4e723958a93762befb4d4a039b44a7d752f9917)\n\n[Yupeng Hou](https://www.semanticscholar.org/author/Yupeng-Hou/151472453)[Junjie Zhang](https://www.semanticscholar.org/author/Junjie-Zhang/2120518257)+4 authors [Wayne Xin Zhao](https://www.semanticscholar.org/author/Wayne-Xin-Zhao/2542603)\n\nComputer Science\n\n[ECIR](https://www.semanticscholar.org/venue?name=ECIR)\n\n*   2024\n\nTLDR\n\nThis work investigates the capacity of LLMs that act as the ranking model for recommender systems, and shows that LLMs have promising zero-shot ranking abilities but struggle to perceive the order of historical interactions, and can be biased by popularity or item positions in the prompts.Expand\n\n*   [230](https://www.semanticscholar.org/paper/f4e723958a93762befb4d4a039b44a7d752f9917#citing-papers)\n[](https://www.semanticscholar.org/reader/f4e723958a93762befb4d4a039b44a7d752f9917)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Training Large-Scale News Recommenders with Pretrained Language Models in the Loop](https://www.semanticscholar.org/paper/Training-Large-Scale-News-Recommenders-with-Models-Xiao-Liu/62842f5a587485e59068e216425b10362b1113e1)\n\n[Shitao Xiao](https://www.semanticscholar.org/author/Shitao-Xiao/2051175765)[Zheng Liu](https://www.semanticscholar.org/author/Zheng-Liu/2145976175)+4 authors [Xing Xie](https://www.semanticscholar.org/author/Xing-Xie/2110972323)\n\nComputer Science\n\n[KDD](https://www.semanticscholar.org/venue?name=KDD)\n\n*   2022\n\nTLDR\n\nA novel framework, SpeedyFeed, which efficiently trains PLMs-based news recommenders of superior quality and is applied to Microsoft News to empower the training of large-scale production models, which demonstrate highly competitive online performances.Expand\n\n*   [36](https://www.semanticscholar.org/paper/62842f5a587485e59068e216425b10362b1113e1#citing-papers)\n[](https://www.semanticscholar.org/reader/62842f5a587485e59068e216425b10362b1113e1)\\[PDF\\]\n\nSave\n\n[### A Survey on Large Language Models for Recommendation](https://www.semanticscholar.org/paper/A-Survey-on-Large-Language-Models-for-Wu-Zheng/b486982fa7c68a8a08df1111ba9607119419c488)\n\n[Likang Wu](https://www.semanticscholar.org/author/Likang-Wu/12892739)[Zhilan Zheng](https://www.semanticscholar.org/author/Zhilan-Zheng/2115548818)+9 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2173129111)\n\nComputer Science\n\n[World Wide Web](https://www.semanticscholar.org/venue?name=World%20Wide%20Web)\n\n*   2024\n\nTLDR\n\nThis survey presents a taxonomy that categorizes existing LLM-based recommendation systems into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLL4Rec (GLLM 4Rec), with the latter being systematically sorted out for the first time.Expand\n\n*   [230](https://www.semanticscholar.org/paper/b486982fa7c68a8a08df1111ba9607119419c488#citing-papers)\n[](https://www.semanticscholar.org/reader/b486982fa7c68a8a08df1111ba9607119419c488)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### Microblog Hashtag Generation via Encoding Conversation Contexts](https://www.semanticscholar.org/paper/Microblog-Hashtag-Generation-via-Encoding-Contexts-Wang-Li/99526e14a0954217f5261bed6a56c548f4e1ef97)\n\n[Yue Wang](https://www.semanticscholar.org/author/Yue-Wang/49416727)[Jing Li](https://www.semanticscholar.org/author/Jing-Li/49297986)[Irwin King](https://www.semanticscholar.org/author/Irwin-King/145310663)[M. Lyu](https://www.semanticscholar.org/author/M.-Lyu/145609003)[Shuming Shi](https://www.semanticscholar.org/author/Shuming-Shi/34720053)\n\nComputer Science\n\n[NAACL](https://www.semanticscholar.org/venue?name=NAACL)\n\n*   2019\n\nTLDR\n\nThis work is the first effort to annotate hashtags with a novel sequence generation framework via viewing the hashtag as a short sequence of words, and proposes to jointly model the target posts and the conversation contexts initiated by them with bidirectional attention.Expand\n\n*   [27](https://www.semanticscholar.org/paper/99526e14a0954217f5261bed6a56c548f4e1ef97#citing-papers)\n[](https://www.semanticscholar.org/reader/99526e14a0954217f5261bed6a56c548f4e1ef97)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### One Embedder, Any Task: Instruction-Finetuned Text Embeddings](https://www.semanticscholar.org/paper/One-Embedder%2C-Any-Task%3A-Instruction-Finetuned-Text-Su-Shi/63c0dbe2426f9c92f469151d1773e5265ae6580e)\n\n[Hongjin Su](https://www.semanticscholar.org/author/Hongjin-Su/2152173042)[Weijia Shi](https://www.semanticscholar.org/author/Weijia-Shi/3040379)+7 authors [Tao Yu](https://www.semanticscholar.org/author/Tao-Yu/48881008)\n\nComputer Science\n\n[ACL](https://www.semanticscholar.org/venue?name=ACL)\n\n*   2023\n\nTLDR\n\nINSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training, and achieves state-of-the-art performance.Expand\n\n*   [232](https://www.semanticscholar.org/paper/63c0dbe2426f9c92f469151d1773e5265ae6580e#citing-papers)\n[](https://www.semanticscholar.org/reader/63c0dbe2426f9c92f469151d1773e5265ae6580e)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### A First Look at LLM-Powered Generative News Recommendation](https://www.semanticscholar.org/paper/A-First-Look-at-LLM-Powered-Generative-News-Liu-Chen/0f4d00d01d43d3967ee92b58481b5ad530a944d1)\n\n[Qijiong Liu](https://www.semanticscholar.org/author/Qijiong-Liu/150270469)[Nuo Chen](https://www.semanticscholar.org/author/Nuo-Chen/2257286538)[Tetsuya Sakai](https://www.semanticscholar.org/author/Tetsuya-Sakai/2257233277)[Xiao-Ming Wu](https://www.semanticscholar.org/author/Xiao-Ming-Wu/2187512110)\n\nComputer Science\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n*   2023\n\nTLDR\n\nGENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data is introduced, to provide a flexible and unified solution for news recommendation.Expand\n\n*   [40](https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/0f4d00d01d43d3967ee92b58481b5ad530a944d1)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Is ChatGPT a Good Recommender? A Preliminary Study](https://www.semanticscholar.org/paper/Is-ChatGPT-a-Good-Recommender-A-Preliminary-Study-Liu-Liu/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3)\n\n[Junling Liu](https://www.semanticscholar.org/author/Junling-Liu/2109104637)[Chaoyong Liu](https://www.semanticscholar.org/author/Chaoyong-Liu/50557623)[Renjie Lv](https://www.semanticscholar.org/author/Renjie-Lv/2214809142)[Kangdi Zhou](https://www.semanticscholar.org/author/Kangdi-Zhou/96746184)[Y. Zhang](https://www.semanticscholar.org/author/Y.-Zhang/2152822535)\n\nComputer Science, Linguistics\n\n[ArXiv](https://www.semanticscholar.org/venue?name=ArXiv)\n\n*   2023\n\nTLDR\n\nThis paper employs ChatGPT as a general-purpose recommendation model to explore its potential for transferring extensive linguistic and world knowledge acquired from large-scale corpora to recommendation scenarios and explores the use of few-shot prompting to inject interaction information that contains user potential interest to helpChatGPT better understand user needs and interests.Expand\n\n*   [207](https://www.semanticscholar.org/paper/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3#citing-papers)\n[](https://www.semanticscholar.org/reader/ca7bd64d372e3bcb3f4633ca4a20291ff57de3c3)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### Large Language Model based Long-tail Query Rewriting in Taobao Search](https://www.semanticscholar.org/paper/Large-Language-Model-based-Long-tail-Query-in-Peng-Li/abf1ddd4f48b2cee7e5c71ee4609f07209189c75)\n\n[Wenjun Peng](https://www.semanticscholar.org/author/Wenjun-Peng/1765218)[Guiyang Li](https://www.semanticscholar.org/author/Guiyang-Li/2187871782)+5 authors [Enhong Chen](https://www.semanticscholar.org/author/Enhong-Chen/2265580543)\n\nComputer Science\n\n[WWW](https://www.semanticscholar.org/venue?name=WWW)\n\n*   2024\n\nTLDR\n\nBEQUE is presented, a comprehensive framework that Bridges the semantic gap for long-tail QUE for e-commerce search and can significantly boost gross merchandise volume (GMV), number of transaction (#Trans) and unique visitor (UV) for long-tail queries.Expand\n\n*   [21](https://www.semanticscholar.org/paper/abf1ddd4f48b2cee7e5c71ee4609f07209189c75#citing-papers)\n[](https://www.semanticscholar.org/reader/abf1ddd4f48b2cee7e5c71ee4609f07209189c75)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\nRelated Papers\n--------------\n\nShowing 1 through 3 of 0 Related Papers\n\n*   [Figures and Tables](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#extracted)\n*   [Topics](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#paper-topics)\n*   [15 Citations](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#citing-papers)\n*   [61 References](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#cited-papers)\n*   [Related Papers](https://www.semanticscholar.org/paper/NoteLLM%3A-A-Retrievable-Large-Language-Model-for-Zhang-Wu/009f64d890d8402cae8739b63bc0c3d06d62ed06#related-papers)\n\nStay Connected With Semantic Scholar\n\nSign Up\n\nWhat Is Semantic Scholar?\n-------------------------\n\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.\n\n[Learn More](https://www.semanticscholar.org/about)\n\n### About\n\n[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)\n\n### Product\n\n[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)\n\n### API\n\n[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)\n\n### Research\n\n[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)\n\n### Help\n\n[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact\n\nProudly built by [Ai2 (opens in a new tab)](http://allenai.org/)\n\nCollaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)\n\n[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)\n\nACCEPT & CONTINUE",
  "usage": {
    "tokens": 10000
  }
}
```
