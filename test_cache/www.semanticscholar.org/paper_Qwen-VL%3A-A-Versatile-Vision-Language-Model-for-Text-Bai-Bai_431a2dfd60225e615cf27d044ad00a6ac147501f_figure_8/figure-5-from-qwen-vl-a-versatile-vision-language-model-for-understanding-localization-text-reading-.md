---
title: Figure 5 from Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | Semantic Scholar
description: Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL - "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
url: https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8
timestamp: 2025-01-20T15:46:41.473Z
domain: www.semanticscholar.org
path: paper_Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai_431a2dfd60225e615cf27d044ad00a6ac147501f_figure_8
---

# Figure 5 from Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | Semantic Scholar


Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL - "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"


## Content

Figure 5 from Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | Semantic Scholar
===============
                                                                

[Skip to search form](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#search-form)[Skip to main content](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#account-menu)

[](https://www.semanticscholar.org/)

Search 223,685,250 papers from all fields of science

Search

Sign InCreate Free Account

*   Corpus ID: 261101015

Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond
====================================================================================================

@inproceedings{Bai2023QwenVLAV,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:261101015}
}

*   [Jinze Bai](https://www.semanticscholar.org/author/Jinze-Bai/41211611), [Shuai Bai](https://www.semanticscholar.org/author/Shuai-Bai/3768186), +6 authors [Jingren Zhou](https://www.semanticscholar.org/author/Jingren-Zhou/1709595)
*   Published 24 August 2023
*   Computer Science, Linguistics

TLDR

The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.Expand

[](https://www.semanticscholar.org/reader/fc6a2f7478f68adefd69e2071f27e38aa1647f2f)\[PDF\] Semantic Reader

Save to LibrarySave

Create AlertAlert

Cite

Share

Figures and Tables from this paper
----------------------------------

*   [![Image 20: figure 1](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/1-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/0)
*   [![Image 21: table 1](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/4-Table1-1.png) table 1](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/1)
*   [![Image 22: figure 2](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/2-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/2)
*   [![Image 23: table 2](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/5-Table2-1.png) table 2](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/3)
*   [![Image 24: table 3](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/6-Table3-1.png) table 3](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/4)
*   [![Image 25: table 4](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/7-Table4-1.png) table 4](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/5)
*   [![Image 26: figure 4](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Figure4-1.png) figure 4](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/6)
*   [![Image 27: table 5](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Table5-1.png) table 5](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/7)
*   [![Image 28: figure 5](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/17-Figure5-1.png) figure 5](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/8)
*   [![Image 29: table 6](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Table6-1.png) table 6](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/9)
*   [![Image 30: figure 6](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/21-Figure6-1.png) figure 6](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/10)
*   [![Image 31: table 7](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/9-Table7-1.png) table 7](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/11)
*   [![Image 32: figure 7](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/22-Figure7-1.png) figure 7](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/12)
*   [![Image 33: table 8](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/20-Table8-1.png) table 8](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/13)
*   [![Image 34: figure 8](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/23-Figure8-1.png) figure 8](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/14)
*   [![Image 35: table 9](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/21-Table9-1.png) table 9](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/15)
*   [![Image 36: table 10](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/22-Table10-1.png) table 10](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/16)
*   [![Image 37: table 11](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/23-Table11-1.png) table 11](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/17)

View All 18 Figures & Tables

Topics
------

AI-Generated

[Qwen-VL-Chat (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/5317388667?corpusId=261101015)[Qwen-VL (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/56854656287?corpusId=261101015)[Text Visual Question Answering (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19105646689?corpusId=261101015)[BuboGPT (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/34049468201?corpusId=261101015)[Kosmos-2 (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/40409326630?corpusId=261101015)[Vision-Language Models (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/7642616386?corpusId=261101015)[Question Answering (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/59651195051?corpusId=261101015)[Visual Grounding (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/54882488326?corpusId=261101015)[Few-shot (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/3675130560?corpusId=261101015)[Zero-shot (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/20795794008?corpusId=261101015)

498 Citations
-------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models](https://www.semanticscholar.org/paper/Constructing-Multilingual-Visual-Text-Datasets-of-Atuhurra-Ali/bf6ae34333441cb37b5d29de59cc5bc067428bc4)

[Jesse Atuhurra](https://www.semanticscholar.org/author/Jesse-Atuhurra/2293720361)[Iqra Ali](https://www.semanticscholar.org/author/Iqra-Ali/2301581430)[Tatsuya Hiraoka](https://www.semanticscholar.org/author/Tatsuya-Hiraoka/2306967077)[Hidetaka Kamigaito](https://www.semanticscholar.org/author/Hidetaka-Kamigaito/2298970864)[Tomoya Iwakura](https://www.semanticscholar.org/author/Tomoya-Iwakura/34758951)[Taro Watanabe](https://www.semanticscholar.org/author/Taro-Watanabe/2266807418)

Computer Science, Linguistics

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

It is shown that VLMs can be fine-tuned on the authors' datasets, and this work is the first to conduct such analyses in Swahili and Urdu, and introduces rationales in VL analysis, which played a vital role in the evaluation.Expand

[](https://www.semanticscholar.org/reader/bf6ae34333441cb37b5d29de59cc5bc067428bc4)\[PDF\]

*   3 Excerpts

Save

[### SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://www.semanticscholar.org/paper/SQ-LLaVA%3A-Self-Questioning-for-Large-Assistant-Sun-Qin/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3)

[Guohao Sun](https://www.semanticscholar.org/author/Guohao-Sun/2292263389)[Can Qin](https://www.semanticscholar.org/author/Can-Qin/2292029737)[Jiamian Wang](https://www.semanticscholar.org/author/Jiamian-Wang/46585138)[Zeyuan Chen](https://www.semanticscholar.org/author/Zeyuan-Chen/2292059557)[Ran Xu](https://www.semanticscholar.org/author/Ran-Xu/2292215275)[Zhiqiang Tao](https://www.semanticscholar.org/author/Zhiqiang-Tao/2162198915)

Computer Science

[European Conference on Computer Vision](https://www.semanticscholar.org/venue?name=European%20Conference%20on%20Computer%20Vision)

*   2024

TLDR

This paper first attempts to harness the overlooked context within visual instruction data, training the model to self-supervised"learning"how to ask high-quality questions, and introduces a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant.Expand

*   [4](https://www.semanticscholar.org/paper/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3)\[PDF\]

*   5 Excerpts

Save

[### InfMLLM: A Unified Framework for Visual-Language Tasks](https://www.semanticscholar.org/paper/InfMLLM%3A-A-Unified-Framework-for-Visual-Language-Zhou-Wang/e066347a8896058f50d0259b91b6ca3c40f52c2d)

[Qiang Zhou](https://www.semanticscholar.org/author/Qiang-Zhou/2266512639)[Zhibin Wang](https://www.semanticscholar.org/author/Zhibin-Wang/2051262469)[Wei Chu](https://www.semanticscholar.org/author/Wei-Chu/2266389707)[Yinghui Xu](https://www.semanticscholar.org/author/Yinghui-Xu/2266466742)[Hao Li](https://www.semanticscholar.org/author/Hao-Li/2266389842)[Yuan Qi](https://www.semanticscholar.org/author/Yuan-Qi/2263501050)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2023

TLDR

This work delves into enabling LLMs to tackle more vision-language-related tasks, particularly image captioning, visual question answering, and visual grounding, by implementing a three-stage training scheme and introducing a straightforward visual adapter module dubbed pool-adapter.Expand

*   [11](https://www.semanticscholar.org/paper/e066347a8896058f50d0259b91b6ca3c40f52c2d#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/e066347a8896058f50d0259b91b6ca3c40f52c2d?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/e066347a8896058f50d0259b91b6ca3c40f52c2d)\[PDF\]

*   4 Excerpts

Save

[### Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types](https://www.semanticscholar.org/paper/Guiding-Vision-Language-Model-Selection-for-Visual-Sinha-Jain/2ec05bd2e12d561494e220009eebb16edaf1975d)

[Neelabh Sinha](https://www.semanticscholar.org/author/Neelabh-Sinha/2306957961)[Vinija Jain](https://www.semanticscholar.org/author/Vinija-Jain/2212131028)[Aman Chadha](https://www.semanticscholar.org/author/Aman-Chadha/2275226689)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

This paper presents VQA360 - a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, for a comprehensive evaluation and introduces GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments.Expand

*   [1](https://www.semanticscholar.org/paper/2ec05bd2e12d561494e220009eebb16edaf1975d#citing-papers)
[](https://www.semanticscholar.org/reader/2ec05bd2e12d561494e220009eebb16edaf1975d)\[PDF\]

*   2 Excerpts

Save

[### 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://www.semanticscholar.org/paper/2.5-Years-in-Class%3A-A-Multimodal-Textbook-for-Zhang-Zhang/d8be52422a9179442fd26b7e9a44b50042415775)

[Wenqi Zhang](https://www.semanticscholar.org/author/Wenqi-Zhang/2135282890)[Hang Zhang](https://www.semanticscholar.org/author/Hang-Zhang/2268645400)+6 authors [Li Bing](https://www.semanticscholar.org/author/Li-Bing/2211459675)

Computer Science, Education

*   2025

TLDR

This paper introduces a high-quality \\textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining, and demonstrates its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista.Expand

*   [PDF](https://www.semanticscholar.org/paper/d8be52422a9179442fd26b7e9a44b50042415775)
    

*   1 Excerpt

Save

[### Teaching VLMs to Localize Specific Objects from In-context Examples](https://www.semanticscholar.org/paper/Teaching-VLMs-to-Localize-Specific-Objects-from-Doveh-Shabtay/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9)

[Sivan Doveh](https://www.semanticscholar.org/author/Sivan-Doveh/2292197899)[Nimrod Shabtay](https://www.semanticscholar.org/author/Nimrod-Shabtay/2192517617)+9 authors [M. J. Mirza](https://www.semanticscholar.org/author/M.-J.-Mirza/2112655160)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

This work is the first to explore and benchmark personalized few-shot localization for VLMs, laying a foundation for future research in context-driven vision-language applications.Expand

*   [Highly Influenced](https://www.semanticscholar.org/paper/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9)\[PDF\]

*   4 Excerpts

Save

[### VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models](https://www.semanticscholar.org/paper/VEGA%3A-Learning-Interleaved-Image-Text-Comprehension-Zhou-Zhang/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7)

[Chenyu Zhou](https://www.semanticscholar.org/author/Chenyu-Zhou/2304367771)[Mengdan Zhang](https://www.semanticscholar.org/author/Mengdan-Zhang/2269716764)+5 authors [Rongrong Ji](https://www.semanticscholar.org/author/Rongrong-Ji/2258711789)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

By employing a multi-task, multi-scale post-training strategy, this work sets a robust baseline for MLLMs on the IITC task, attaining an $85.8% accuracy rate in image association and a $0.508$ Rouge score, which validate the effectiveness of the dataset in improving MLLMs capabilities for nuanced image-text comprehension.Expand

*   [2](https://www.semanticscholar.org/paper/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7)\[PDF\]

*   7 Excerpts

Save

[### Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models](https://www.semanticscholar.org/paper/Behind-the-Magic%2C-MERLIM%3A-Multi-modal-Evaluation-Villa-Alc'azar/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d)

[Andrés Villa](https://www.semanticscholar.org/author/Andr%C3%A9s-Villa/2064406147)[Juan Carlos Le'on Alc'azar](https://www.semanticscholar.org/author/Juan-Carlos-Le'on-Alc'azar/2269733505)[Alvaro Soto](https://www.semanticscholar.org/author/Alvaro-Soto/2269735606)[Bernard Ghanem](https://www.semanticscholar.org/author/Bernard-Ghanem/2269732636)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2023

TLDR

A Multi-modal Evaluation Benchmark named MERLIM is introduced, a scalable test-bed to assess the capabilities of IT-LVLMs on fundamental computer vision tasks and suggests that these models have weak visual grounding, but manage to make adequate guesses from global visual patterns or language biases contained in the LLM component.Expand

*   [7](https://www.semanticscholar.org/paper/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d#citing-papers)
[](https://www.semanticscholar.org/reader/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d)\[PDF\]

*   3 Excerpts

Save

[### Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://www.semanticscholar.org/paper/Exploring-the-Frontier-of-Vision-Language-Models%3A-A-Ghosh-Acharya/5506409ad47e709ed1a1e4ab0319f4f931b7e379)

[Akash Ghosh](https://www.semanticscholar.org/author/Akash-Ghosh/2275156796)[Arkadeep Acharya](https://www.semanticscholar.org/author/Arkadeep-Acharya/2273560136)[Sriparna Saha](https://www.semanticscholar.org/author/Sriparna-Saha/2244669276)[Vinija Jain](https://www.semanticscholar.org/author/Vinija-Jain/2212131028)[Aman Chadha](https://www.semanticscholar.org/author/Aman-Chadha/2275226689)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

This classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.Expand

*   [9](https://www.semanticscholar.org/paper/5506409ad47e709ed1a1e4ab0319f4f931b7e379#citing-papers)
[](https://www.semanticscholar.org/reader/5506409ad47e709ed1a1e4ab0319f4f931b7e379)\[PDF\]

*   2 Excerpts

Save

[### Visual Large Language Models for Generalized and Specialized Applications](https://www.semanticscholar.org/paper/Visual-Large-Language-Models-for-Generalized-and-Li-Lai/cca431c69807196211c657a35697644743944b12)

[Yifan Li](https://www.semanticscholar.org/author/Yifan-Li/2267393864)[Zhixin Lai](https://www.semanticscholar.org/author/Zhixin-Lai/2334744441)+7 authors [Yu Kong](https://www.semanticscholar.org/author/Yu-Kong/2267333754)

Computer Science, Engineering

*   2025

TLDR

This survey focuses on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development, to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs.Expand

[](https://www.semanticscholar.org/reader/cca431c69807196211c657a35697644743944b12)\[PDF\]

*   3 Excerpts

Save

...

1

2

3

4

5

...

86 References
-------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://www.semanticscholar.org/paper/BLIP%3A-Bootstrapping-Language-Image-Pre-training-for-Li-Li/a3b42a83669998f65df60d7c065a70d07ca95e99)

[Junnan Li](https://www.semanticscholar.org/author/Junnan-Li/49299019)[Dongxu Li](https://www.semanticscholar.org/author/Dongxu-Li/2981509)[Caiming Xiong](https://www.semanticscholar.org/author/Caiming-Xiong/2054594326)[S. Hoi](https://www.semanticscholar.org/author/S.-Hoi/1741126)

Computer Science

[International Conference on Machine Learning](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Machine%20Learning)

*   2022

TLDR

BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones, and demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner.Expand

*   [3,290](https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99#citing-papers)
[](https://www.semanticscholar.org/reader/a3b42a83669998f65df60d7c065a70d07ca95e99)\[PDF\]

Save

[### Flamingo: a Visual Language Model for Few-Shot Learning](https://www.semanticscholar.org/paper/Flamingo%3A-a-Visual-Language-Model-for-Few-Shot-Alayrac-Donahue/26218bdcc3945c7edae7aa2adbfba4cd820a2df3)

[Jean-Baptiste Alayrac](https://www.semanticscholar.org/author/Jean-Baptiste-Alayrac/2285263)[Jeff Donahue](https://www.semanticscholar.org/author/Jeff-Donahue/7408951)+24 authors [K. Simonyan](https://www.semanticscholar.org/author/K.-Simonyan/34838386)

Computer Science

[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)

*   2022

TLDR

This work introduces Flamingo, a family of Visual Language Models (VLM) with this ability to bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs.Expand

*   [2,690](https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3#citing-papers)
*   [Highly Influential](https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/26218bdcc3945c7edae7aa2adbfba4cd820a2df3)\[PDF\]

*   9 Excerpts

Save

[### VinVL: Revisiting Visual Representations in Vision-Language Models](https://www.semanticscholar.org/paper/VinVL%3A-Revisiting-Visual-Representations-in-Models-Zhang-Li/63c74d15940af1af9b386b5762e4445e54c73719)

[Pengchuan Zhang](https://www.semanticscholar.org/author/Pengchuan-Zhang/9325940)[Xiujun Li](https://www.semanticscholar.org/author/Xiujun-Li/47058148)+5 authors [Jianfeng Gao](https://www.semanticscholar.org/author/Jianfeng-Gao/48441311)

Computer Science

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2021

TLDR

This paper develops an improved object detection model to provide object-centric representations of images and feeds the visual features generated into a Transformer-based VL fusion model OSCAR, and utilizes an improved approach OSCar+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks.Expand

*   [829](https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719)
    

Save

[### VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://www.semanticscholar.org/paper/VL-BERT%3A-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu/4aa6298b606941a282d735fa3143da293199d2ca)

[Weijie Su](https://www.semanticscholar.org/author/Weijie-Su/145499378)[Xizhou Zhu](https://www.semanticscholar.org/author/Xizhou-Zhu/2578924)+4 authors [Jifeng Dai](https://www.semanticscholar.org/author/Jifeng-Dai/3304536)

Computer Science, Linguistics

[International Conference on Learning…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Learning%20Representations)

*   2020

TLDR

A new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT), which adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input.Expand

*   [1,582](https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca)
    

*   1 Excerpt

Save

[### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://www.semanticscholar.org/paper/Conceptual-12M%3A-Pushing-Web-Scale-Image-Text-To-Changpinyo-Sharma/394be105b87e9bfe72c20efe6338de10604e1a11)

[Soravit Changpinyo](https://www.semanticscholar.org/author/Soravit-Changpinyo/2059199)[P. Sharma](https://www.semanticscholar.org/author/P.-Sharma/46756891)[Nan Ding](https://www.semanticscholar.org/author/Nan-Ding/2066767241)[Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)

Computer Science

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2021

TLDR

The results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.Expand

*   [939](https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11#citing-papers)
[](https://www.semanticscholar.org/reader/394be105b87e9bfe72c20efe6338de10604e1a11)\[PDF\]

*   2 Excerpts

Save

[### Language Is Not All You Need: Aligning Perception with Language Models](https://www.semanticscholar.org/paper/Language-Is-Not-All-You-Need%3A-Aligning-Perception-Huang-Dong/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c)

[Shaohan Huang](https://www.semanticscholar.org/author/Shaohan-Huang/3110003)[Li Dong](https://www.semanticscholar.org/author/Li-Dong/145307652)+14 authors [Furu Wei](https://www.semanticscholar.org/author/Furu-Wei/49807919)

Computer Science, Linguistics

[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)

*   2023

TLDR

This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.Expand

*   [467](https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c#citing-papers)
[](https://www.semanticscholar.org/reader/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c)\[PDF\]

*   2 Excerpts

Save

[### Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://www.semanticscholar.org/paper/Image-as-a-Foreign-Language%3A-BEiT-Pretraining-for-Wang-Bao/02251886950770e82b3d68564d60cdfe15e73199)

[Wenhui Wang](https://www.semanticscholar.org/author/Wenhui-Wang/51456429)[Hangbo Bao](https://www.semanticscholar.org/author/Hangbo-Bao/10699417)+8 authors [Furu Wei](https://www.semanticscholar.org/author/Furu-Wei/49807919)

Computer Science, Linguistics

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2022

TLDR

This work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding.Expand

*   [583](https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199#citing-papers)
[](https://www.semanticscholar.org/reader/02251886950770e82b3d68564d60cdfe15e73199)\[PDF\]

*   1 Excerpt

Save

[### PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://www.semanticscholar.org/paper/PaLI%3A-A-Jointly-Scaled-Multilingual-Language-Image-Chen-Wang/28630034bb29760df01ab033b743e30b37f336ae)

[Xi Chen](https://www.semanticscholar.org/author/Xi-Chen/2145309103)[Xiao Wang](https://www.semanticscholar.org/author/Xiao-Wang/144129720)+26 authors [Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)

Computer Science, Linguistics

[International Conference on Learning…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Learning%20Representations)

*   2023

TLDR

The PaLI (Pathways Language and Image model), a model that achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.Expand

*   [599](https://www.semanticscholar.org/paper/28630034bb29760df01ab033b743e30b37f336ae#citing-papers)
[](https://www.semanticscholar.org/reader/28630034bb29760df01ab033b743e30b37f336ae)\[PDF\]

*   3 Excerpts

Save

[### PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://www.semanticscholar.org/paper/PaLI-X%3A-On-Scaling-up-a-Multilingual-Vision-and-Chen-Djolonga/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a)

[Xi Chen](https://www.semanticscholar.org/author/Xi-Chen/2145309103)[Josip Djolonga](https://www.semanticscholar.org/author/Josip-Djolonga/2941141)+40 authors [Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)

Computer Science, Linguistics

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2023

TLDR

PaLI-X, a multilingual vision and language model, advances the state-of-the-art on most vision-and-language benchmarks considered and observes emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.Expand

*   [165](https://www.semanticscholar.org/paper/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a#citing-papers)
[](https://www.semanticscholar.org/reader/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a)\[PDF\]

Save

[### ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1)

[Wonjae Kim](https://www.semanticscholar.org/author/Wonjae-Kim/2382193)[Bokyung Son](https://www.semanticscholar.org/author/Bokyung-Son/65842988)[Ildoo Kim](https://www.semanticscholar.org/author/Ildoo-Kim/14972026)

Computer Science

[International Conference on Machine Learning](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Machine%20Learning)

*   2021

TLDR

A minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that the authors process textual inputs, showing that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance.Expand

*   [1,513](https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1#citing-papers)
[](https://www.semanticscholar.org/reader/0839722fb5369c0abaff8515bfc08299efc790a1)\[PDF\]

*   1 Excerpt

Save

...

1

2

3

4

5

...

Related Papers
--------------

Showing 1 through 3 of 0 Related Papers

*   [Figures and Tables](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#extracted)
*   [Topics](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#paper-topics)
*   [498 Citations](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#citing-papers)
*   [86 References](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#cited-papers)
*   [Related Papers](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#related-papers)

[](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/7)

![Image 38](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/17-Figure5-1.png)

Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL 

Published in 2023

[Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f)[Jinze Bai](https://www.semanticscholar.org/author/Jinze-Bai/41211611)[Shuai Bai](https://www.semanticscholar.org/author/Shuai-Bai/3768186)+6 authors [Jingren Zhou](https://www.semanticscholar.org/author/Jingren-Zhou/1709595)

[](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/9)Figure 9 of 18

Stay Connected With Semantic Scholar

Sign Up

What Is Semantic Scholar?
-------------------------

Semantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.

[Learn More](https://www.semanticscholar.org/about)

### About

[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)

### Product

[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)

### API

[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)

### Research

[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)

### Help

[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact

Proudly built by [Ai2 (opens in a new tab)](http://allenai.org/)

Collaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)

[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)

By clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)

ACCEPT & CONTINUE

## Metadata

```json
{
  "title": "Figure 5 from Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | Semantic Scholar",
  "description": "Figure 5: Visualization of the Grounding and OCR data used for training Qwen-VL - \"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\"",
  "url": "https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8",
  "content": "Figure 5 from Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond | Semantic Scholar\n===============\n                                                                \n\n[Skip to search form](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#search-form)[Skip to main content](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#account-menu)\n\n[](https://www.semanticscholar.org/)\n\nSearch 223,685,250 papers from all fields of science\n\nSearch\n\nSign InCreate Free Account\n\n*   Corpus ID: 261101015\n\nQwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\n====================================================================================================\n\n@inproceedings{Bai2023QwenVLAV,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},\n  year={2023},\n  url={https://api.semanticscholar.org/CorpusID:261101015}\n}\n\n*   [Jinze Bai](https://www.semanticscholar.org/author/Jinze-Bai/41211611), [Shuai Bai](https://www.semanticscholar.org/author/Shuai-Bai/3768186), +6 authors [Jingren Zhou](https://www.semanticscholar.org/author/Jingren-Zhou/1709595)\n*   Published 24 August 2023\n*   Computer Science, Linguistics\n\nTLDR\n\nThe Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.Expand\n\n[](https://www.semanticscholar.org/reader/fc6a2f7478f68adefd69e2071f27e38aa1647f2f)\\[PDF\\] Semantic Reader\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\nFigures and Tables from this paper\n----------------------------------\n\n*   [![Image 20: figure 1](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/1-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/0)\n*   [![Image 21: table 1](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/4-Table1-1.png) table 1](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/1)\n*   [![Image 22: figure 2](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/2-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/2)\n*   [![Image 23: table 2](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/5-Table2-1.png) table 2](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/3)\n*   [![Image 24: table 3](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/6-Table3-1.png) table 3](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/4)\n*   [![Image 25: table 4](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/7-Table4-1.png) table 4](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/5)\n*   [![Image 26: figure 4](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Figure4-1.png) figure 4](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/6)\n*   [![Image 27: table 5](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Table5-1.png) table 5](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/7)\n*   [![Image 28: figure 5](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/17-Figure5-1.png) figure 5](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/8)\n*   [![Image 29: table 6](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/8-Table6-1.png) table 6](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/9)\n*   [![Image 30: figure 6](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/21-Figure6-1.png) figure 6](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/10)\n*   [![Image 31: table 7](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/9-Table7-1.png) table 7](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/11)\n*   [![Image 32: figure 7](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/22-Figure7-1.png) figure 7](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/12)\n*   [![Image 33: table 8](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/20-Table8-1.png) table 8](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/13)\n*   [![Image 34: figure 8](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/23-Figure8-1.png) figure 8](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/14)\n*   [![Image 35: table 9](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/21-Table9-1.png) table 9](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/15)\n*   [![Image 36: table 10](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/22-Table10-1.png) table 10](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/16)\n*   [![Image 37: table 11](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/23-Table11-1.png) table 11](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/17)\n\nView All 18 Figures & Tables\n\nTopics\n------\n\nAI-Generated\n\n[Qwen-VL-Chat (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/5317388667?corpusId=261101015)[Qwen-VL (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/56854656287?corpusId=261101015)[Text Visual Question Answering (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/19105646689?corpusId=261101015)[BuboGPT (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/34049468201?corpusId=261101015)[Kosmos-2 (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/40409326630?corpusId=261101015)[Vision-Language Models (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/7642616386?corpusId=261101015)[Question Answering (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/59651195051?corpusId=261101015)[Visual Grounding (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/54882488326?corpusId=261101015)[Few-shot (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/3675130560?corpusId=261101015)[Zero-shot (opens in a new tab)](https://topics-beta.apps.semanticscholar.org/topic/20795794008?corpusId=261101015)\n\n498 Citations\n-------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### Constructing Multilingual Visual-Text Datasets Revealing Visual Multilingual Ability of Vision Language Models](https://www.semanticscholar.org/paper/Constructing-Multilingual-Visual-Text-Datasets-of-Atuhurra-Ali/bf6ae34333441cb37b5d29de59cc5bc067428bc4)\n\n[Jesse Atuhurra](https://www.semanticscholar.org/author/Jesse-Atuhurra/2293720361)[Iqra Ali](https://www.semanticscholar.org/author/Iqra-Ali/2301581430)[Tatsuya Hiraoka](https://www.semanticscholar.org/author/Tatsuya-Hiraoka/2306967077)[Hidetaka Kamigaito](https://www.semanticscholar.org/author/Hidetaka-Kamigaito/2298970864)[Tomoya Iwakura](https://www.semanticscholar.org/author/Tomoya-Iwakura/34758951)[Taro Watanabe](https://www.semanticscholar.org/author/Taro-Watanabe/2266807418)\n\nComputer Science, Linguistics\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nIt is shown that VLMs can be fine-tuned on the authors' datasets, and this work is the first to conduct such analyses in Swahili and Urdu, and introduces rationales in VL analysis, which played a vital role in the evaluation.Expand\n\n[](https://www.semanticscholar.org/reader/bf6ae34333441cb37b5d29de59cc5bc067428bc4)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant](https://www.semanticscholar.org/paper/SQ-LLaVA%3A-Self-Questioning-for-Large-Assistant-Sun-Qin/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3)\n\n[Guohao Sun](https://www.semanticscholar.org/author/Guohao-Sun/2292263389)[Can Qin](https://www.semanticscholar.org/author/Can-Qin/2292029737)[Jiamian Wang](https://www.semanticscholar.org/author/Jiamian-Wang/46585138)[Zeyuan Chen](https://www.semanticscholar.org/author/Zeyuan-Chen/2292059557)[Ran Xu](https://www.semanticscholar.org/author/Ran-Xu/2292215275)[Zhiqiang Tao](https://www.semanticscholar.org/author/Zhiqiang-Tao/2162198915)\n\nComputer Science\n\n[European Conference on Computer Vision](https://www.semanticscholar.org/venue?name=European%20Conference%20on%20Computer%20Vision)\n\n*   2024\n\nTLDR\n\nThis paper first attempts to harness the overlooked context within visual instruction data, training the model to self-supervised\"learning\"how to ask high-quality questions, and introduces a novel framework named SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant.Expand\n\n*   [4](https://www.semanticscholar.org/paper/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/0a7aad85e06dd46ce96bf0e0d20979678b5d4cd3)\\[PDF\\]\n\n*   5 Excerpts\n\nSave\n\n[### InfMLLM: A Unified Framework for Visual-Language Tasks](https://www.semanticscholar.org/paper/InfMLLM%3A-A-Unified-Framework-for-Visual-Language-Zhou-Wang/e066347a8896058f50d0259b91b6ca3c40f52c2d)\n\n[Qiang Zhou](https://www.semanticscholar.org/author/Qiang-Zhou/2266512639)[Zhibin Wang](https://www.semanticscholar.org/author/Zhibin-Wang/2051262469)[Wei Chu](https://www.semanticscholar.org/author/Wei-Chu/2266389707)[Yinghui Xu](https://www.semanticscholar.org/author/Yinghui-Xu/2266466742)[Hao Li](https://www.semanticscholar.org/author/Hao-Li/2266389842)[Yuan Qi](https://www.semanticscholar.org/author/Yuan-Qi/2263501050)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2023\n\nTLDR\n\nThis work delves into enabling LLMs to tackle more vision-language-related tasks, particularly image captioning, visual question answering, and visual grounding, by implementing a three-stage training scheme and introducing a straightforward visual adapter module dubbed pool-adapter.Expand\n\n*   [11](https://www.semanticscholar.org/paper/e066347a8896058f50d0259b91b6ca3c40f52c2d#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/e066347a8896058f50d0259b91b6ca3c40f52c2d?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/e066347a8896058f50d0259b91b6ca3c40f52c2d)\\[PDF\\]\n\n*   4 Excerpts\n\nSave\n\n[### Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types](https://www.semanticscholar.org/paper/Guiding-Vision-Language-Model-Selection-for-Visual-Sinha-Jain/2ec05bd2e12d561494e220009eebb16edaf1975d)\n\n[Neelabh Sinha](https://www.semanticscholar.org/author/Neelabh-Sinha/2306957961)[Vinija Jain](https://www.semanticscholar.org/author/Vinija-Jain/2212131028)[Aman Chadha](https://www.semanticscholar.org/author/Aman-Chadha/2275226689)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nThis paper presents VQA360 - a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, for a comprehensive evaluation and introduces GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments.Expand\n\n*   [1](https://www.semanticscholar.org/paper/2ec05bd2e12d561494e220009eebb16edaf1975d#citing-papers)\n[](https://www.semanticscholar.org/reader/2ec05bd2e12d561494e220009eebb16edaf1975d)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### 2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://www.semanticscholar.org/paper/2.5-Years-in-Class%3A-A-Multimodal-Textbook-for-Zhang-Zhang/d8be52422a9179442fd26b7e9a44b50042415775)\n\n[Wenqi Zhang](https://www.semanticscholar.org/author/Wenqi-Zhang/2135282890)[Hang Zhang](https://www.semanticscholar.org/author/Hang-Zhang/2268645400)+6 authors [Li Bing](https://www.semanticscholar.org/author/Li-Bing/2211459675)\n\nComputer Science, Education\n\n*   2025\n\nTLDR\n\nThis paper introduces a high-quality \\\\textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining, and demonstrates its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista.Expand\n\n*   [PDF](https://www.semanticscholar.org/paper/d8be52422a9179442fd26b7e9a44b50042415775)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Teaching VLMs to Localize Specific Objects from In-context Examples](https://www.semanticscholar.org/paper/Teaching-VLMs-to-Localize-Specific-Objects-from-Doveh-Shabtay/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9)\n\n[Sivan Doveh](https://www.semanticscholar.org/author/Sivan-Doveh/2292197899)[Nimrod Shabtay](https://www.semanticscholar.org/author/Nimrod-Shabtay/2192517617)+9 authors [M. J. Mirza](https://www.semanticscholar.org/author/M.-J.-Mirza/2112655160)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nThis work is the first to explore and benchmark personalized few-shot localization for VLMs, laying a foundation for future research in context-driven vision-language applications.Expand\n\n*   [Highly Influenced](https://www.semanticscholar.org/paper/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/11b7fd9bf6b5af0b42c6e5dd1c7be547ae385ad9)\\[PDF\\]\n\n*   4 Excerpts\n\nSave\n\n[### VEGA: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models](https://www.semanticscholar.org/paper/VEGA%3A-Learning-Interleaved-Image-Text-Comprehension-Zhou-Zhang/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7)\n\n[Chenyu Zhou](https://www.semanticscholar.org/author/Chenyu-Zhou/2304367771)[Mengdan Zhang](https://www.semanticscholar.org/author/Mengdan-Zhang/2269716764)+5 authors [Rongrong Ji](https://www.semanticscholar.org/author/Rongrong-Ji/2258711789)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nBy employing a multi-task, multi-scale post-training strategy, this work sets a robust baseline for MLLMs on the IITC task, attaining an $85.8% accuracy rate in image association and a $0.508$ Rouge score, which validate the effectiveness of the dataset in improving MLLMs capabilities for nuanced image-text comprehension.Expand\n\n*   [2](https://www.semanticscholar.org/paper/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/5c6b4d01babbf24c3474e6a10fb7154bd515b3b7)\\[PDF\\]\n\n*   7 Excerpts\n\nSave\n\n[### Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models](https://www.semanticscholar.org/paper/Behind-the-Magic%2C-MERLIM%3A-Multi-modal-Evaluation-Villa-Alc'azar/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d)\n\n[Andrés Villa](https://www.semanticscholar.org/author/Andr%C3%A9s-Villa/2064406147)[Juan Carlos Le'on Alc'azar](https://www.semanticscholar.org/author/Juan-Carlos-Le'on-Alc'azar/2269733505)[Alvaro Soto](https://www.semanticscholar.org/author/Alvaro-Soto/2269735606)[Bernard Ghanem](https://www.semanticscholar.org/author/Bernard-Ghanem/2269732636)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2023\n\nTLDR\n\nA Multi-modal Evaluation Benchmark named MERLIM is introduced, a scalable test-bed to assess the capabilities of IT-LVLMs on fundamental computer vision tasks and suggests that these models have weak visual grounding, but manage to make adequate guesses from global visual patterns or language biases contained in the LLM component.Expand\n\n*   [7](https://www.semanticscholar.org/paper/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d#citing-papers)\n[](https://www.semanticscholar.org/reader/62aa2c64f3f4a1d357f1f1e6f5ce8cb0bbe9bb6d)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions](https://www.semanticscholar.org/paper/Exploring-the-Frontier-of-Vision-Language-Models%3A-A-Ghosh-Acharya/5506409ad47e709ed1a1e4ab0319f4f931b7e379)\n\n[Akash Ghosh](https://www.semanticscholar.org/author/Akash-Ghosh/2275156796)[Arkadeep Acharya](https://www.semanticscholar.org/author/Arkadeep-Acharya/2273560136)[Sriparna Saha](https://www.semanticscholar.org/author/Sriparna-Saha/2244669276)[Vinija Jain](https://www.semanticscholar.org/author/Vinija-Jain/2212131028)[Aman Chadha](https://www.semanticscholar.org/author/Aman-Chadha/2275226689)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nThis classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.Expand\n\n*   [9](https://www.semanticscholar.org/paper/5506409ad47e709ed1a1e4ab0319f4f931b7e379#citing-papers)\n[](https://www.semanticscholar.org/reader/5506409ad47e709ed1a1e4ab0319f4f931b7e379)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### Visual Large Language Models for Generalized and Specialized Applications](https://www.semanticscholar.org/paper/Visual-Large-Language-Models-for-Generalized-and-Li-Lai/cca431c69807196211c657a35697644743944b12)\n\n[Yifan Li](https://www.semanticscholar.org/author/Yifan-Li/2267393864)[Zhixin Lai](https://www.semanticscholar.org/author/Zhixin-Lai/2334744441)+7 authors [Yu Kong](https://www.semanticscholar.org/author/Yu-Kong/2267333754)\n\nComputer Science, Engineering\n\n*   2025\n\nTLDR\n\nThis survey focuses on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development, to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs.Expand\n\n[](https://www.semanticscholar.org/reader/cca431c69807196211c657a35697644743944b12)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\n86 References\n-------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://www.semanticscholar.org/paper/BLIP%3A-Bootstrapping-Language-Image-Pre-training-for-Li-Li/a3b42a83669998f65df60d7c065a70d07ca95e99)\n\n[Junnan Li](https://www.semanticscholar.org/author/Junnan-Li/49299019)[Dongxu Li](https://www.semanticscholar.org/author/Dongxu-Li/2981509)[Caiming Xiong](https://www.semanticscholar.org/author/Caiming-Xiong/2054594326)[S. Hoi](https://www.semanticscholar.org/author/S.-Hoi/1741126)\n\nComputer Science\n\n[International Conference on Machine Learning](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Machine%20Learning)\n\n*   2022\n\nTLDR\n\nBLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones, and demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner.Expand\n\n*   [3,290](https://www.semanticscholar.org/paper/a3b42a83669998f65df60d7c065a70d07ca95e99#citing-papers)\n[](https://www.semanticscholar.org/reader/a3b42a83669998f65df60d7c065a70d07ca95e99)\\[PDF\\]\n\nSave\n\n[### Flamingo: a Visual Language Model for Few-Shot Learning](https://www.semanticscholar.org/paper/Flamingo%3A-a-Visual-Language-Model-for-Few-Shot-Alayrac-Donahue/26218bdcc3945c7edae7aa2adbfba4cd820a2df3)\n\n[Jean-Baptiste Alayrac](https://www.semanticscholar.org/author/Jean-Baptiste-Alayrac/2285263)[Jeff Donahue](https://www.semanticscholar.org/author/Jeff-Donahue/7408951)+24 authors [K. Simonyan](https://www.semanticscholar.org/author/K.-Simonyan/34838386)\n\nComputer Science\n\n[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)\n\n*   2022\n\nTLDR\n\nThis work introduces Flamingo, a family of Visual Language Models (VLM) with this ability to bridge powerful pretrained vision-only and language-only models, handle sequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as inputs.Expand\n\n*   [2,690](https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3#citing-papers)\n*   [Highly Influential](https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/26218bdcc3945c7edae7aa2adbfba4cd820a2df3)\\[PDF\\]\n\n*   9 Excerpts\n\nSave\n\n[### VinVL: Revisiting Visual Representations in Vision-Language Models](https://www.semanticscholar.org/paper/VinVL%3A-Revisiting-Visual-Representations-in-Models-Zhang-Li/63c74d15940af1af9b386b5762e4445e54c73719)\n\n[Pengchuan Zhang](https://www.semanticscholar.org/author/Pengchuan-Zhang/9325940)[Xiujun Li](https://www.semanticscholar.org/author/Xiujun-Li/47058148)+5 authors [Jianfeng Gao](https://www.semanticscholar.org/author/Jianfeng-Gao/48441311)\n\nComputer Science\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2021\n\nTLDR\n\nThis paper develops an improved object detection model to provide object-centric representations of images and feeds the visual features generated into a Transformer-based VL fusion model OSCAR, and utilizes an improved approach OSCar+ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks.Expand\n\n*   [829](https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/63c74d15940af1af9b386b5762e4445e54c73719)\n    \n\nSave\n\n[### VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://www.semanticscholar.org/paper/VL-BERT%3A-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu/4aa6298b606941a282d735fa3143da293199d2ca)\n\n[Weijie Su](https://www.semanticscholar.org/author/Weijie-Su/145499378)[Xizhou Zhu](https://www.semanticscholar.org/author/Xizhou-Zhu/2578924)+4 authors [Jifeng Dai](https://www.semanticscholar.org/author/Jifeng-Dai/3304536)\n\nComputer Science, Linguistics\n\n[International Conference on Learning…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Learning%20Representations)\n\n*   2020\n\nTLDR\n\nA new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT), which adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input.Expand\n\n*   [1,582](https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/4aa6298b606941a282d735fa3143da293199d2ca)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://www.semanticscholar.org/paper/Conceptual-12M%3A-Pushing-Web-Scale-Image-Text-To-Changpinyo-Sharma/394be105b87e9bfe72c20efe6338de10604e1a11)\n\n[Soravit Changpinyo](https://www.semanticscholar.org/author/Soravit-Changpinyo/2059199)[P. Sharma](https://www.semanticscholar.org/author/P.-Sharma/46756891)[Nan Ding](https://www.semanticscholar.org/author/Nan-Ding/2066767241)[Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)\n\nComputer Science\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2021\n\nTLDR\n\nThe results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.Expand\n\n*   [939](https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11#citing-papers)\n[](https://www.semanticscholar.org/reader/394be105b87e9bfe72c20efe6338de10604e1a11)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### Language Is Not All You Need: Aligning Perception with Language Models](https://www.semanticscholar.org/paper/Language-Is-Not-All-You-Need%3A-Aligning-Perception-Huang-Dong/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c)\n\n[Shaohan Huang](https://www.semanticscholar.org/author/Shaohan-Huang/3110003)[Li Dong](https://www.semanticscholar.org/author/Li-Dong/145307652)+14 authors [Furu Wei](https://www.semanticscholar.org/author/Furu-Wei/49807919)\n\nComputer Science, Linguistics\n\n[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)\n\n*   2023\n\nTLDR\n\nThis work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.Expand\n\n*   [467](https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c#citing-papers)\n[](https://www.semanticscholar.org/reader/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://www.semanticscholar.org/paper/Image-as-a-Foreign-Language%3A-BEiT-Pretraining-for-Wang-Bao/02251886950770e82b3d68564d60cdfe15e73199)\n\n[Wenhui Wang](https://www.semanticscholar.org/author/Wenhui-Wang/51456429)[Hangbo Bao](https://www.semanticscholar.org/author/Hangbo-Bao/10699417)+8 authors [Furu Wei](https://www.semanticscholar.org/author/Furu-Wei/49807919)\n\nComputer Science, Linguistics\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2022\n\nTLDR\n\nThis work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding.Expand\n\n*   [583](https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199#citing-papers)\n[](https://www.semanticscholar.org/reader/02251886950770e82b3d68564d60cdfe15e73199)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://www.semanticscholar.org/paper/PaLI%3A-A-Jointly-Scaled-Multilingual-Language-Image-Chen-Wang/28630034bb29760df01ab033b743e30b37f336ae)\n\n[Xi Chen](https://www.semanticscholar.org/author/Xi-Chen/2145309103)[Xiao Wang](https://www.semanticscholar.org/author/Xiao-Wang/144129720)+26 authors [Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)\n\nComputer Science, Linguistics\n\n[International Conference on Learning…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Learning%20Representations)\n\n*   2023\n\nTLDR\n\nThe PaLI (Pathways Language and Image model), a model that achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.Expand\n\n*   [599](https://www.semanticscholar.org/paper/28630034bb29760df01ab033b743e30b37f336ae#citing-papers)\n[](https://www.semanticscholar.org/reader/28630034bb29760df01ab033b743e30b37f336ae)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### PaLI-X: On Scaling up a Multilingual Vision and Language Model](https://www.semanticscholar.org/paper/PaLI-X%3A-On-Scaling-up-a-Multilingual-Vision-and-Chen-Djolonga/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a)\n\n[Xi Chen](https://www.semanticscholar.org/author/Xi-Chen/2145309103)[Josip Djolonga](https://www.semanticscholar.org/author/Josip-Djolonga/2941141)+40 authors [Radu Soricut](https://www.semanticscholar.org/author/Radu-Soricut/1737285)\n\nComputer Science, Linguistics\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2023\n\nTLDR\n\nPaLI-X, a multilingual vision and language model, advances the state-of-the-art on most vision-and-language benchmarks considered and observes emerging capabilities, such as complex counting and multilingual object detection, tasks that are not explicitly in the training mix.Expand\n\n*   [165](https://www.semanticscholar.org/paper/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a#citing-papers)\n[](https://www.semanticscholar.org/reader/3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a)\\[PDF\\]\n\nSave\n\n[### ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1)\n\n[Wonjae Kim](https://www.semanticscholar.org/author/Wonjae-Kim/2382193)[Bokyung Son](https://www.semanticscholar.org/author/Bokyung-Son/65842988)[Ildoo Kim](https://www.semanticscholar.org/author/Ildoo-Kim/14972026)\n\nComputer Science\n\n[International Conference on Machine Learning](https://www.semanticscholar.org/venue?name=International%20Conference%20on%20Machine%20Learning)\n\n*   2021\n\nTLDR\n\nA minimal VLP model, Vision-and-Language Transformer (ViLT), monolithic in the sense that the processing of visual inputs is drastically simplified to just the same convolution-free manner that the authors process textual inputs, showing that ViLT is up to tens of times faster than previous VLP models, yet with competitive or better downstream task performance.Expand\n\n*   [1,513](https://www.semanticscholar.org/paper/0839722fb5369c0abaff8515bfc08299efc790a1#citing-papers)\n[](https://www.semanticscholar.org/reader/0839722fb5369c0abaff8515bfc08299efc790a1)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\nRelated Papers\n--------------\n\nShowing 1 through 3 of 0 Related Papers\n\n*   [Figures and Tables](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#extracted)\n*   [Topics](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#paper-topics)\n*   [498 Citations](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#citing-papers)\n*   [86 References](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#cited-papers)\n*   [Related Papers](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/431a2dfd60225e615cf27d044ad00a6ac147501f/figure/8#related-papers)\n\n[](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/7)\n\n![Image 38](https://figures.semanticscholar.org/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/17-Figure5-1.png)\n\nFigure 5: Visualization of the Grounding and OCR data used for training Qwen-VL \n\nPublished in 2023\n\n[Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f)[Jinze Bai](https://www.semanticscholar.org/author/Jinze-Bai/41211611)[Shuai Bai](https://www.semanticscholar.org/author/Shuai-Bai/3768186)+6 authors [Jingren Zhou](https://www.semanticscholar.org/author/Jingren-Zhou/1709595)\n\n[](https://www.semanticscholar.org/paper/Qwen-VL%3A-A-Versatile-Vision-Language-Model-for-Text-Bai-Bai/fc6a2f7478f68adefd69e2071f27e38aa1647f2f/figure/9)Figure 9 of 18\n\nStay Connected With Semantic Scholar\n\nSign Up\n\nWhat Is Semantic Scholar?\n-------------------------\n\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.\n\n[Learn More](https://www.semanticscholar.org/about)\n\n### About\n\n[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)\n\n### Product\n\n[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)\n\n### API\n\n[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)\n\n### Research\n\n[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)\n\n### Help\n\n[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact\n\nProudly built by [Ai2 (opens in a new tab)](http://allenai.org/)\n\nCollaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)\n\n[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)\n\nACCEPT & CONTINUE",
  "usage": {
    "tokens": 12045
  }
}
```
