---
title: [PDF] SceneFormer: Indoor Scene Generation with Transformers | Semantic Scholar
description: This work generates a sequence of objects, along with their locations and orientations conditioned on a room layout, using only the cross-attention mechanism of transformers, and shows that this model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. We address the task of indoor scene generation by generating a sequence of objects, along with their locations and orientations conditioned on a room layout. Large-scale indoor scene datasets allow us to extract patterns from user-designed indoor scenes, and generate new scenes based on these patterns. Existing methods rely on the 2D or 3D appearance of these scenes in addition to object positions, and make assumptions about the possible relations between objects. In contrast, we do not use any appearance information, and implicitly learn object relations using the self-attention mechanism of transformers. We show that our model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. Our method is also flexible, as it can be conditioned not only on the room layout but also on text descriptions of the room, using only the cross-attention mechanism of transformers. Our user study shows that our generated scenes are preferred to the state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom and living room scenes, respectively. At the same time, we generate a scene in 1.48 seconds on average, 20% faster than FastSynth.
url: https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db
timestamp: 2025-01-20T15:42:33.305Z
domain: www.semanticscholar.org
path: paper_SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth_082946b064a90a612bb73fcece6ab9667beaf8db
---

# [PDF] SceneFormer: Indoor Scene Generation with Transformers | Semantic Scholar


This work generates a sequence of objects, along with their locations and orientations conditioned on a room layout, using only the cross-attention mechanism of transformers, and shows that this model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. We address the task of indoor scene generation by generating a sequence of objects, along with their locations and orientations conditioned on a room layout. Large-scale indoor scene datasets allow us to extract patterns from user-designed indoor scenes, and generate new scenes based on these patterns. Existing methods rely on the 2D or 3D appearance of these scenes in addition to object positions, and make assumptions about the possible relations between objects. In contrast, we do not use any appearance information, and implicitly learn object relations using the self-attention mechanism of transformers. We show that our model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. Our method is also flexible, as it can be conditioned not only on the room layout but also on text descriptions of the room, using only the cross-attention mechanism of transformers. Our user study shows that our generated scenes are preferred to the state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom and living room scenes, respectively. At the same time, we generate a scene in 1.48 seconds on average, 20% faster than FastSynth.


## Content

\[PDF\] SceneFormer: Indoor Scene Generation with Transformers | Semantic Scholar
===============
                                                          

[Skip to search form](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#search-form)[Skip to main content](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#account-menu)

[](https://www.semanticscholar.org/)

Search 223,685,209 papers from all fields of science

Search

Sign InCreate Free Account

*   DOI:[10.1109/3DV53792.2021.00021](https://doi.org/10.1109/3DV53792.2021.00021)
    
*   Corpus ID: 229297859

SceneFormer: Indoor Scene Generation with Transformers
======================================================

@article{Wang2020SceneFormerIS,
  title={SceneFormer: Indoor Scene Generation with Transformers},
  author={Xinpeng Wang and Chandan Yeshwanth and Matthias Nie{\\ss}ner},
  journal={2021 International Conference on 3D Vision (3DV)},
  year={2020},
  pages={106-115},
  url={https://api.semanticscholar.org/CorpusID:229297859}
}

*   [Xinpeng Wang](https://www.semanticscholar.org/author/Xinpeng-Wang/2108254820), [Chandan Yeshwanth](https://www.semanticscholar.org/author/Chandan-Yeshwanth/9537630), [M. Nießner](https://www.semanticscholar.org/author/M.-Nie%C3%9Fner/2209612)
*   Published in [International Conference on…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%203D%20Vision) 17 December 2020
*   Computer Science

TLDR

This work generates a sequence of objects, along with their locations and orientations conditioned on a room layout, using only the cross-attention mechanism of transformers, and shows that this model design leads to faster scene generation with similar or improved levels of realism compared to previous methods.Expand

[](https://www.semanticscholar.org/reader/30e77fb57578bf6c7fd8f44d63a9467e6f7804dd)\[PDF\] Semantic Reader

Save to LibrarySave

Create AlertAlert

Cite

Share

120 Citations
-------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### ATISS: Autoregressive Transformers for Indoor Scene Synthesis](https://www.semanticscholar.org/paper/ATISS%3A-Autoregressive-Transformers-for-Indoor-Scene-Paschalidou-Kar/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596)

[Despoina Paschalidou](https://www.semanticscholar.org/author/Despoina-Paschalidou/3493472)[Amlan Kar](https://www.semanticscholar.org/author/Amlan-Kar/24899770)[Maria Shugrina](https://www.semanticscholar.org/author/Maria-Shugrina/2854827)[Karsten Kreis](https://www.semanticscholar.org/author/Karsten-Kreis/32113848)[Andreas Geiger](https://www.semanticscholar.org/author/Andreas-Geiger/47237027)[S. Fidler](https://www.semanticscholar.org/author/S.-Fidler/37895334)

Computer Science, Engineering

[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)

*   2021

TLDR

ATISS is presented, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan, which has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.Expand

*   [116](https://www.semanticscholar.org/paper/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596)\[PDF\]

*   10 Excerpts

Save

[### S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field](https://www.semanticscholar.org/paper/S-INF%3A-Towards-Realistic-Indoor-Scene-Synthesis-via-Liang-Xu/96eff6040870452d0c02c19482117f8774ded23d)

[Zixi Liang](https://www.semanticscholar.org/author/Zixi-Liang/2337339953)[Guowei Xu](https://www.semanticscholar.org/author/Guowei-Xu/2311498835)[Haifeng Wu](https://www.semanticscholar.org/author/Haifeng-Wu/2336954780)[Ye Huang](https://www.semanticscholar.org/author/Ye-Huang/2275178703)[Wen Li](https://www.semanticscholar.org/author/Wen-Li/2145169829)[Lixin Duan](https://www.semanticscholar.org/author/Lixin-Duan/2256685323)

Computer Science, Engineering

*   2024

TLDR

A new method, Scene Implicit Neural Field (S-INF), is introduced, aiming to learn meaningful representations of multimodal relationships, to enhance the realism of indoor scene synthesis.Expand

*   [PDF](https://www.semanticscholar.org/paper/96eff6040870452d0c02c19482117f8774ded23d)
    

*   1 Excerpt

Save

[### Learning Object Context for Novel-view Scene Layout Generation](https://www.semanticscholar.org/paper/Learning-Object-Context-for-Novel-view-Scene-Layout-Qiao-Hancke/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f)

[Xiaotian Qiao](https://www.semanticscholar.org/author/Xiaotian-Qiao/2131614)[G. Hancke](https://www.semanticscholar.org/author/G.-Hancke/32434985)[Rynson W. H. Lau](https://www.semanticscholar.org/author/Rynson-W.-H.-Lau/1726262)

Computer Science

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2022

TLDR

A deep model to capture contextualized object representation by explicitly modeling the object context transformation in the scene is proposed, essential in generating geometrically and semantically consistent scene layouts of different views.Expand

*   [2](https://www.semanticscholar.org/paper/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f)
    

*   1 Excerpt

Save

[### Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases](https://www.semanticscholar.org/paper/Open-Universe-Indoor-Scene-Generation-using-LLM-and-Aguina-Kang-Gumin/6768fd3a63ade98b09c83e18939c19abb6573575)

[Rio Aguina-Kang](https://www.semanticscholar.org/author/Rio-Aguina-Kang/2291965006)[Maxim Gumin](https://www.semanticscholar.org/author/Maxim-Gumin/2291963571)+7 authors [Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/2286615965)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

Experimental evaluations show that the system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.Expand

*   [12](https://www.semanticscholar.org/paper/6768fd3a63ade98b09c83e18939c19abb6573575#citing-papers)
[](https://www.semanticscholar.org/reader/6768fd3a63ade98b09c83e18939c19abb6573575)\[PDF\]

Save

[### 3D scene generation from scene graphs and self-attention](https://www.semanticscholar.org/paper/3D-scene-generation-from-scene-graphs-and-Bonazzi-Wang/e0f0edb887481146b7c58ea7907829056bdbc4cb)

[Pietro Bonazzi](https://www.semanticscholar.org/author/Pietro-Bonazzi/2294575411)[Mengqi Wang](https://www.semanticscholar.org/author/Mengqi-Wang/2298157499)+4 authors [Davide Scaramuzza](https://www.semanticscholar.org/author/Davide-Scaramuzza/2243182830)

Computer Science

*   2024

TLDR

A variant of the conditional variational autoencoder (cVAE) model is presented to synthesize 3D scenes from scene graphs and floor plans, exploiting the properties of self-attention layers to capture high-level relationships between objects in a scene and using these as the building blocks of the model.Expand

*   [Highly Influenced](https://www.semanticscholar.org/paper/e0f0edb887481146b7c58ea7907829056bdbc4cb?sort=is-influential#citing-papers)
    
*   [PDF](https://www.semanticscholar.org/paper/e0f0edb887481146b7c58ea7907829056bdbc4cb)
    

*   4 Excerpts

Save

[### CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding](https://www.semanticscholar.org/paper/CLIP-Layout%3A-Style-Consistent-Indoor-Scene-with-Liu-Xiong/d8417801c29b4429b4ecaf77d7e68407af51dbfe)

[Jingyu Liu](https://www.semanticscholar.org/author/Jingyu-Liu/2301500865)[Wenhan Xiong](https://www.semanticscholar.org/author/Wenhan-Xiong/22253126)[Ian Jones](https://www.semanticscholar.org/author/Ian-Jones/2210857390)[Yixin Nie](https://www.semanticscholar.org/author/Yixin-Nie/40383658)[Anchit Gupta](https://www.semanticscholar.org/author/Anchit-Gupta/3377939)[Barlas Ouguz](https://www.semanticscholar.org/author/Barlas-Ouguz/1628391446)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2023

TLDR

This paper introduces an auto-regressive scene model which can output instance-level predictions, using general purpose image embedding based on CLIP, which allows for zero-shot text-guided scene synthesis and editing, and easily generalizes to furniture not seen during training.Expand

*   [12](https://www.semanticscholar.org/paper/d8417801c29b4429b4ecaf77d7e68407af51dbfe#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/d8417801c29b4429b4ecaf77d7e68407af51dbfe?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/d8417801c29b4429b4ecaf77d7e68407af51dbfe)\[PDF\]

*   4 Excerpts

Save

[### Mixed Diffusion for 3D Indoor Scene Synthesis](https://www.semanticscholar.org/paper/Mixed-Diffusion-for-3D-Indoor-Scene-Synthesis-Hu-Arroyo/c1b1eda55d53f7eae4ef6a6e4ca23af56bebd368)

[Siyi Hu](https://www.semanticscholar.org/author/Siyi-Hu/2267482402)[Diego Martín Arroyo](https://www.semanticscholar.org/author/Diego-Mart%C3%ADn-Arroyo/52133089)[Stephanie Debats](https://www.semanticscholar.org/author/Stephanie-Debats/2243335286)[Fabian Manhardt](https://www.semanticscholar.org/author/Fabian-Manhardt/2741443)[Luca Carlone](https://www.semanticscholar.org/author/Luca-Carlone/2239485876)[Federico Tombari](https://www.semanticscholar.org/author/Federico-Tombari/2275248895)

Computer Science, Engineering

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

MiDiffusion is presented, a novel mixed discrete-continuous diffusion model designed to synthesize plausible 3D indoor scenes given a floor plan and pre-arranged objects that outperforms state-of-the-art autoregressive and diffusion models in floor-conditioned 3D scene synthesis.Expand

*   [PDF](https://www.semanticscholar.org/paper/c1b1eda55d53f7eae4ef6a6e4ca23af56bebd368)
    

*   3 Excerpts

Save

[### CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs](https://www.semanticscholar.org/paper/CommonScenes%3A-Generating-Commonsense-3D-Indoor-with-Zhai-%C3%96rnek/3572436b7601ab9c6af302c702bdebc4440a83ff)

[Guangyao Zhai](https://www.semanticscholar.org/author/Guangyao-Zhai/15629119)[Evin Pınar Örnek](https://www.semanticscholar.org/author/Evin-P%C4%B1nar-%C3%96rnek/1491550942)+4 authors [Benjamin Busam](https://www.semanticscholar.org/author/Benjamin-Busam/2139554669)

Computer Science, Engineering

[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)

*   2023

TLDR

CommonScenes is a fully generative model that converts scene graphs into corresponding controllable 3D scenes, which are semantically realistic and conform to commonsense and shows clear advantages over other methods regarding generation consistency, quality, and diversity.Expand

*   [31](https://www.semanticscholar.org/paper/3572436b7601ab9c6af302c702bdebc4440a83ff#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/3572436b7601ab9c6af302c702bdebc4440a83ff)
    

*   1 Excerpt

Save

[### SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation With Fine-Grained Geometry](https://www.semanticscholar.org/paper/SceneHGN%3A-Hierarchical-Graph-Networks-for-3D-Indoor-Gao-Sun/09fe4b78184db956e73e7fac084391a7d66bf934)

[Lin Gao](https://www.semanticscholar.org/author/Lin-Gao/144614914)[Jiali Sun](https://www.semanticscholar.org/author/Jiali-Sun/2167760559)[Kaichun Mo](https://www.semanticscholar.org/author/Kaichun-Mo/2216377)[Yu-Kun Lai](https://www.semanticscholar.org/author/Yu-Kun-Lai/7827503)[L. Guibas](https://www.semanticscholar.org/author/L.-Guibas/51352814)[J. Yang](https://www.semanticscholar.org/author/J.-Yang/37302154)

Computer Science, Engineering

[IEEE Transactions on Pattern Analysis and Machine…](https://www.semanticscholar.org/venue?name=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence)

*   2023

TLDR

The generation network is a conditional recursive neural network (RvNN) based variational autoencoder (VAE) that learns to generate detailed content with fine-grained geometry for a room, given the room boundary as the condition.Expand

*   [24](https://www.semanticscholar.org/paper/09fe4b78184db956e73e7fac084391a7d66bf934#citing-papers)
[](https://www.semanticscholar.org/reader/09fe4b78184db956e73e7fac084391a7d66bf934)\[PDF\]

*   1 Excerpt

Save

[### DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis](https://www.semanticscholar.org/paper/DiffuScene%3A-Denoising-Diffusion-Models-for-Indoor-Tang-Nie/7ea7133db0daf6842b86b6797a057e5650a59d87)

[Jiapeng Tang](https://www.semanticscholar.org/author/Jiapeng-Tang/2269685310)[Yinyu Nie](https://www.semanticscholar.org/author/Yinyu-Nie/2269734614)[Lev Markhasin](https://www.semanticscholar.org/author/Lev-Markhasin/2398593)[Angela Dai](https://www.semanticscholar.org/author/Angela-Dai/2208531)[Justus Thies](https://www.semanticscholar.org/author/Justus-Thies/2058610268)[M. Nießner](https://www.semanticscholar.org/author/M.-Nie%C3%9Fner/2209612)

Computer Science

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2024

TLDR

A diffusion network to synthesize a collection of 3D indoor objects by denoising a set of unordered object attributes and enables many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis.Expand

*   [13](https://www.semanticscholar.org/paper/7ea7133db0daf6842b86b6797a057e5650a59d87#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/7ea7133db0daf6842b86b6797a057e5650a59d87?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/7ea7133db0daf6842b86b6797a057e5650a59d87)\[PDF\]

*   7 Excerpts

Save

...

1

2

3

4

5

...

36 References
-------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### Deep Generative Modeling for Scene Synthesis via Hybrid Representations](https://www.semanticscholar.org/paper/Deep-Generative-Modeling-for-Scene-Synthesis-via-Zhang-Yang/672037ca32175b2b1da22d653953ae2ce689443f)

[Zaiwei Zhang](https://www.semanticscholar.org/author/Zaiwei-Zhang/3467579)[Zhenpei Yang](https://www.semanticscholar.org/author/Zhenpei-Yang/41033760)+4 authors [Qi-Xing Huang](https://www.semanticscholar.org/author/Qi-Xing-Huang/151485038)

Computer Science, Engineering

[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)

*   2020

TLDR

A deep generative scene modeling technique using a feed-forward neural network that maps a prior distribution to the distribution of primary objects in indoor scenes, and introduces a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes.Expand

*   [109](https://www.semanticscholar.org/paper/672037ca32175b2b1da22d653953ae2ce689443f#citing-papers)
[](https://www.semanticscholar.org/reader/672037ca32175b2b1da22d653953ae2ce689443f)\[PDF\]

*   3 Excerpts

Save

[### SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation](https://www.semanticscholar.org/paper/SceneGraphNet%3A-Neural-Message-Passing-for-3D-Indoor-Zhou-While/99e3da8da39ece922fca46ce9f2301626354baaf)

[Yang Zhou](https://www.semanticscholar.org/author/Yang-Zhou/2145498628)[Zachary While](https://www.semanticscholar.org/author/Zachary-While/151270911)[E. Kalogerakis](https://www.semanticscholar.org/author/E.-Kalogerakis/2808670)

Computer Science

[IEEE International Conference on Computer Vision](https://www.semanticscholar.org/venue?name=IEEE%20International%20Conference%20on%20Computer%20Vision)

*   2019

TLDR

A neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings by weighting messages through an attention mechanism, which significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene.Expand

*   [85](https://www.semanticscholar.org/paper/99e3da8da39ece922fca46ce9f2301626354baaf#citing-papers)
[](https://www.semanticscholar.org/reader/99e3da8da39ece922fca46ce9f2301626354baaf)\[PDF\]

*   2 Excerpts

Save

[### End-to-End Optimization of Scene Layout](https://www.semanticscholar.org/paper/End-to-End-Optimization-of-Scene-Layout-Luo-Zhang/635481cdd65f08af4912ed02115dd14805859465)

[Andrew Luo](https://www.semanticscholar.org/author/Andrew-Luo/2080134012)[Zhoutong Zhang](https://www.semanticscholar.org/author/Zhoutong-Zhang/2717546)[Jiajun Wu](https://www.semanticscholar.org/author/Jiajun-Wu/3045089)[J. Tenenbaum](https://www.semanticscholar.org/author/J.-Tenenbaum/1763295)

Computer Science

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2020

TLDR

An end-to-end variational generative model for scene layout synthesis conditioned on scene graphs that achieves higher accuracy and diversity in conditional scene synthesis and allows exemplar-based scene generation from various input forms is proposed.Expand

*   [59](https://www.semanticscholar.org/paper/635481cdd65f08af4912ed02115dd14805859465#citing-papers)
[](https://www.semanticscholar.org/reader/635481cdd65f08af4912ed02115dd14805859465)\[PDF\]

*   2 Excerpts

Save

[### Language-driven synthesis of 3D scenes from scene databases](https://www.semanticscholar.org/paper/Language-driven-synthesis-of-3D-scenes-from-scene-Ma-Patil/d299e7d5348f1e585810775175a7d832b998871e)

[Rui Ma](https://www.semanticscholar.org/author/Rui-Ma/50397286)[A. Patil](https://www.semanticscholar.org/author/A.-Patil/32349824)+7 authors [Hao Zhang](https://www.semanticscholar.org/author/Hao-Zhang/39497427)

Computer Science

[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)

*   2018

We introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene… Expand

*   [91](https://www.semanticscholar.org/paper/d299e7d5348f1e585810775175a7d832b998871e#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/d299e7d5348f1e585810775175a7d832b998871e)
    

*   1 Excerpt

Save

[### Deep convolutional priors for indoor scene synthesis](https://www.semanticscholar.org/paper/Deep-convolutional-priors-for-indoor-scene-Wang-Savva/696f4757994d13352b5b934d3c1a18dfabb71d1c)

[Kai Wang](https://www.semanticscholar.org/author/Kai-Wang/2148895382)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[Angel X. Chang](https://www.semanticscholar.org/author/Angel-X.-Chang/145830541)[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)

Computer Science

[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)

*   2018

TLDR

This work presents a convolutional neural network based approach for indoor scene synthesis that generates scenes that are preferred over the baselines, and in some cases are equally preferred to human-created scenes.Expand

*   [214](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c#citing-papers)
*   [Highly Influential](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c?sort=is-influential#citing-papers)
    
*   [PDF](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c)
    

*   10 Excerpts

Save

[### SceneSeer: 3D Scene Design with Natural Language](https://www.semanticscholar.org/paper/SceneSeer%3A-3D-Scene-Design-with-Natural-Language-Chang-Eric/dd3cd261320bb3f6dcc455a918422821c5f14ade)

[Angel X. Chang](https://www.semanticscholar.org/author/Angel-X.-Chang/145830541)[Mihail Eric](https://www.semanticscholar.org/author/Mihail-Eric/144528428)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[Christopher D. Manning](https://www.semanticscholar.org/author/Christopher-D.-Manning/144783904)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2017

Designing 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the… Expand

*   [56](https://www.semanticscholar.org/paper/dd3cd261320bb3f6dcc455a918422821c5f14ade#citing-papers)
[](https://www.semanticscholar.org/reader/dd3cd261320bb3f6dcc455a918422821c5f14ade)\[PDF\]

*   3 Excerpts

Save

[### Learning 3D Scene Synthesis from Annotated RGB‐D Images](https://www.semanticscholar.org/paper/Learning-3D-Scene-Synthesis-from-Annotated-RGB%E2%80%90D-Kermani-Liao/a0d8f3429d442920f2ee37c151006a4d802f90dc)

[Z. S. Kermani](https://www.semanticscholar.org/author/Z.-S.-Kermani/47382027)[Z. Liao](https://www.semanticscholar.org/author/Z.-Liao/1453653507)[P. Tan](https://www.semanticscholar.org/author/P.-Tan/145604260)[H. Zhang](https://www.semanticscholar.org/author/H.-Zhang/1682058)

Computer Science, Engineering

[Computer graphics forum (Print)](https://www.semanticscholar.org/venue?name=Computer%20graphics%20forum%20%28Print%29)

*   2016

TLDR

While the algorithm inserts objects one at a time, it attains holistic plausibility of the whole current scene while offering controllability through progressive synthesis, compared to previous works on probabilistic learning for object placement.Expand

*   [48](https://www.semanticscholar.org/paper/a0d8f3429d442920f2ee37c151006a4d802f90dc#citing-papers)
*   [PDF](https://www.semanticscholar.org/paper/a0d8f3429d442920f2ee37c151006a4d802f90dc)
    

*   1 Excerpt

Save

[### Example-based synthesis of 3D object arrangements](https://www.semanticscholar.org/paper/Example-based-synthesis-of-3D-object-arrangements-Fisher-Ritchie/2039f127f8c567691418e925b7582e69f7cdd861)

[Matthew Fisher](https://www.semanticscholar.org/author/Matthew-Fisher/145002004)[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[T. Funkhouser](https://www.semanticscholar.org/author/T.-Funkhouser/1807080)[P. Hanrahan](https://www.semanticscholar.org/author/P.-Hanrahan/144872229)

Computer Science

[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)

*   2012

TLDR

This work introduces a probabilistic model for scenes based on Bayesian networks and Gaussian mixtures that can be trained from a small number of input examples, and develops a clustering algorithm that groups objects occurring in a database of scenes according to their local scene neighborhoods.Expand

*   [369](https://www.semanticscholar.org/paper/2039f127f8c567691418e925b7582e69f7cdd861#citing-papers)

*   1 Excerpt

Save

[### Learning to generate new indoor scenes](https://www.semanticscholar.org/paper/Learning-to-generate-new-indoor-scenes-Purkait-Zach/d3fb5edef50e11351459e6f989c70fa7b45c69f2)

[Pulak Purkait](https://www.semanticscholar.org/author/Pulak-Purkait/33305173)[C. Zach](https://www.semanticscholar.org/author/C.-Zach/1713941)[I. Reid](https://www.semanticscholar.org/author/I.-Reid/145950884)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2019

TLDR

This work proposes a neural network to learn a generative model for sampling consistent indoor scene layouts that learns the co-occurrences, and appearance parameters such as shape and pose, for different objects categories through a grammar-based auto-encoder, resulting in a compact and accurate representation for scene layouts.Expand

*   [1](https://www.semanticscholar.org/paper/d3fb5edef50e11351459e6f989c70fa7b45c69f2#citing-papers)
[](https://www.semanticscholar.org/reader/d3fb5edef50e11351459e6f989c70fa7b45c69f2)\[PDF\]

*   1 Excerpt

Save

[### Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models](https://www.semanticscholar.org/paper/Fast-and-Flexible-Indoor-Scene-Synthesis-via-Deep-Ritchie-Wang/df81de654e048f710088fce45ee9846f3abd6b79)

[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)[Kai Wang](https://www.semanticscholar.org/author/Kai-Wang/2148895382)[Yu-An Lin](https://www.semanticscholar.org/author/Yu-An-Lin/2165971076)

Computer Science, Engineering

[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)

*   2019

TLDR

A new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models, and generates results that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.Expand

*   [136](https://www.semanticscholar.org/paper/df81de654e048f710088fce45ee9846f3abd6b79#citing-papers)
*   [Highly Influential](https://www.semanticscholar.org/paper/df81de654e048f710088fce45ee9846f3abd6b79?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/df81de654e048f710088fce45ee9846f3abd6b79)\[PDF\]

*   6 Excerpts

Save

...

1

2

3

4

...

Related Papers
--------------

Showing 1 through 3 of 0 Related Papers

*   [120 Citations](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#citing-papers)
*   [36 References](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#cited-papers)
*   [Related Papers](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#related-papers)

Stay Connected With Semantic Scholar

Sign Up

What Is Semantic Scholar?
-------------------------

Semantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.

[Learn More](https://www.semanticscholar.org/about)

### About

[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)

### Product

[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)

### API

[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)

### Research

[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)

### Help

[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact

Proudly built by [Ai2 (opens in a new tab)](http://allenai.org/)

Collaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)

[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)

By clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)

ACCEPT & CONTINUE

## Metadata

```json
{
  "title": "[PDF] SceneFormer: Indoor Scene Generation with Transformers | Semantic Scholar",
  "description": "This work generates a sequence of objects, along with their locations and orientations conditioned on a room layout, using only the cross-attention mechanism of transformers, and shows that this model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. We address the task of indoor scene generation by generating a sequence of objects, along with their locations and orientations conditioned on a room layout. Large-scale indoor scene datasets allow us to extract patterns from user-designed indoor scenes, and generate new scenes based on these patterns. Existing methods rely on the 2D or 3D appearance of these scenes in addition to object positions, and make assumptions about the possible relations between objects. In contrast, we do not use any appearance information, and implicitly learn object relations using the self-attention mechanism of transformers. We show that our model design leads to faster scene generation with similar or improved levels of realism compared to previous methods. Our method is also flexible, as it can be conditioned not only on the room layout but also on text descriptions of the room, using only the cross-attention mechanism of transformers. Our user study shows that our generated scenes are preferred to the state-of-the-art FastSynth scenes 53.9% and 56.7% of the time for bedroom and living room scenes, respectively. At the same time, we generate a scene in 1.48 seconds on average, 20% faster than FastSynth.",
  "url": "https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db",
  "content": "\\[PDF\\] SceneFormer: Indoor Scene Generation with Transformers | Semantic Scholar\n===============\n                                                          \n\n[Skip to search form](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#search-form)[Skip to main content](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#account-menu)\n\n[](https://www.semanticscholar.org/)\n\nSearch 223,685,209 papers from all fields of science\n\nSearch\n\nSign InCreate Free Account\n\n*   DOI:[10.1109/3DV53792.2021.00021](https://doi.org/10.1109/3DV53792.2021.00021)\n    \n*   Corpus ID: 229297859\n\nSceneFormer: Indoor Scene Generation with Transformers\n======================================================\n\n@article{Wang2020SceneFormerIS,\n  title={SceneFormer: Indoor Scene Generation with Transformers},\n  author={Xinpeng Wang and Chandan Yeshwanth and Matthias Nie{\\\\ss}ner},\n  journal={2021 International Conference on 3D Vision (3DV)},\n  year={2020},\n  pages={106-115},\n  url={https://api.semanticscholar.org/CorpusID:229297859}\n}\n\n*   [Xinpeng Wang](https://www.semanticscholar.org/author/Xinpeng-Wang/2108254820), [Chandan Yeshwanth](https://www.semanticscholar.org/author/Chandan-Yeshwanth/9537630), [M. Nießner](https://www.semanticscholar.org/author/M.-Nie%C3%9Fner/2209612)\n*   Published in [International Conference on…](https://www.semanticscholar.org/venue?name=International%20Conference%20on%203D%20Vision) 17 December 2020\n*   Computer Science\n\nTLDR\n\nThis work generates a sequence of objects, along with their locations and orientations conditioned on a room layout, using only the cross-attention mechanism of transformers, and shows that this model design leads to faster scene generation with similar or improved levels of realism compared to previous methods.Expand\n\n[](https://www.semanticscholar.org/reader/30e77fb57578bf6c7fd8f44d63a9467e6f7804dd)\\[PDF\\] Semantic Reader\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n120 Citations\n-------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### ATISS: Autoregressive Transformers for Indoor Scene Synthesis](https://www.semanticscholar.org/paper/ATISS%3A-Autoregressive-Transformers-for-Indoor-Scene-Paschalidou-Kar/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596)\n\n[Despoina Paschalidou](https://www.semanticscholar.org/author/Despoina-Paschalidou/3493472)[Amlan Kar](https://www.semanticscholar.org/author/Amlan-Kar/24899770)[Maria Shugrina](https://www.semanticscholar.org/author/Maria-Shugrina/2854827)[Karsten Kreis](https://www.semanticscholar.org/author/Karsten-Kreis/32113848)[Andreas Geiger](https://www.semanticscholar.org/author/Andreas-Geiger/47237027)[S. Fidler](https://www.semanticscholar.org/author/S.-Fidler/37895334)\n\nComputer Science, Engineering\n\n[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)\n\n*   2021\n\nTLDR\n\nATISS is presented, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its floor plan, which has fewer parameters, is simpler to implement and train and runs up to 8 times faster than existing methods.Expand\n\n*   [116](https://www.semanticscholar.org/paper/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/68f14f333dad84ec35fc0eb9fcfd2c41fdc02596)\\[PDF\\]\n\n*   10 Excerpts\n\nSave\n\n[### S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field](https://www.semanticscholar.org/paper/S-INF%3A-Towards-Realistic-Indoor-Scene-Synthesis-via-Liang-Xu/96eff6040870452d0c02c19482117f8774ded23d)\n\n[Zixi Liang](https://www.semanticscholar.org/author/Zixi-Liang/2337339953)[Guowei Xu](https://www.semanticscholar.org/author/Guowei-Xu/2311498835)[Haifeng Wu](https://www.semanticscholar.org/author/Haifeng-Wu/2336954780)[Ye Huang](https://www.semanticscholar.org/author/Ye-Huang/2275178703)[Wen Li](https://www.semanticscholar.org/author/Wen-Li/2145169829)[Lixin Duan](https://www.semanticscholar.org/author/Lixin-Duan/2256685323)\n\nComputer Science, Engineering\n\n*   2024\n\nTLDR\n\nA new method, Scene Implicit Neural Field (S-INF), is introduced, aiming to learn meaningful representations of multimodal relationships, to enhance the realism of indoor scene synthesis.Expand\n\n*   [PDF](https://www.semanticscholar.org/paper/96eff6040870452d0c02c19482117f8774ded23d)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Learning Object Context for Novel-view Scene Layout Generation](https://www.semanticscholar.org/paper/Learning-Object-Context-for-Novel-view-Scene-Layout-Qiao-Hancke/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f)\n\n[Xiaotian Qiao](https://www.semanticscholar.org/author/Xiaotian-Qiao/2131614)[G. Hancke](https://www.semanticscholar.org/author/G.-Hancke/32434985)[Rynson W. H. Lau](https://www.semanticscholar.org/author/Rynson-W.-H.-Lau/1726262)\n\nComputer Science\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2022\n\nTLDR\n\nA deep model to capture contextualized object representation by explicitly modeling the object context transformation in the scene is proposed, essential in generating geometrically and semantically consistent scene layouts of different views.Expand\n\n*   [2](https://www.semanticscholar.org/paper/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/c4ee5177b78c29fc14f2f5e05a27e5ec3efb1b2f)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Open-Universe Indoor Scene Generation using LLM Program Synthesis and Uncurated Object Databases](https://www.semanticscholar.org/paper/Open-Universe-Indoor-Scene-Generation-using-LLM-and-Aguina-Kang-Gumin/6768fd3a63ade98b09c83e18939c19abb6573575)\n\n[Rio Aguina-Kang](https://www.semanticscholar.org/author/Rio-Aguina-Kang/2291965006)[Maxim Gumin](https://www.semanticscholar.org/author/Maxim-Gumin/2291963571)+7 authors [Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/2286615965)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nExperimental evaluations show that the system outperforms generative models trained on 3D data for traditional, closed-universe scene generation tasks; it also outperforms a recent LLM-based layout generation method on open-universe scene generation.Expand\n\n*   [12](https://www.semanticscholar.org/paper/6768fd3a63ade98b09c83e18939c19abb6573575#citing-papers)\n[](https://www.semanticscholar.org/reader/6768fd3a63ade98b09c83e18939c19abb6573575)\\[PDF\\]\n\nSave\n\n[### 3D scene generation from scene graphs and self-attention](https://www.semanticscholar.org/paper/3D-scene-generation-from-scene-graphs-and-Bonazzi-Wang/e0f0edb887481146b7c58ea7907829056bdbc4cb)\n\n[Pietro Bonazzi](https://www.semanticscholar.org/author/Pietro-Bonazzi/2294575411)[Mengqi Wang](https://www.semanticscholar.org/author/Mengqi-Wang/2298157499)+4 authors [Davide Scaramuzza](https://www.semanticscholar.org/author/Davide-Scaramuzza/2243182830)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nA variant of the conditional variational autoencoder (cVAE) model is presented to synthesize 3D scenes from scene graphs and floor plans, exploiting the properties of self-attention layers to capture high-level relationships between objects in a scene and using these as the building blocks of the model.Expand\n\n*   [Highly Influenced](https://www.semanticscholar.org/paper/e0f0edb887481146b7c58ea7907829056bdbc4cb?sort=is-influential#citing-papers)\n    \n*   [PDF](https://www.semanticscholar.org/paper/e0f0edb887481146b7c58ea7907829056bdbc4cb)\n    \n\n*   4 Excerpts\n\nSave\n\n[### CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding](https://www.semanticscholar.org/paper/CLIP-Layout%3A-Style-Consistent-Indoor-Scene-with-Liu-Xiong/d8417801c29b4429b4ecaf77d7e68407af51dbfe)\n\n[Jingyu Liu](https://www.semanticscholar.org/author/Jingyu-Liu/2301500865)[Wenhan Xiong](https://www.semanticscholar.org/author/Wenhan-Xiong/22253126)[Ian Jones](https://www.semanticscholar.org/author/Ian-Jones/2210857390)[Yixin Nie](https://www.semanticscholar.org/author/Yixin-Nie/40383658)[Anchit Gupta](https://www.semanticscholar.org/author/Anchit-Gupta/3377939)[Barlas Ouguz](https://www.semanticscholar.org/author/Barlas-Ouguz/1628391446)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2023\n\nTLDR\n\nThis paper introduces an auto-regressive scene model which can output instance-level predictions, using general purpose image embedding based on CLIP, which allows for zero-shot text-guided scene synthesis and editing, and easily generalizes to furniture not seen during training.Expand\n\n*   [12](https://www.semanticscholar.org/paper/d8417801c29b4429b4ecaf77d7e68407af51dbfe#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/d8417801c29b4429b4ecaf77d7e68407af51dbfe?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/d8417801c29b4429b4ecaf77d7e68407af51dbfe)\\[PDF\\]\n\n*   4 Excerpts\n\nSave\n\n[### Mixed Diffusion for 3D Indoor Scene Synthesis](https://www.semanticscholar.org/paper/Mixed-Diffusion-for-3D-Indoor-Scene-Synthesis-Hu-Arroyo/c1b1eda55d53f7eae4ef6a6e4ca23af56bebd368)\n\n[Siyi Hu](https://www.semanticscholar.org/author/Siyi-Hu/2267482402)[Diego Martín Arroyo](https://www.semanticscholar.org/author/Diego-Mart%C3%ADn-Arroyo/52133089)[Stephanie Debats](https://www.semanticscholar.org/author/Stephanie-Debats/2243335286)[Fabian Manhardt](https://www.semanticscholar.org/author/Fabian-Manhardt/2741443)[Luca Carlone](https://www.semanticscholar.org/author/Luca-Carlone/2239485876)[Federico Tombari](https://www.semanticscholar.org/author/Federico-Tombari/2275248895)\n\nComputer Science, Engineering\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nMiDiffusion is presented, a novel mixed discrete-continuous diffusion model designed to synthesize plausible 3D indoor scenes given a floor plan and pre-arranged objects that outperforms state-of-the-art autoregressive and diffusion models in floor-conditioned 3D scene synthesis.Expand\n\n*   [PDF](https://www.semanticscholar.org/paper/c1b1eda55d53f7eae4ef6a6e4ca23af56bebd368)\n    \n\n*   3 Excerpts\n\nSave\n\n[### CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs](https://www.semanticscholar.org/paper/CommonScenes%3A-Generating-Commonsense-3D-Indoor-with-Zhai-%C3%96rnek/3572436b7601ab9c6af302c702bdebc4440a83ff)\n\n[Guangyao Zhai](https://www.semanticscholar.org/author/Guangyao-Zhai/15629119)[Evin Pınar Örnek](https://www.semanticscholar.org/author/Evin-P%C4%B1nar-%C3%96rnek/1491550942)+4 authors [Benjamin Busam](https://www.semanticscholar.org/author/Benjamin-Busam/2139554669)\n\nComputer Science, Engineering\n\n[Neural Information Processing Systems](https://www.semanticscholar.org/venue?name=Neural%20Information%20Processing%20Systems)\n\n*   2023\n\nTLDR\n\nCommonScenes is a fully generative model that converts scene graphs into corresponding controllable 3D scenes, which are semantically realistic and conform to commonsense and shows clear advantages over other methods regarding generation consistency, quality, and diversity.Expand\n\n*   [31](https://www.semanticscholar.org/paper/3572436b7601ab9c6af302c702bdebc4440a83ff#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/3572436b7601ab9c6af302c702bdebc4440a83ff)\n    \n\n*   1 Excerpt\n\nSave\n\n[### SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation With Fine-Grained Geometry](https://www.semanticscholar.org/paper/SceneHGN%3A-Hierarchical-Graph-Networks-for-3D-Indoor-Gao-Sun/09fe4b78184db956e73e7fac084391a7d66bf934)\n\n[Lin Gao](https://www.semanticscholar.org/author/Lin-Gao/144614914)[Jiali Sun](https://www.semanticscholar.org/author/Jiali-Sun/2167760559)[Kaichun Mo](https://www.semanticscholar.org/author/Kaichun-Mo/2216377)[Yu-Kun Lai](https://www.semanticscholar.org/author/Yu-Kun-Lai/7827503)[L. Guibas](https://www.semanticscholar.org/author/L.-Guibas/51352814)[J. Yang](https://www.semanticscholar.org/author/J.-Yang/37302154)\n\nComputer Science, Engineering\n\n[IEEE Transactions on Pattern Analysis and Machine…](https://www.semanticscholar.org/venue?name=IEEE%20Transactions%20on%20Pattern%20Analysis%20and%20Machine%20Intelligence)\n\n*   2023\n\nTLDR\n\nThe generation network is a conditional recursive neural network (RvNN) based variational autoencoder (VAE) that learns to generate detailed content with fine-grained geometry for a room, given the room boundary as the condition.Expand\n\n*   [24](https://www.semanticscholar.org/paper/09fe4b78184db956e73e7fac084391a7d66bf934#citing-papers)\n[](https://www.semanticscholar.org/reader/09fe4b78184db956e73e7fac084391a7d66bf934)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis](https://www.semanticscholar.org/paper/DiffuScene%3A-Denoising-Diffusion-Models-for-Indoor-Tang-Nie/7ea7133db0daf6842b86b6797a057e5650a59d87)\n\n[Jiapeng Tang](https://www.semanticscholar.org/author/Jiapeng-Tang/2269685310)[Yinyu Nie](https://www.semanticscholar.org/author/Yinyu-Nie/2269734614)[Lev Markhasin](https://www.semanticscholar.org/author/Lev-Markhasin/2398593)[Angela Dai](https://www.semanticscholar.org/author/Angela-Dai/2208531)[Justus Thies](https://www.semanticscholar.org/author/Justus-Thies/2058610268)[M. Nießner](https://www.semanticscholar.org/author/M.-Nie%C3%9Fner/2209612)\n\nComputer Science\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2024\n\nTLDR\n\nA diffusion network to synthesize a collection of 3D indoor objects by denoising a set of unordered object attributes and enables many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis.Expand\n\n*   [13](https://www.semanticscholar.org/paper/7ea7133db0daf6842b86b6797a057e5650a59d87#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/7ea7133db0daf6842b86b6797a057e5650a59d87?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/7ea7133db0daf6842b86b6797a057e5650a59d87)\\[PDF\\]\n\n*   7 Excerpts\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n4\n\n5\n\n...\n\n36 References\n-------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### Deep Generative Modeling for Scene Synthesis via Hybrid Representations](https://www.semanticscholar.org/paper/Deep-Generative-Modeling-for-Scene-Synthesis-via-Zhang-Yang/672037ca32175b2b1da22d653953ae2ce689443f)\n\n[Zaiwei Zhang](https://www.semanticscholar.org/author/Zaiwei-Zhang/3467579)[Zhenpei Yang](https://www.semanticscholar.org/author/Zhenpei-Yang/41033760)+4 authors [Qi-Xing Huang](https://www.semanticscholar.org/author/Qi-Xing-Huang/151485038)\n\nComputer Science, Engineering\n\n[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)\n\n*   2020\n\nTLDR\n\nA deep generative scene modeling technique using a feed-forward neural network that maps a prior distribution to the distribution of primary objects in indoor scenes, and introduces a 3D object arrangement representation that models the locations and orientations of objects, based on their size and shape attributes.Expand\n\n*   [109](https://www.semanticscholar.org/paper/672037ca32175b2b1da22d653953ae2ce689443f#citing-papers)\n[](https://www.semanticscholar.org/reader/672037ca32175b2b1da22d653953ae2ce689443f)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation](https://www.semanticscholar.org/paper/SceneGraphNet%3A-Neural-Message-Passing-for-3D-Indoor-Zhou-While/99e3da8da39ece922fca46ce9f2301626354baaf)\n\n[Yang Zhou](https://www.semanticscholar.org/author/Yang-Zhou/2145498628)[Zachary While](https://www.semanticscholar.org/author/Zachary-While/151270911)[E. Kalogerakis](https://www.semanticscholar.org/author/E.-Kalogerakis/2808670)\n\nComputer Science\n\n[IEEE International Conference on Computer Vision](https://www.semanticscholar.org/venue?name=IEEE%20International%20Conference%20on%20Computer%20Vision)\n\n*   2019\n\nTLDR\n\nA neural message passing approach to augment an input 3D indoor scene with new objects matching their surroundings by weighting messages through an attention mechanism, which significantly outperforms state-of-the-art approaches in terms of correctly predicting objects missing in a scene.Expand\n\n*   [85](https://www.semanticscholar.org/paper/99e3da8da39ece922fca46ce9f2301626354baaf#citing-papers)\n[](https://www.semanticscholar.org/reader/99e3da8da39ece922fca46ce9f2301626354baaf)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### End-to-End Optimization of Scene Layout](https://www.semanticscholar.org/paper/End-to-End-Optimization-of-Scene-Layout-Luo-Zhang/635481cdd65f08af4912ed02115dd14805859465)\n\n[Andrew Luo](https://www.semanticscholar.org/author/Andrew-Luo/2080134012)[Zhoutong Zhang](https://www.semanticscholar.org/author/Zhoutong-Zhang/2717546)[Jiajun Wu](https://www.semanticscholar.org/author/Jiajun-Wu/3045089)[J. Tenenbaum](https://www.semanticscholar.org/author/J.-Tenenbaum/1763295)\n\nComputer Science\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2020\n\nTLDR\n\nAn end-to-end variational generative model for scene layout synthesis conditioned on scene graphs that achieves higher accuracy and diversity in conditional scene synthesis and allows exemplar-based scene generation from various input forms is proposed.Expand\n\n*   [59](https://www.semanticscholar.org/paper/635481cdd65f08af4912ed02115dd14805859465#citing-papers)\n[](https://www.semanticscholar.org/reader/635481cdd65f08af4912ed02115dd14805859465)\\[PDF\\]\n\n*   2 Excerpts\n\nSave\n\n[### Language-driven synthesis of 3D scenes from scene databases](https://www.semanticscholar.org/paper/Language-driven-synthesis-of-3D-scenes-from-scene-Ma-Patil/d299e7d5348f1e585810775175a7d832b998871e)\n\n[Rui Ma](https://www.semanticscholar.org/author/Rui-Ma/50397286)[A. Patil](https://www.semanticscholar.org/author/A.-Patil/32349824)+7 authors [Hao Zhang](https://www.semanticscholar.org/author/Hao-Zhang/39497427)\n\nComputer Science\n\n[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)\n\n*   2018\n\nWe introduce a novel framework for using natural language to generate and edit 3D indoor scenes, harnessing scene semantics and text-scene grounding knowledge learned from large annotated 3D scene… Expand\n\n*   [91](https://www.semanticscholar.org/paper/d299e7d5348f1e585810775175a7d832b998871e#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/d299e7d5348f1e585810775175a7d832b998871e)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Deep convolutional priors for indoor scene synthesis](https://www.semanticscholar.org/paper/Deep-convolutional-priors-for-indoor-scene-Wang-Savva/696f4757994d13352b5b934d3c1a18dfabb71d1c)\n\n[Kai Wang](https://www.semanticscholar.org/author/Kai-Wang/2148895382)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[Angel X. Chang](https://www.semanticscholar.org/author/Angel-X.-Chang/145830541)[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)\n\nComputer Science\n\n[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)\n\n*   2018\n\nTLDR\n\nThis work presents a convolutional neural network based approach for indoor scene synthesis that generates scenes that are preferred over the baselines, and in some cases are equally preferred to human-created scenes.Expand\n\n*   [214](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c#citing-papers)\n*   [Highly Influential](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c?sort=is-influential#citing-papers)\n    \n*   [PDF](https://www.semanticscholar.org/paper/696f4757994d13352b5b934d3c1a18dfabb71d1c)\n    \n\n*   10 Excerpts\n\nSave\n\n[### SceneSeer: 3D Scene Design with Natural Language](https://www.semanticscholar.org/paper/SceneSeer%3A-3D-Scene-Design-with-Natural-Language-Chang-Eric/dd3cd261320bb3f6dcc455a918422821c5f14ade)\n\n[Angel X. Chang](https://www.semanticscholar.org/author/Angel-X.-Chang/145830541)[Mihail Eric](https://www.semanticscholar.org/author/Mihail-Eric/144528428)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[Christopher D. Manning](https://www.semanticscholar.org/author/Christopher-D.-Manning/144783904)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2017\n\nDesigning 3D scenes is currently a creative task that requires significant expertise and effort in using complex 3D design interfaces. This effortful design process starts in stark contrast to the… Expand\n\n*   [56](https://www.semanticscholar.org/paper/dd3cd261320bb3f6dcc455a918422821c5f14ade#citing-papers)\n[](https://www.semanticscholar.org/reader/dd3cd261320bb3f6dcc455a918422821c5f14ade)\\[PDF\\]\n\n*   3 Excerpts\n\nSave\n\n[### Learning 3D Scene Synthesis from Annotated RGB‐D Images](https://www.semanticscholar.org/paper/Learning-3D-Scene-Synthesis-from-Annotated-RGB%E2%80%90D-Kermani-Liao/a0d8f3429d442920f2ee37c151006a4d802f90dc)\n\n[Z. S. Kermani](https://www.semanticscholar.org/author/Z.-S.-Kermani/47382027)[Z. Liao](https://www.semanticscholar.org/author/Z.-Liao/1453653507)[P. Tan](https://www.semanticscholar.org/author/P.-Tan/145604260)[H. Zhang](https://www.semanticscholar.org/author/H.-Zhang/1682058)\n\nComputer Science, Engineering\n\n[Computer graphics forum (Print)](https://www.semanticscholar.org/venue?name=Computer%20graphics%20forum%20%28Print%29)\n\n*   2016\n\nTLDR\n\nWhile the algorithm inserts objects one at a time, it attains holistic plausibility of the whole current scene while offering controllability through progressive synthesis, compared to previous works on probabilistic learning for object placement.Expand\n\n*   [48](https://www.semanticscholar.org/paper/a0d8f3429d442920f2ee37c151006a4d802f90dc#citing-papers)\n*   [PDF](https://www.semanticscholar.org/paper/a0d8f3429d442920f2ee37c151006a4d802f90dc)\n    \n\n*   1 Excerpt\n\nSave\n\n[### Example-based synthesis of 3D object arrangements](https://www.semanticscholar.org/paper/Example-based-synthesis-of-3D-object-arrangements-Fisher-Ritchie/2039f127f8c567691418e925b7582e69f7cdd861)\n\n[Matthew Fisher](https://www.semanticscholar.org/author/Matthew-Fisher/145002004)[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)[M. Savva](https://www.semanticscholar.org/author/M.-Savva/2295141)[T. Funkhouser](https://www.semanticscholar.org/author/T.-Funkhouser/1807080)[P. Hanrahan](https://www.semanticscholar.org/author/P.-Hanrahan/144872229)\n\nComputer Science\n\n[ACM Transactions on Graphics](https://www.semanticscholar.org/venue?name=ACM%20Transactions%20on%20Graphics)\n\n*   2012\n\nTLDR\n\nThis work introduces a probabilistic model for scenes based on Bayesian networks and Gaussian mixtures that can be trained from a small number of input examples, and develops a clustering algorithm that groups objects occurring in a database of scenes according to their local scene neighborhoods.Expand\n\n*   [369](https://www.semanticscholar.org/paper/2039f127f8c567691418e925b7582e69f7cdd861#citing-papers)\n\n*   1 Excerpt\n\nSave\n\n[### Learning to generate new indoor scenes](https://www.semanticscholar.org/paper/Learning-to-generate-new-indoor-scenes-Purkait-Zach/d3fb5edef50e11351459e6f989c70fa7b45c69f2)\n\n[Pulak Purkait](https://www.semanticscholar.org/author/Pulak-Purkait/33305173)[C. Zach](https://www.semanticscholar.org/author/C.-Zach/1713941)[I. Reid](https://www.semanticscholar.org/author/I.-Reid/145950884)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2019\n\nTLDR\n\nThis work proposes a neural network to learn a generative model for sampling consistent indoor scene layouts that learns the co-occurrences, and appearance parameters such as shape and pose, for different objects categories through a grammar-based auto-encoder, resulting in a compact and accurate representation for scene layouts.Expand\n\n*   [1](https://www.semanticscholar.org/paper/d3fb5edef50e11351459e6f989c70fa7b45c69f2#citing-papers)\n[](https://www.semanticscholar.org/reader/d3fb5edef50e11351459e6f989c70fa7b45c69f2)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models](https://www.semanticscholar.org/paper/Fast-and-Flexible-Indoor-Scene-Synthesis-via-Deep-Ritchie-Wang/df81de654e048f710088fce45ee9846f3abd6b79)\n\n[Daniel Ritchie](https://www.semanticscholar.org/author/Daniel-Ritchie/34491001)[Kai Wang](https://www.semanticscholar.org/author/Kai-Wang/2148895382)[Yu-An Lin](https://www.semanticscholar.org/author/Yu-An-Lin/2165971076)\n\nComputer Science, Engineering\n\n[Computer Vision and Pattern Recognition](https://www.semanticscholar.org/venue?name=Computer%20Vision%20and%20Pattern%20Recognition)\n\n*   2019\n\nTLDR\n\nA new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models, and generates results that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.Expand\n\n*   [136](https://www.semanticscholar.org/paper/df81de654e048f710088fce45ee9846f3abd6b79#citing-papers)\n*   [Highly Influential](https://www.semanticscholar.org/paper/df81de654e048f710088fce45ee9846f3abd6b79?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/df81de654e048f710088fce45ee9846f3abd6b79)\\[PDF\\]\n\n*   6 Excerpts\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n4\n\n...\n\nRelated Papers\n--------------\n\nShowing 1 through 3 of 0 Related Papers\n\n*   [120 Citations](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#citing-papers)\n*   [36 References](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#cited-papers)\n*   [Related Papers](https://www.semanticscholar.org/paper/SceneFormer%3A-Indoor-Scene-Generation-with-Wang-Yeshwanth/082946b064a90a612bb73fcece6ab9667beaf8db#related-papers)\n\nStay Connected With Semantic Scholar\n\nSign Up\n\nWhat Is Semantic Scholar?\n-------------------------\n\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.\n\n[Learn More](https://www.semanticscholar.org/about)\n\n### About\n\n[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)\n\n### Product\n\n[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)\n\n### API\n\n[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)\n\n### Research\n\n[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)\n\n### Help\n\n[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact\n\nProudly built by [Ai2 (opens in a new tab)](http://allenai.org/)\n\nCollaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)\n\n[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)\n\nACCEPT & CONTINUE",
  "usage": {
    "tokens": 8975
  }
}
```
