---
title: [PDF] Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis | Semantic Scholar
description: A general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders, and proposes inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Recently, Large Language Models~(LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, \ie task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.
url: https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db
timestamp: 2025-01-20T15:59:23.700Z
domain: www.semanticscholar.org
path: paper_Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang_156af81de93f840df39c0973ef1343629427a7db
---

# [PDF] Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis | Semantic Scholar


A general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders, and proposes inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Recently, Large Language Models~(LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, \ie task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.


## Content

\[PDF\] Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis | Semantic Scholar
===============
                                                              

[Skip to search form](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#search-form)[Skip to main content](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#account-menu)

[](https://www.semanticscholar.org/)

Search 223,685,295 papers from all fields of science

Search

Sign InCreate Free Account

*   Corpus ID: 266902921

Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis
=======================================================================================================================

@inproceedings{Xu2024TappingTP,
  title={Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis},
  author={Lanling Xu and Junjie Zhang and Bingqian Li and Jinpeng Wang and Sheng Chen and Wayne Xin Zhao and Ji-Rong Wen},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:266902921}
}

*   [Lanling Xu](https://www.semanticscholar.org/author/Lanling-Xu/2167464968), [Junjie Zhang](https://www.semanticscholar.org/author/Junjie-Zhang/2120518257), +4 authors [Ji-Rong Wen](https://www.semanticscholar.org/author/Ji-Rong-Wen/2274218622)
*   Published 10 January 2024
*   Computer Science

TLDR

A general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders, and proposes inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance.Expand

[View PDF on arXiv](https://arxiv.org/pdf/2401.04997.pdf "https://arxiv.org/pdf/2401.04997.pdf")

Save to LibrarySave

Create AlertAlert

Cite

Share

22 Citations

[Highly Influential Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)

2

[Background Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)

12

[Methods Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)

4

[Results Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)

1

[View All](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)

Figures and Tables from this paper
----------------------------------

*   [![Image 25: table 1](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/3-Table1-1.png) table 1](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/0)
*   [![Image 26: figure 1](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/9-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/1)
*   [![Image 27: table 2](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/11-Table2-1.png) table 2](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/2)
*   [![Image 28: figure 2](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/13-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/3)
*   [![Image 29: table 3](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/12-Table3-1.png) table 3](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/4)
*   [![Image 30: figure 3](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/21-Figure3-1.png) figure 3](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/5)
*   [![Image 31: table 4](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/20-Table4-1.png) table 4](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/6)
*   [![Image 32: figure 4](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/23-Figure4-1.png) figure 4](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/7)
*   [![Image 33: figure 5](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/24-Figure5-1.png) figure 5](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/8)
*   [![Image 34: table 6](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/25-Table6-1.png) table 6](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/9)
*   [![Image 35: figure 6](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/26-Figure6-1.png) figure 6](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/10)
*   [![Image 36: table 7](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/25-Table7-1.png) table 7](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/11)
*   [![Image 37: figure 7](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/26-Figure7-1.png) figure 7](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/12)
*   [![Image 38: figure 8](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/28-Figure8-1.png) figure 8](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/13)
*   [![Image 39: table 8](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/30-Table8-1.png) table 8](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/14)
*   [![Image 40: figure 9](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/33-Figure9-1.png) figure 9](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/15)
*   [![Image 41: table 9](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/35-Table9-1.png) table 9](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/16)
*   [![Image 42: table 10](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/35-Table10-1.png) table 10](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/17)
*   [![Image 43: figure 10](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/36-Figure10-1.png) figure 10](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/18)
*   [![Image 44: table 11](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/38-Table11-1.png) table 11](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/19)
*   [![Image 45: figure 11](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/41-Figure11-1.png) figure 11](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/20)
*   [![Image 46: table 12](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/38-Table12-1.png) table 12](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/21)
*   [![Image 47: table 13](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/42-Table13-1.png) table 13](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/22)
*   [![Image 48: table 14](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/42-Table14-1.png) table 14](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/23)

View All 24 Figures & Tables

22 Citations
------------

Citation Type

Has PDF

Author

More Filters

More Filters

Filters

[### Large Language Models as Recommender Systems: A Study of Popularity Bias](https://www.semanticscholar.org/paper/Large-Language-Models-as-Recommender-Systems%3A-A-of-Lichtenberg-Buchholz/b105ada7e385c041a27ba04a71628678b77701df)

[Jan Malte Lichtenberg](https://www.semanticscholar.org/author/Jan-Malte-Lichtenberg/2060219456)[Alexander Buchholz](https://www.semanticscholar.org/author/Alexander-Buchholz/2237805687)[Pola Schwöbel](https://www.semanticscholar.org/author/Pola-Schw%C3%B6bel/1631697931)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

A principled way to measure popularity bias is introduced by discussing existing metrics and proposing a novel metric that fulfills a series of desiderata, and it is found that the LLM recommender exhibits less popularity bias, even without any explicit mitigation.Expand

*   [1](https://www.semanticscholar.org/paper/b105ada7e385c041a27ba04a71628678b77701df#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/b105ada7e385c041a27ba04a71628678b77701df?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/b105ada7e385c041a27ba04a71628678b77701df)\[PDF\]

*   1 Excerpt

Save

[### Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations](https://www.semanticscholar.org/paper/Efficient-and-Responsible-Adaptation-of-Large-for-Kaur-Shah/d78203d0a70ff513776ae013e33b461b396a11bf)

[Kirandeep Kaur](https://www.semanticscholar.org/author/Kirandeep-Kaur/2299326811)[Chirag Shah](https://www.semanticscholar.org/author/Chirag-Shah/2299327433)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

A hybrid task allocation framework that utilizes the capabilities of both LLMs and traditional RSs is proposed that shows a significant reduction in weak users and improved robustness of RSs to sub-populations and overall performance without disproportionately escalating costs.Expand

*   [1](https://www.semanticscholar.org/paper/d78203d0a70ff513776ae013e33b461b396a11bf#citing-papers)
*   [Highly Influenced](https://www.semanticscholar.org/paper/d78203d0a70ff513776ae013e33b461b396a11bf?sort=is-influential#citing-papers)
    
[](https://www.semanticscholar.org/reader/d78203d0a70ff513776ae013e33b461b396a11bf)\[PDF\]

*   4 Excerpts

Save

[### Sequential recommendation by reprogramming pretrained transformer](https://www.semanticscholar.org/paper/Sequential-recommendation-by-reprogramming-Tang-Cui/47df06ebb6aebe64dc27ac6ccbcd2ae1e3917513)

[Min Tang](https://www.semanticscholar.org/author/Min-Tang/2321263286)[Shujie Cui](https://www.semanticscholar.org/author/Shujie-Cui/2320523193)[Zhe Jin](https://www.semanticscholar.org/author/Zhe-Jin/2185947941)[Shiuan-ni Liang](https://www.semanticscholar.org/author/Shiuan-ni-Liang/2299503261)[Chenliang Li](https://www.semanticscholar.org/author/Chenliang-Li/2136338739)[Lixin Zou](https://www.semanticscholar.org/author/Lixin-Zou/2240533680)

Computer Science, Engineering

[Information Processing & Management](https://www.semanticscholar.org/venue?name=Information%20Processing%20%26%20Management)

*   2025

Save

[### End-to-end Training for Recommendation with Language-based User Profiles](https://www.semanticscholar.org/paper/End-to-end-Training-for-Recommendation-with-User-Gao-Zhou/925893a71370237e537cb19ed8439fb31dfbd502)

[Zhaolin Gao](https://www.semanticscholar.org/author/Zhaolin-Gao/2298394607)[Joyce Zhou](https://www.semanticscholar.org/author/Joyce-Zhou/153823289)[Yijia Dai](https://www.semanticscholar.org/author/Yijia-Dai/2287934813)[Thorsten Joachims](https://www.semanticscholar.org/author/Thorsten-Joachims/2243190230)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

This paper introduces LangPTune, the first end-to-end learning method for training LLMs to produce language-based user profiles that optimize recommendation effectiveness and validates the relative interpretability of these language-based user profiles through user studies involving crowdworkers and GPT-4-based evaluations.Expand

[](https://www.semanticscholar.org/reader/925893a71370237e537cb19ed8439fb31dfbd502)\[PDF\]

*   1 Excerpt

Save

[### STAR: A Simple Training-free Approach for Recommendations using Large Language Models](https://www.semanticscholar.org/paper/STAR%3A-A-Simple-Training-free-Approach-for-using-Lee-Kraft/0545c6784b90415a3c02a823d1e89b526e772663)

[Dong-Ho Lee](https://www.semanticscholar.org/author/Dong-Ho-Lee/2327177362)[Adam Kraft](https://www.semanticscholar.org/author/Adam-Kraft/2314693663)+5 authors [Xinyang Yi](https://www.semanticscholar.org/author/Xinyang-Yi/2838461)

Computer Science

*   2024

TLDR

A Simple Training-free Approach for Recommendation (STAR), a framework that utilizes LLMs and can be applied to various recommendation tasks without the need for fine-tuning, highlighting the potential of LLMs in recommendation systems without extensive training or custom architectures.Expand

[](https://www.semanticscholar.org/reader/0545c6784b90415a3c02a823d1e89b526e772663)\[PDF\]

Save

[### Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond](https://www.semanticscholar.org/paper/Towards-Next-Generation-LLM-based-Recommender-A-and-Wang-Li/7e06467f1af79880640ff1f2790255741103e48f)

[Qi Wang](https://www.semanticscholar.org/author/Qi-Wang/2284061879)[Jindong Li](https://www.semanticscholar.org/author/Jindong-Li/2243469243)+7 authors [Chengqi Zhang](https://www.semanticscholar.org/author/Chengqi-Zhang/2283189862)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

A novel taxonomy is introduced that originates from the intrinsic essence of recommendation, delving into the application of large language model-based recommendation systems and their industrial implementation, and proposes a three-tier structure that more accurately reflects the developmental progression of recommendation systems from research to practical implementation.Expand

*   [1](https://www.semanticscholar.org/paper/7e06467f1af79880640ff1f2790255741103e48f#citing-papers)
[](https://www.semanticscholar.org/reader/7e06467f1af79880640ff1f2790255741103e48f)\[PDF\]

Save

[### See Where You Read with Eye Gaze Tracking and Large Language Model](https://www.semanticscholar.org/paper/See-Where-You-Read-with-Eye-Gaze-Tracking-and-Large-Yang-Yan/c4e77af36ba48c5b4417123597e4604c27a84c51)

[Sikai Yang](https://www.semanticscholar.org/author/Sikai-Yang/2323521464)[Gang Yan](https://www.semanticscholar.org/author/Gang-Yan/2323790397)

Computer Science

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

Two gaze error models are designed to enable both jump reading detection and relocation, and a reading tracking domain-specific line-gaze alignment opportunity is exploited to enable dynamic and frequent calibration of the gaze results.Expand

*   [PDF](https://www.semanticscholar.org/paper/c4e77af36ba48c5b4417123597e4604c27a84c51)
    

*   3 Excerpts

Save

[### Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation](https://www.semanticscholar.org/paper/Train-Once%2C-Deploy-Anywhere%3A-Matryoshka-Learning-Wang-Yue/020b09bd0757bf41a8b3c99300feb223404035ed)

[Yueqi Wang](https://www.semanticscholar.org/author/Yueqi-Wang/2253442064)[Zhenrui Yue](https://www.semanticscholar.org/author/Zhenrui-Yue/2028213158)[Huimin Zeng](https://www.semanticscholar.org/author/Huimin-Zeng/2113559163)[Dong Wang](https://www.semanticscholar.org/author/Dong-Wang/2254248851)[Julian McAuley](https://www.semanticscholar.org/author/Julian-McAuley/2303602937)

Computer Science

[Conference on Empirical Methods in Natural…](https://www.semanticscholar.org/venue?name=Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing)

*   2024

TLDR

This study focuses on sequential recommendation and introduces a lightweight framework called full-scale Matryoshka representation learning for multimodal recommendation (fMRLRec), which scales to different dimensions and only requires one-time training to produce multiple models tailored to various granularities.Expand

*   [1](https://www.semanticscholar.org/paper/020b09bd0757bf41a8b3c99300feb223404035ed#citing-papers)
[](https://www.semanticscholar.org/reader/020b09bd0757bf41a8b3c99300feb223404035ed)\[PDF\]

*   1 Excerpt

Save

[### All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era](https://www.semanticscholar.org/paper/All-Roads-Lead-to-Rome%3A-Unveiling-the-Trajectory-of-Chen-Dai/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a)

[Bo Chen](https://www.semanticscholar.org/author/Bo-Chen/2258709565)[Xinyi Dai](https://www.semanticscholar.org/author/Xinyi-Dai/2105646417)+9 authors [Hao Zhang](https://www.semanticscholar.org/author/Hao-Zhang/2298987734)

Computer Science, Linguistics

[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)

*   2024

TLDR

A comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation, and identifies two evolution paths of modern recommender systems -- via list-wise recommendation and conversational recommendation.Expand

*   [2](https://www.semanticscholar.org/paper/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a#citing-papers)
[](https://www.semanticscholar.org/reader/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a)\[PDF\]

*   1 Excerpt

Save

[### Language Representations Can be What Recommenders Need: Findings and Potentials](https://www.semanticscholar.org/paper/Language-Representations-Can-be-What-Recommenders-Sheng-Zhang/cb085ce72c1647d23da2514dc45e74fbf88f9224)

[Leheng Sheng](https://www.semanticscholar.org/author/Leheng-Sheng/2258731846)[An Zhang](https://www.semanticscholar.org/author/An-Zhang/2153659066)[Yi Zhang](https://www.semanticscholar.org/author/Yi-Zhang/2305963111)[Yuxin Chen](https://www.semanticscholar.org/author/Yuxin-Chen/2258784735)[Xiang Wang](https://www.semanticscholar.org/author/Xiang-Wang/2257436645)[Tat-Seng Chua](https://www.semanticscholar.org/author/Tat-Seng-Chua/2257036129)

Computer Science

*   2024

TLDR

These findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance and suggest the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Expand

[](https://www.semanticscholar.org/reader/cb085ce72c1647d23da2514dc45e74fbf88f9224)\[PDF\]

*   1 Excerpt

Save

...

1

2

3

...

Related Papers
--------------

Showing 1 through 3 of 0 Related Papers

*   [Figures and Tables](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#extracted)
*   [22 Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)
*   [Related Papers](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#related-papers)

Stay Connected With Semantic Scholar

Sign Up

What Is Semantic Scholar?
-------------------------

Semantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.

[Learn More](https://www.semanticscholar.org/about)

### About

[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)

### Product

[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)

### API

[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)

### Research

[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)

### Help

[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact

Proudly built by [Ai2 (opens in a new tab)](http://allenai.org/)

Collaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)

[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)

By clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)

ACCEPT & CONTINUE

## Metadata

```json
{
  "title": "[PDF] Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis | Semantic Scholar",
  "description": "A general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders, and proposes inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Recently, Large Language Models~(LLMs) such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the potential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study primarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize the input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be generalized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, \\ie task descriptions, user interest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in line with the existing literature. Then, we propose inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance. Based on our empirical analysis, we finally summarize promising directions to shed lights on future research.",
  "url": "https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db",
  "content": "\\[PDF\\] Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis | Semantic Scholar\n===============\n                                                              \n\n[Skip to search form](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#search-form)[Skip to main content](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#main-content)[Skip to account menu](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#account-menu)\n\n[](https://www.semanticscholar.org/)\n\nSearch 223,685,295 papers from all fields of science\n\nSearch\n\nSign InCreate Free Account\n\n*   Corpus ID: 266902921\n\nTapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis\n=======================================================================================================================\n\n@inproceedings{Xu2024TappingTP,\n  title={Tapping the Potential of Large Language Models as Recommender Systems: A Comprehensive Framework and Empirical Analysis},\n  author={Lanling Xu and Junjie Zhang and Bingqian Li and Jinpeng Wang and Sheng Chen and Wayne Xin Zhao and Ji-Rong Wen},\n  year={2024},\n  url={https://api.semanticscholar.org/CorpusID:266902921}\n}\n\n*   [Lanling Xu](https://www.semanticscholar.org/author/Lanling-Xu/2167464968), [Junjie Zhang](https://www.semanticscholar.org/author/Junjie-Zhang/2120518257), +4 authors [Ji-Rong Wen](https://www.semanticscholar.org/author/Ji-Rong-Wen/2274218622)\n*   Published 10 January 2024\n*   Computer Science\n\nTLDR\n\nA general framework for utilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders, and proposes inspiring research questions followed by detailed experiments on two public datasets, in order to systematically analyze the impact of different factors on performance.Expand\n\n[View PDF on arXiv](https://arxiv.org/pdf/2401.04997.pdf \"https://arxiv.org/pdf/2401.04997.pdf\")\n\nSave to LibrarySave\n\nCreate AlertAlert\n\nCite\n\nShare\n\n22 Citations\n\n[Highly Influential Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)[](https://www.semanticscholar.org/faq#influential-citations)\n\n2\n\n[Background Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)\n\n12\n\n[Methods Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)\n\n4\n\n[Results Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)\n\n1\n\n[View All](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)\n\nFigures and Tables from this paper\n----------------------------------\n\n*   [![Image 25: table 1](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/3-Table1-1.png) table 1](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/0)\n*   [![Image 26: figure 1](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/9-Figure1-1.png) figure 1](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/1)\n*   [![Image 27: table 2](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/11-Table2-1.png) table 2](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/2)\n*   [![Image 28: figure 2](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/13-Figure2-1.png) figure 2](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/3)\n*   [![Image 29: table 3](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/12-Table3-1.png) table 3](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/4)\n*   [![Image 30: figure 3](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/21-Figure3-1.png) figure 3](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/5)\n*   [![Image 31: table 4](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/20-Table4-1.png) table 4](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/6)\n*   [![Image 32: figure 4](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/23-Figure4-1.png) figure 4](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/7)\n*   [![Image 33: figure 5](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/24-Figure5-1.png) figure 5](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/8)\n*   [![Image 34: table 6](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/25-Table6-1.png) table 6](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/9)\n*   [![Image 35: figure 6](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/26-Figure6-1.png) figure 6](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/10)\n*   [![Image 36: table 7](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/25-Table7-1.png) table 7](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/11)\n*   [![Image 37: figure 7](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/26-Figure7-1.png) figure 7](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/12)\n*   [![Image 38: figure 8](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/28-Figure8-1.png) figure 8](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/13)\n*   [![Image 39: table 8](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/30-Table8-1.png) table 8](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/14)\n*   [![Image 40: figure 9](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/33-Figure9-1.png) figure 9](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/15)\n*   [![Image 41: table 9](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/35-Table9-1.png) table 9](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/16)\n*   [![Image 42: table 10](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/35-Table10-1.png) table 10](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/17)\n*   [![Image 43: figure 10](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/36-Figure10-1.png) figure 10](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/18)\n*   [![Image 44: table 11](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/38-Table11-1.png) table 11](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/19)\n*   [![Image 45: figure 11](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/41-Figure11-1.png) figure 11](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/20)\n*   [![Image 46: table 12](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/38-Table12-1.png) table 12](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/21)\n*   [![Image 47: table 13](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/42-Table13-1.png) table 13](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/22)\n*   [![Image 48: table 14](https://figures.semanticscholar.org/7ac50c0a4ca1f306d731392bfb16446db11109a1/42-Table14-1.png) table 14](https://www.semanticscholar.org/paper/Tapping-the-Potential-of-Large-Language-Models-as-A-Xu-Zhang/7ac50c0a4ca1f306d731392bfb16446db11109a1/figure/23)\n\nView All 24 Figures & Tables\n\n22 Citations\n------------\n\nCitation Type\n\nHas PDF\n\nAuthor\n\nMore Filters\n\nMore Filters\n\nFilters\n\n[### Large Language Models as Recommender Systems: A Study of Popularity Bias](https://www.semanticscholar.org/paper/Large-Language-Models-as-Recommender-Systems%3A-A-of-Lichtenberg-Buchholz/b105ada7e385c041a27ba04a71628678b77701df)\n\n[Jan Malte Lichtenberg](https://www.semanticscholar.org/author/Jan-Malte-Lichtenberg/2060219456)[Alexander Buchholz](https://www.semanticscholar.org/author/Alexander-Buchholz/2237805687)[Pola Schwöbel](https://www.semanticscholar.org/author/Pola-Schw%C3%B6bel/1631697931)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nA principled way to measure popularity bias is introduced by discussing existing metrics and proposing a novel metric that fulfills a series of desiderata, and it is found that the LLM recommender exhibits less popularity bias, even without any explicit mitigation.Expand\n\n*   [1](https://www.semanticscholar.org/paper/b105ada7e385c041a27ba04a71628678b77701df#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/b105ada7e385c041a27ba04a71628678b77701df?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/b105ada7e385c041a27ba04a71628678b77701df)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Efficient and Responsible Adaptation of Large Language Models for Robust Top-k Recommendations](https://www.semanticscholar.org/paper/Efficient-and-Responsible-Adaptation-of-Large-for-Kaur-Shah/d78203d0a70ff513776ae013e33b461b396a11bf)\n\n[Kirandeep Kaur](https://www.semanticscholar.org/author/Kirandeep-Kaur/2299326811)[Chirag Shah](https://www.semanticscholar.org/author/Chirag-Shah/2299327433)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nA hybrid task allocation framework that utilizes the capabilities of both LLMs and traditional RSs is proposed that shows a significant reduction in weak users and improved robustness of RSs to sub-populations and overall performance without disproportionately escalating costs.Expand\n\n*   [1](https://www.semanticscholar.org/paper/d78203d0a70ff513776ae013e33b461b396a11bf#citing-papers)\n*   [Highly Influenced](https://www.semanticscholar.org/paper/d78203d0a70ff513776ae013e33b461b396a11bf?sort=is-influential#citing-papers)\n    \n[](https://www.semanticscholar.org/reader/d78203d0a70ff513776ae013e33b461b396a11bf)\\[PDF\\]\n\n*   4 Excerpts\n\nSave\n\n[### Sequential recommendation by reprogramming pretrained transformer](https://www.semanticscholar.org/paper/Sequential-recommendation-by-reprogramming-Tang-Cui/47df06ebb6aebe64dc27ac6ccbcd2ae1e3917513)\n\n[Min Tang](https://www.semanticscholar.org/author/Min-Tang/2321263286)[Shujie Cui](https://www.semanticscholar.org/author/Shujie-Cui/2320523193)[Zhe Jin](https://www.semanticscholar.org/author/Zhe-Jin/2185947941)[Shiuan-ni Liang](https://www.semanticscholar.org/author/Shiuan-ni-Liang/2299503261)[Chenliang Li](https://www.semanticscholar.org/author/Chenliang-Li/2136338739)[Lixin Zou](https://www.semanticscholar.org/author/Lixin-Zou/2240533680)\n\nComputer Science, Engineering\n\n[Information Processing & Management](https://www.semanticscholar.org/venue?name=Information%20Processing%20%26%20Management)\n\n*   2025\n\nSave\n\n[### End-to-end Training for Recommendation with Language-based User Profiles](https://www.semanticscholar.org/paper/End-to-end-Training-for-Recommendation-with-User-Gao-Zhou/925893a71370237e537cb19ed8439fb31dfbd502)\n\n[Zhaolin Gao](https://www.semanticscholar.org/author/Zhaolin-Gao/2298394607)[Joyce Zhou](https://www.semanticscholar.org/author/Joyce-Zhou/153823289)[Yijia Dai](https://www.semanticscholar.org/author/Yijia-Dai/2287934813)[Thorsten Joachims](https://www.semanticscholar.org/author/Thorsten-Joachims/2243190230)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nThis paper introduces LangPTune, the first end-to-end learning method for training LLMs to produce language-based user profiles that optimize recommendation effectiveness and validates the relative interpretability of these language-based user profiles through user studies involving crowdworkers and GPT-4-based evaluations.Expand\n\n[](https://www.semanticscholar.org/reader/925893a71370237e537cb19ed8439fb31dfbd502)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### STAR: A Simple Training-free Approach for Recommendations using Large Language Models](https://www.semanticscholar.org/paper/STAR%3A-A-Simple-Training-free-Approach-for-using-Lee-Kraft/0545c6784b90415a3c02a823d1e89b526e772663)\n\n[Dong-Ho Lee](https://www.semanticscholar.org/author/Dong-Ho-Lee/2327177362)[Adam Kraft](https://www.semanticscholar.org/author/Adam-Kraft/2314693663)+5 authors [Xinyang Yi](https://www.semanticscholar.org/author/Xinyang-Yi/2838461)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nA Simple Training-free Approach for Recommendation (STAR), a framework that utilizes LLMs and can be applied to various recommendation tasks without the need for fine-tuning, highlighting the potential of LLMs in recommendation systems without extensive training or custom architectures.Expand\n\n[](https://www.semanticscholar.org/reader/0545c6784b90415a3c02a823d1e89b526e772663)\\[PDF\\]\n\nSave\n\n[### Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond](https://www.semanticscholar.org/paper/Towards-Next-Generation-LLM-based-Recommender-A-and-Wang-Li/7e06467f1af79880640ff1f2790255741103e48f)\n\n[Qi Wang](https://www.semanticscholar.org/author/Qi-Wang/2284061879)[Jindong Li](https://www.semanticscholar.org/author/Jindong-Li/2243469243)+7 authors [Chengqi Zhang](https://www.semanticscholar.org/author/Chengqi-Zhang/2283189862)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nA novel taxonomy is introduced that originates from the intrinsic essence of recommendation, delving into the application of large language model-based recommendation systems and their industrial implementation, and proposes a three-tier structure that more accurately reflects the developmental progression of recommendation systems from research to practical implementation.Expand\n\n*   [1](https://www.semanticscholar.org/paper/7e06467f1af79880640ff1f2790255741103e48f#citing-papers)\n[](https://www.semanticscholar.org/reader/7e06467f1af79880640ff1f2790255741103e48f)\\[PDF\\]\n\nSave\n\n[### See Where You Read with Eye Gaze Tracking and Large Language Model](https://www.semanticscholar.org/paper/See-Where-You-Read-with-Eye-Gaze-Tracking-and-Large-Yang-Yan/c4e77af36ba48c5b4417123597e4604c27a84c51)\n\n[Sikai Yang](https://www.semanticscholar.org/author/Sikai-Yang/2323521464)[Gang Yan](https://www.semanticscholar.org/author/Gang-Yan/2323790397)\n\nComputer Science\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nTwo gaze error models are designed to enable both jump reading detection and relocation, and a reading tracking domain-specific line-gaze alignment opportunity is exploited to enable dynamic and frequent calibration of the gaze results.Expand\n\n*   [PDF](https://www.semanticscholar.org/paper/c4e77af36ba48c5b4417123597e4604c27a84c51)\n    \n\n*   3 Excerpts\n\nSave\n\n[### Train Once, Deploy Anywhere: Matryoshka Representation Learning for Multimodal Recommendation](https://www.semanticscholar.org/paper/Train-Once%2C-Deploy-Anywhere%3A-Matryoshka-Learning-Wang-Yue/020b09bd0757bf41a8b3c99300feb223404035ed)\n\n[Yueqi Wang](https://www.semanticscholar.org/author/Yueqi-Wang/2253442064)[Zhenrui Yue](https://www.semanticscholar.org/author/Zhenrui-Yue/2028213158)[Huimin Zeng](https://www.semanticscholar.org/author/Huimin-Zeng/2113559163)[Dong Wang](https://www.semanticscholar.org/author/Dong-Wang/2254248851)[Julian McAuley](https://www.semanticscholar.org/author/Julian-McAuley/2303602937)\n\nComputer Science\n\n[Conference on Empirical Methods in Natural…](https://www.semanticscholar.org/venue?name=Conference%20on%20Empirical%20Methods%20in%20Natural%20Language%20Processing)\n\n*   2024\n\nTLDR\n\nThis study focuses on sequential recommendation and introduces a lightweight framework called full-scale Matryoshka representation learning for multimodal recommendation (fMRLRec), which scales to different dimensions and only requires one-time training to produce multiple models tailored to various granularities.Expand\n\n*   [1](https://www.semanticscholar.org/paper/020b09bd0757bf41a8b3c99300feb223404035ed#citing-papers)\n[](https://www.semanticscholar.org/reader/020b09bd0757bf41a8b3c99300feb223404035ed)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era](https://www.semanticscholar.org/paper/All-Roads-Lead-to-Rome%3A-Unveiling-the-Trajectory-of-Chen-Dai/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a)\n\n[Bo Chen](https://www.semanticscholar.org/author/Bo-Chen/2258709565)[Xinyi Dai](https://www.semanticscholar.org/author/Xinyi-Dai/2105646417)+9 authors [Hao Zhang](https://www.semanticscholar.org/author/Hao-Zhang/2298987734)\n\nComputer Science, Linguistics\n\n[arXiv.org](https://www.semanticscholar.org/venue?name=arXiv.org)\n\n*   2024\n\nTLDR\n\nA comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation, and identifies two evolution paths of modern recommender systems -- via list-wise recommendation and conversational recommendation.Expand\n\n*   [2](https://www.semanticscholar.org/paper/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a#citing-papers)\n[](https://www.semanticscholar.org/reader/ae1cd3e1db5268dd77d10924717b87c7eb8b7c7a)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n[### Language Representations Can be What Recommenders Need: Findings and Potentials](https://www.semanticscholar.org/paper/Language-Representations-Can-be-What-Recommenders-Sheng-Zhang/cb085ce72c1647d23da2514dc45e74fbf88f9224)\n\n[Leheng Sheng](https://www.semanticscholar.org/author/Leheng-Sheng/2258731846)[An Zhang](https://www.semanticscholar.org/author/An-Zhang/2153659066)[Yi Zhang](https://www.semanticscholar.org/author/Yi-Zhang/2305963111)[Yuxin Chen](https://www.semanticscholar.org/author/Yuxin-Chen/2258784735)[Xiang Wang](https://www.semanticscholar.org/author/Xiang-Wang/2257436645)[Tat-Seng Chua](https://www.semanticscholar.org/author/Tat-Seng-Chua/2257036129)\n\nComputer Science\n\n*   2024\n\nTLDR\n\nThese findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance and suggest the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.Expand\n\n[](https://www.semanticscholar.org/reader/cb085ce72c1647d23da2514dc45e74fbf88f9224)\\[PDF\\]\n\n*   1 Excerpt\n\nSave\n\n...\n\n1\n\n2\n\n3\n\n...\n\nRelated Papers\n--------------\n\nShowing 1 through 3 of 0 Related Papers\n\n*   [Figures and Tables](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#extracted)\n*   [22 Citations](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#citing-papers)\n*   [Related Papers](https://www.semanticscholar.org/paper/Prompting-Large-Language-Models-for-Recommender-A-Xu-Zhang/156af81de93f840df39c0973ef1343629427a7db#related-papers)\n\nStay Connected With Semantic Scholar\n\nSign Up\n\nWhat Is Semantic Scholar?\n-------------------------\n\nSemantic Scholar is a free, AI-powered research tool for scientific literature, based at Ai2.\n\n[Learn More](https://www.semanticscholar.org/about)\n\n### About\n\n[About Us](https://www.semanticscholar.org/about)[Meet the Team](https://www.semanticscholar.org/about/team)[Publishers](https://www.semanticscholar.org/about/publishers)[Blog (opens in a new tab)](https://medium.com/ai2-blog/semantic-scholar/home)[Ai2 Careers (opens in a new tab)](https://allenai.org/careers?team=semantic+scholar#current-openings)\n\n### Product\n\n[Product Overview](https://www.semanticscholar.org/product)[Semantic Reader](https://www.semanticscholar.org/product/semantic-reader)[Scholar's Hub](https://www.semanticscholar.org/product/scholars-hub)[Beta Program](https://www.semanticscholar.org/product/beta-program)[Release Notes](https://www.semanticscholar.org/product/release-notes)\n\n### API\n\n[API Overview](https://www.semanticscholar.org/product/api)[API Tutorials](https://www.semanticscholar.org/product/api%2Ftutorial)[API Documentation (opens in a new tab)](https://api.semanticscholar.org/api-docs/)[API Gallery](https://www.semanticscholar.org/product/api%2Fgallery)\n\n### Research\n\n[Publications](https://www.semanticscholar.org/research/publications)[Researchers](https://www.semanticscholar.org/research/research-team)[Research Careers](https://www.semanticscholar.org/research/careers)[Prototypes](https://www.semanticscholar.org/research/prototypes)[Resources](https://www.semanticscholar.org/resources)\n\n### Help\n\n[FAQ](https://www.semanticscholar.org/faq)[Librarians](https://www.semanticscholar.org/about/librarians)[Tutorials](https://www.semanticscholar.org/product/tutorials)Contact\n\nProudly built by [Ai2 (opens in a new tab)](http://allenai.org/)\n\nCollaborators & Attributions •[Terms of Service (opens in a new tab)](https://allenai.org/terms)•[Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html)•[API License Agreement](https://www.semanticscholar.org/product/api/license)\n\n[The Allen Institute for AI (opens in a new tab)](http://allenai.org/)\n\nBy clicking accept or continuing to use the site, you agree to the terms outlined in our [Privacy Policy (opens in a new tab)](https://allenai.org/privacy-policy.html), [Terms of Service (opens in a new tab)](https://allenai.org/terms), and [Dataset License (opens in a new tab)](http://api.semanticscholar.org/corpus/legal)\n\nACCEPT & CONTINUE",
  "usage": {
    "tokens": 7752
  }
}
```
