---
title: Exa | Web API for AI
description: Find exactly what you are looking for
url: https://exa.ai/search
timestamp: 2025-01-20T16:15:33.984Z
domain: exa.ai
path: search
---

# Exa | Web API for AI


Find exactly what you are looking for


## Content

Exa Search
===============

[](https://exa.ai/search/back)

[We're hiring](https://exa.ai/careers)

[](https://exa.ai/search/back)

[](https://discord.gg/HCShtBqbfV)[](https://twitter.com/ExaAILabs)

[Go to homepage](https://exa.ai/search/back)

[](https://exa.ai/search/back)

[We're hiring](https://exa.ai/careers)

[](https://exa.ai/search/back)

[](https://discord.gg/HCShtBqbfV)[](https://twitter.com/ExaAILabs)

[Go to homepage](https://exa.ai/search/back)

The web, **organized**
======================

Exa search uses embeddings to understand meaning. [Learn more](https://docs.exa.ai/reference/prompting-guide)

Auto search

Autoprompt

Search

POPULAR SEARCHES

[a short article about the early days of Google](https://exa.ai/search?autopromptString=Here%27s%20a%20short%20article%20about%20the%20early%20days%20of%20Google%3A&q=a%20short%20article%20about%20the%20early%20days%20of%20Google&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22neural%22%7D)[Start ups working on genetic sequencing](https://exa.ai/search?autopromptString=Here%20is%20a%20start-up%20working%20on%20genetic%20sequencing%3A&q=Start%20ups%20working%20on%20genetic%20sequencing&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)[Similar to https://waitbutwhy.com](https://exa.ai/search?q=https%3A%2F%2Fwaitbutwhy.com&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%2C%22excludeDomains%22%3A%5B%22waitbutwhy.com%22%5D%7D)[Samsung earnings report](https://exa.ai/search?autopromptString=Here%20is%20the%20Samsung%20earnings%20report%3A%20(pdf%3A%20&q=Samsung%20earnings%20report&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)[Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.](https://exa.ai/search?autopromptString=Here%20is%20a%20paper%20with%20similar%20ideas%20to%20the%20dominant%20sequence%20transduction%20models%3A&q=Abstract%3A%20The%20dominant%20sequence%20transduction%20models%20are%20based%20on%20complex%20recurrent%20or%20convolutional%20neural%20networks%20in%20an%20encoder-decoder%20configuration.%20The%20best%20performing%20models%20also%20connect%20the%20encoder%20and%20decoder%20through%20an%20attention%20mechanism.%20We%20propose%20a%20new%20simple%20network%20architecture%2C%20the%20Transformer%2C%20based%20solely%20on%20attention%20mechanisms%2C%20dispensing%20with%20recurrence%20and%20convolutions%20entirely.%20Experiments%20on%20two%20machine%20translation%20tasks%20show%20these%20models%20to%20be%20superior%20in%20quality%20while%20being%20more%20parallelizable%20and%20requiring%20significantly%20less%20time%20to%20train.%20Our%20model%20achieves%2028.4%20BLEU%20on%20the%20WMT%202014%20English-to-German%20translation%20task%2C%20improving%20over%20the%20existing%20best%20results%2C%20including%20ensembles%20by%20over%202%20BLEU.%20On%20the%20WMT%202014%20English-to-French%20translation%20task%2C%20our%20model%20establishes%20a%20new%20single-model%20state-of-the-art%20BLEU%20score%20of%2041.8%20after%20training%20for%203.5%20days%20on%20eight%20GPUs%2C%20a%20small%20fraction%20of%20the%20training%20costs%20of%20the%20best%20models%20from%20the%20literature.%20We%20show%20that%20the%20Transformer%20generalizes%20well%20to%20other%20tasks%20by%20applying%20it%20successfully%20to%20English%20constituency%20parsing%20both%20with%20large%20and%20limited%20training%20data.%0A%0AHere%20is%20a%20paper%20that%20has%20similar%20ideas%3A&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)

All resultsAny timeDomain filterPhrase filter20 Results

Category

*   All
*   Company
*   News
*   Paper
*   Linkedin profile
*   Financial report
Show more

*   All
*   Company
*   News
*   Paper
*   Linkedin profile
*   Financial report
*   Tweet
*   Blog post
*   PDF
*   Github
*   Personal site
*   

Publish dateAny time

*   Past day
*   Past week
*   Past month
*   Past year
*   Any time
*   After
    

Domain filter

Include

Exclude

Enter domain...

Phrase filterNew

Include phrase

Exclude phrase

Number of results20

*   10 Results
*   20 Results
*   30 Results
*

## Metadata

```json
{
  "title": "Exa | Web API for AI",
  "description": "Find exactly what you are looking for",
  "url": "https://exa.ai/search",
  "content": "Exa Search\n===============\n\n[](https://exa.ai/search/back)\n\n[We're hiring](https://exa.ai/careers)\n\n[](https://exa.ai/search/back)\n\n[](https://discord.gg/HCShtBqbfV)[](https://twitter.com/ExaAILabs)\n\n[Go to homepage](https://exa.ai/search/back)\n\n[](https://exa.ai/search/back)\n\n[We're hiring](https://exa.ai/careers)\n\n[](https://exa.ai/search/back)\n\n[](https://discord.gg/HCShtBqbfV)[](https://twitter.com/ExaAILabs)\n\n[Go to homepage](https://exa.ai/search/back)\n\nThe web, **organized**\n======================\n\nExa search uses embeddings to understand meaning. [Learn more](https://docs.exa.ai/reference/prompting-guide)\n\nAuto search\n\nAutoprompt\n\nSearch\n\nPOPULAR SEARCHES\n\n[a short article about the early days of Google](https://exa.ai/search?autopromptString=Here%27s%20a%20short%20article%20about%20the%20early%20days%20of%20Google%3A&q=a%20short%20article%20about%20the%20early%20days%20of%20Google&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22neural%22%7D)[Start ups working on genetic sequencing](https://exa.ai/search?autopromptString=Here%20is%20a%20start-up%20working%20on%20genetic%20sequencing%3A&q=Start%20ups%20working%20on%20genetic%20sequencing&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)[Similar to https://waitbutwhy.com](https://exa.ai/search?q=https%3A%2F%2Fwaitbutwhy.com&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%2C%22excludeDomains%22%3A%5B%22waitbutwhy.com%22%5D%7D)[Samsung earnings report](https://exa.ai/search?autopromptString=Here%20is%20the%20Samsung%20earnings%20report%3A%20(pdf%3A%20&q=Samsung%20earnings%20report&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)[Abstract: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.](https://exa.ai/search?autopromptString=Here%20is%20a%20paper%20with%20similar%20ideas%20to%20the%20dominant%20sequence%20transduction%20models%3A&q=Abstract%3A%20The%20dominant%20sequence%20transduction%20models%20are%20based%20on%20complex%20recurrent%20or%20convolutional%20neural%20networks%20in%20an%20encoder-decoder%20configuration.%20The%20best%20performing%20models%20also%20connect%20the%20encoder%20and%20decoder%20through%20an%20attention%20mechanism.%20We%20propose%20a%20new%20simple%20network%20architecture%2C%20the%20Transformer%2C%20based%20solely%20on%20attention%20mechanisms%2C%20dispensing%20with%20recurrence%20and%20convolutions%20entirely.%20Experiments%20on%20two%20machine%20translation%20tasks%20show%20these%20models%20to%20be%20superior%20in%20quality%20while%20being%20more%20parallelizable%20and%20requiring%20significantly%20less%20time%20to%20train.%20Our%20model%20achieves%2028.4%20BLEU%20on%20the%20WMT%202014%20English-to-German%20translation%20task%2C%20improving%20over%20the%20existing%20best%20results%2C%20including%20ensembles%20by%20over%202%20BLEU.%20On%20the%20WMT%202014%20English-to-French%20translation%20task%2C%20our%20model%20establishes%20a%20new%20single-model%20state-of-the-art%20BLEU%20score%20of%2041.8%20after%20training%20for%203.5%20days%20on%20eight%20GPUs%2C%20a%20small%20fraction%20of%20the%20training%20costs%20of%20the%20best%20models%20from%20the%20literature.%20We%20show%20that%20the%20Transformer%20generalizes%20well%20to%20other%20tasks%20by%20applying%20it%20successfully%20to%20English%20constituency%20parsing%20both%20with%20large%20and%20limited%20training%20data.%0A%0AHere%20is%20a%20paper%20that%20has%20similar%20ideas%3A&filters=%7B%22numResults%22%3A20%2C%22domainFilterType%22%3A%22include%22%2C%22type%22%3A%22auto%22%7D)\n\nAll resultsAny timeDomain filterPhrase filter20 Results\n\nCategory\n\n*   All\n*   Company\n*   News\n*   Paper\n*   Linkedin profile\n*   Financial report\nShow more\n\n*   All\n*   Company\n*   News\n*   Paper\n*   Linkedin profile\n*   Financial report\n*   Tweet\n*   Blog post\n*   PDF\n*   Github\n*   Personal site\n*   \n\nPublish dateAny time\n\n*   Past day\n*   Past week\n*   Past month\n*   Past year\n*   Any time\n*   After\n    \n\nDomain filter\n\nInclude\n\nExclude\n\nEnter domain...\n\nPhrase filterNew\n\nInclude phrase\n\nExclude phrase\n\nNumber of results20\n\n*   10 Results\n*   20 Results\n*   30 Results\n*",
  "usage": {
    "tokens": 1774
  }
}
```
