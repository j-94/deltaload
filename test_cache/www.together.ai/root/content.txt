Together AI – The AI Acceleration Cloud - Fast Inference, Fine-Tuning & Training
===============
        

This website uses cookies to anonymously analyze website traffic using Google Analytics.

[Accept](https://www.together.ai/#)[Decline](https://www.together.ai/#)

[Together GPU Clusters featuring NVIDIA GB200, H200, and H100. Request your cluster today](https://www.together.ai/forms/gpu-cluster-requests?utm_source=website&utm_medium=banner)

[](https://www.together.ai/)

[Products](https://www.together.ai/products)

[Together Inference](https://www.together.ai/products#inference)[Together Fine-tuning](https://www.together.ai/products#fine-tuning)[Together Custom Models](https://www.together.ai/products#custom-models)[Together GPU Clusters](https://www.together.ai/gpu-clusters)[Model Library](https://www.together.ai/models)

For Business

[Enterprise](https://www.together.ai/enterprise)[Customer stories](https://www.together.ai/solutions#customer-stories)[Why open-source](https://www.together.ai/solutions#why-open-source)[Industries & use cases](https://www.together.ai/solutions#use-cases)

For Developers

[Docs](https://docs.together.ai/docs/introduction)[Model Library](https://www.together.ai/models)[Example Apps](https://www.together.ai/demos)[Cookbooks](https://github.com/togethercomputer/together-cookbook)[Playground](https://api.together.ai/playground)

[Pricing](https://www.together.ai/pricing)[Research](https://www.together.ai/research)

[Company](https://www.together.ai/about)

[Blog](https://www.together.ai/blog)[Values](https://www.together.ai/about#values)[Careers](https://www.together.ai/about#careers)[Team](https://www.together.ai/about#team)

[Docs](https://docs.together.ai/?utm_source=website&utm_medium=top-nav)[Contact](https://www.together.ai/contact)[Get Started](http://api.together.ai/)

The AI Acceleration AccelerationCloud
=====================================

Train, fine-tune-and run inference on AI models blazing fast, at low cost, and at production scale.

[Start building now](https://api.together.ai/)[Contact sales](https://www.together.ai/forms/contact-sales?utm_source=website&utm_medium=internal&utm_campaign=home-page)

Trusted by

![Image 129](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/678856af51e3535c6d87e0c4_image%20(4).png)

![Image 130](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/669699c9db7ea778827647e6_zoom.svg)

![Image 131](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fb1b8fb65b656b70f67683_salesforce.png)

![Image 132](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67859b39204d46be77360f36_cognition.png)

![Image 133](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/669699f7f90c23bfb9bc981a_quora.svg)

![Image 134](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67885796eb1dbda7d557af65_sk%20telecom.png)

![Image 135](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6691712f335833eaee4ee7d7_zomato.png)

![Image 136](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/669699e722ecf8bfac9ca0ce_wp.svg)

![Image 137](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6691713f9bf0f7b095c4187a_duck-duck-go.png)

![Image 138](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/663e642284752bf9f69788e2_logo_pika.svg)

![Image 139](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fb1baf89363300511525e3_jasperai.png)

![Image 140](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67859b4f2717224286ad2dfd_hedra.png)

![Image 141](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fb1bbc44c37f50aa7c674f_krea.png)

![Image 142](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66969a3fd3c20dedcdcfd159_w-b.svg)

![Image 143](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67859adf117aafb90214bbc0_nous.png)

![Image 144](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66969a1516fb6a7b2cb631d6_zoho.svg)

![Image 145](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66969a664fcb862b651137ce_snorkel.svg)

![Image 146](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/663e644edb3a20d534421b06_latitude.svg)

![Image 147](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/663e650b724784de1c6840dd_nexusflow.svg)

![Image 148](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/663e646d5b0fb699cc853e20_iamplus.png)

![Image 149](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/663e64e6e0044fbd97b96964_upstage.png)

![Image 150](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6696a1566d5329f2589ca83c_leonardo.svg)

![Image 151](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6696a17951dada396587f1bb_ai2.svg)

![Image 152](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6696a1de8bbc116d3e73cb12_medal.png)

200+ generative AI models
-------------------------

Build with open-source and specialized multimodal models for chat, images, code, and more. Migrate from closed models with OpenAI-compatible APIs.

[All](https://www.together.ai/#)

All

Chat

Image

Vision

Language

Code

Embeddings

Rerank

Thank you! Your submission has been received!

Oops! Something went wrong while submitting the form.

[Try now](http://api.together.ai/)

together.ai

[Chat ![Image 153](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.3 70B Instruct Turbo Free --------------------------------- Free endpoint to try this 70B multilingual LLM optimized for dialogue, excelling in benchmarks and surpassing many chat models. TRY THIS MODEL Free](https://api.together.ai/models/meta-llama/Llama-3.3-70B-Instruct-Turbo-Free)

[Chat ![Image 154](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.3 70B ------------- The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. TRY THIS MODEL](https://api.together.ai/playground/chat/meta-llama/Llama-3.3-70B-Instruct-Turbo)

[Chat ![Image 155](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f4199145ee71d1e691cde8_google-logo.webp) Gemma-2 Instruct (27B) ---------------------- Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. TRY THIS MODEL](https://api.together.ai/playground/chat/google/gemma-2-27b-it)

[Chat ![Image 156](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen 2.5 72B ------------ Powerful decoder-only models available in 7B and 72B variants, developed by Alibaba Cloud's Qwen team for advanced language processing. TRY THIS MODEL](https://api.together.xyz/playground/chat/Qwen/Qwen2.5-72B-Instruct-Turbo)

[Chat ![Image 157](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6733a0e28da9cb30a106ce4d_nvidia-logo.png) Llama 3.1 Nemotron 70B Instruct ------------------------------- This LLM is customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. TRY THIS MODEL](https://api.together.ai/playground/chat/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF)

[Chat ![Image 158](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a324f1d713df2cbfbf4_deepseek-logo.webp) DeepSeek-V3 ----------- DeepSeek's latest open Mixture-of-Experts model challenging top AI models at much lower cost. TRY THIS MODEL](https://api.together.xyz/playground/chat/deepseek-ai/DeepSeek-V3)

[Language ![Image 159](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f4197d83b94e540f009dc3_mistral-logo.webp) Mixtral-8x22B ------------- The Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. TRY THIS MODEL](https://api.together.xyz/playground/language/mistralai/Mixtral-8x22B)

[Chat ![Image 160](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a22506fc209003d5722_databricks-logo.webp) DBRX-Instruct ------------- DBRX Instruct is a mixture-of-experts (MoE) large language model trained from scratch by Databricks. DBRX Instruct specializes in few-turn. TRY THIS MODEL](https://api.together.xyz/playground/chat/databricks/dbrx-instruct)

[Chat ![Image 161](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a324f1d713df2cbfbf4_deepseek-logo.webp) Deepseek-67B ------------ Trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. TRY THIS MODEL](https://api.together.xyz/playground/chat/deepseek-ai/deepseek-llm-67b-chat)

[Chat ![Image 162](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a14044776e2666a1746_snowflake-logo.webp) Arctic-Instruct --------------- Arctic is a dense-MoE Hybrid transformer architecture pre-trained from scratch by the Snowflake AI Research Team. TRY THIS MODEL](https://api.together.xyz/playground/chat/Snowflake/snowflake-arctic-instruct)

[Chat ![Image 163](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f419f202c64707fcabd6ac_nous-logo.webp) Striped Hyena Nous ------------------ A hybrid architecture composed of multi-head, grouped-query attention and gated convolutions arranged in Hyena blocks, different from traditional decoder-only Transformers TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/StripedHyena-Nous-7B)

[Vision ![Image 164](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.2 11B Free ------------------ Free endpoint to try Llama 3.2 11B. TRY THIS MODEL Free](https://api.together.ai/playground/chat/meta-llama/Llama-Vision-Free)

[Vision ![Image 165](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.2 90B ------------- The Llama 3.2-Vision collection features multimodal LLMs (11B and 90B) optimized for visual recognition, image reasoning, captioning, and answering image-related questions. TRY THIS MODEL](https://api.together.ai/playground/chat/meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo)

[Vision ![Image 166](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen2-VL-72B-Instruct --------------------- A powerful OSS vision model by Alibaba that combines advanced vision capabilities with instruction-tuned language understanding for sophisticated visual reasoning tasks. TRY THIS MODEL](https://api.together.xyz/playground/chat/Qwen/Qwen2-VL-72B-Instruct)

[Chat ![Image 167](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.1 8B ------------ The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes, that outperform many available open source and closed chat models on common industry benchmarks. TRY THIS MODEL](https://api.together.xyz/playground/chat/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo)

[Vision ![Image 168](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.2 11B ------------- The Llama 3.2-Vision collection features multimodal LLMs (11B and 90B) optimized for visual recognition, image reasoning, captioning, and answering image-related questions. TRY THIS MODEL](https://api.together.xyz/playground/chat/meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo)

[Image ![Image 169](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 \[schnell\] ------------------ Fastest available endpoint for the SOTA open-source image generation model by Black Forest Labs. TRY THIS MODEL](https://api.together.xyz/playground/image/black-forest-labs/FLUX.1-schnell)

[Chat ![Image 170](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.1 405B -------------- The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes, that outperform many available open source and closed chat models on common industry benchmarks. TRY THIS MODEL](https://api.together.ai/playground/chat/meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo)

[Chat ![Image 171](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen QwQ 32B Preview -------------------- Experimental research model by Alibaba's Qwen team focused on enhancing AI reasoning capabilities. TRY THIS MODEL](https://api.together.ai/playground/chat/Qwen/QwQ-32B-Preview)

[Embeddings ![Image 172](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a4c89fc8c798a5e655e_uae-logo.png) UAE-Large v1 ------------ An universal English sentence embedding model by WhereIsAI. Its embedding dimension is 1024, it takes up to 512 context length. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Chat ![Image 173](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen 2 ------ A transformer-based decoder-only language model pre-trained on a large amount of data. In comparison with the previously released Qwen TRY THIS MODEL](https://api.together.xyz/playground/chat/Qwen/Qwen2-72B-Instruct)

[Language ![Image 174](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a58774fe61629007a0f_ai-yi-logo.png) 01-AI Yi -------- The Yi series models are large language models trained from scratch by developers at 01.AI TRY THIS MODEL](https://api.together.xyz/playground/language/zero-one-ai/Yi-34B)

[Code ![Image 175](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen 2.5 Coder 32B Instruct --------------------------- SOTA code LLM with advanced code generation, reasoning, fixing, and support for up to 128K tokens. TRY THIS MODEL](https://api.together.ai/playground/chat/Qwen/Qwen2.5-Coder-32B-Instruct)

[Chat ![Image 176](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67659ef6b5672a49cb026f39_SCB10X.webp) Typhoon 1.5X 70B-awq -------------------- Thai language 70B instruct model rivaling GPT-4-0612; optimized for RAG, constrained generation, and reasoning tasks. TRY THIS MODEL](https://api.together.ai/playground/chat/scb10x/scb10x-llama3-typhoon-v1-5x-4f316)

[Chat ![Image 177](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f4197d83b94e540f009dc3_mistral-logo.webp) Mixtral 8x7B ------------ The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. TRY THIS MODEL](https://api.together.xyz/playground/chat/mistralai/Mixtral-8x7B-Instruct-v0.1)

[Embeddings ![Image 178](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a6683b94e540f016596_bert-logo.webp) M2-BERT 80M 32K Retrieval ------------------------- An 80M checkpoint of M2-BERT, pretrained with sequence length 32768, and it has been fine-tuned for long-context retrieval. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Image ![Image 179](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a719b5da80c0b2057f7_stability-logo.webp) Stable Diffusion XL 1.0 ----------------------- A text-to-image generative AI model that excels at creating 1024x1024 images. TRY THIS MODEL](https://api.together.xyz/playground/image/stabilityai/stable-diffusion-xl-base-1.0)

[Chat ![Image 180](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f4197d83b94e540f009dc3_mistral-logo.webp) Mistral Instruct ---------------- instruct fine-tuned version of Mistral-7B-v0.1 TRY THIS MODEL](https://api.together.xyz/playground/chat/mistralai/Mistral-7B-Instruct-v0.1)

[Image ![Image 181](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 \[dev\] -------------- 12 billion parameter rectified flow transformer capable of generating images from text descriptions. TRY THIS MODEL](https://api.together.ai/playground/image/black-forest-labs/FLUX.1-dev)

[Language ![Image 182](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41aa6f28ecd4055c3704f_nexus-logo.webp) Nexus Raven ----------- NexusRaven is an open-source and commercially viable function calling LLM that surpasses the state-of-the-art in function calling capabilities. TRY THIS MODEL](https://api.together.xyz/playground/language/zero-one-ai/Yi-34B)

[Chat ![Image 183](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/67659ef6b5672a49cb026f39_SCB10X.webp) Typhoon 1.5 8B Instruct ----------------------- Instruct Thai large language model with 8 billion parameters based on Llama3-8B. TRY THIS MODEL](https://api.together.ai/playground/chat/scb10x/scb10x-llama3-typhoon-v1-5-8b-instruct)

[Chat ![Image 184](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f419f202c64707fcabd6ac_nous-logo.webp) Nous Capybara ------------- First Nous collection of dataset and models made by fine-tuning mostly on data created by Nous in-house TRY THIS MODEL](https://api.together.xyz/playground/chat/NousResearch/Nous-Capybara-7B-V1p9)

[Image ![Image 185](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 Redux \[dev\] -------------------- Adapter for FLUX.1 models enabling image variation, refining input images, and integrating into advanced restyling workflows. TRY THIS MODEL 8](https://api.together.ai/playground/image/black-forest-labs/FLUX.1-redux)

[Chat ![Image 186](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Code Llama Instruct ------------------- Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. TRY THIS MODEL](https://api.together.xyz/playground/chat/codellama/CodeLlama-70b-Instruct-hf)

[Language ![Image 187](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) RedPajama-INCITE Instruct ------------------------- Designed for few-shot prompts, fine-tuned over the RedPajama-INCITE-Base-7B-v1 base model. TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/RedPajama-INCITE-7B-Instruct)

[Chat ![Image 188](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41afa542cc239b4ffe96a_lmsys-logo.webp) vicuna-v1-5-16k --------------- Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT. TRY THIS MODEL](https://api.together.xyz/playground/chat/lmsys/vicuna-13b-v1.5-16k)

[Image ![Image 189](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 Canny \[dev\] -------------------- 12 billion parameter rectified flow transformer capable of generating an image based on a text description while following the structure of a given input image. TRY THIS MODEL](https://api.together.ai/playground/image/black-forest-labs/FLUX.1-canny)

[Chat ![Image 190](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f419f202c64707fcabd6ac_nous-logo.webp) Nous Hermes Llama-2 ------------------- Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. TRY THIS MODEL](https://api.together.xyz/playground/chat/NousResearch/Nous-Hermes-Llama2-13b)

[Chat ![Image 191](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41b23243f3397b0035786_wizard-lm-logo.webp) Wizard LM --------- This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities TRY THIS MODEL](https://api.together.xyz/playground/chat/WizardLM/WizardLM-13B-V1.2)

[Image ![Image 192](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 Depth \[dev\] -------------------- 12 billion parameter rectified flow transformer capable of generating an image based on a text description while following the structure of a given input image. TRY THIS MODEL](https://api.together.ai/playground/image/black-forest-labs/FLUX.1-depth)

[Embeddings ![Image 193](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41b3005f80aa28cf47758_baai-logo.webp) BGE-Large-EN v1.5 ----------------- BAAI general embedding - large, english - model v1.5. FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Embeddings ![Image 194](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a6683b94e540f016596_bert-logo.webp) M2-BERT 80M 2K Retrieval ------------------------ An 80M checkpoint of M2-BERT, pretrained with sequence length 2048, and it has been fine-tuned for long-context retrieval. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Embeddings ![Image 195](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a6683b94e540f016596_bert-logo.webp) M2-BERT 80M 8K Retrieval ------------------------ An 80M checkpoint of M2-BERT, pretrained with sequence length 8192, and it has been fine-tuned for long-context retrieval. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Code ![Image 196](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41b23243f3397b0035786_wizard-lm-logo.webp) WizardCoder Python v1.0 ----------------------- This model empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. TRY THIS MODEL](https://api.together.xyz/playground/code/WizardLM/WizardCoder-Python-34B-V1.0)

[Embeddings ![Image 197](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41b3005f80aa28cf47758_baai-logo.webp) BGE-Base-EN v1.5 ---------------- BAAI general embedding - base, english - model v1.5. FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector databases for LLMs. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Chat ![Image 198](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) LLaMA-2-7B-32K-Instruct ----------------------- Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations, instruction tuned by Together TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/Llama-2-7B-32K-Instruct)

[Language ![Image 199](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f4197d83b94e540f009dc3_mistral-logo.webp) Mistral ------- 7.3B parameter model that outperforms Llama 2 13B on all benchmarks, approaches CodeLlama 7B performance on code, Uses Grouped-query attention (GQA) for faster inference and Sliding Window Attention (SWA) to handle longer sequences at smaller cost TRY THIS MODEL](https://api.together.xyz/playground/language/mistralai/Mistral-7B-v0.1)

[Code ![Image 200](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Code Llama Python ----------------- Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. TRY THIS MODEL](https://api.together.xyz/playground/code/codellama/CodeLlama-70b-Python-hf)

[Chat ![Image 201](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41afa542cc239b4ffe96a_lmsys-logo.webp) Vicuna v1.5 ----------- Vicuna is a chat assistant trained by fine-tuning Llama 2 on user-shared conversations collected from ShareGPT. TRY THIS MODEL](https://api.together.xyz/playground/chat/lmsys/vicuna-13b-v1.5)

[Language ![Image 202](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41bcbe6f60c03dd817a9a_phind-logo.webp) Phind Code LLaMA v2 ------------------- Phind-CodeLlama-34B-v1 trained on additional 1.5B tokens high-quality programming-related data proficient in Python, C/C++, TypeScript, Java, and more. TRY THIS MODEL](https://api.together.xyz/playground/language/Phind/Phind-CodeLlama-34B-v2)

[Language ![Image 203](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) LLaMA-2 ------- Language model trained on 2 trillion tokens with double the context length of Llama 1. Available in three sizes: 7B, 13B and 70B parameters TRY THIS MODEL](https://api.together.xyz/playground/language/meta-llama/Llama-2-70b-hf)

[Code ![Image 204](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Code Llama ---------- Code Llama is a family of large language models for code based on Llama 2 providing infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. TRY THIS MODEL](https://api.together.xyz/playground/code/togethercomputer/CodeLlama-34b)

[Language ![Image 205](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) LLaMA-2-32K ----------- Extending LLaMA-2 to 32K context, built with Meta's Position Interpolation and Together AI's data recipe and system optimizations. TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/LLaMA-2-7B-32K)

[Chat ![Image 206](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41ce6ea6a06c87ce760a5_chronos-hermes-logo.webp) Chronos Hermes -------------- This model is a 75/25 merge of Chronos (13B) and Nous Hermes (13B) models resulting in having a great ability to produce evocative storywriting and follow a narrative. TRY THIS MODEL](https://api.together.xyz/playground/chat/Austism/chronos-hermes-13b)

[Rerank ![Image 207](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41cf2b91f6e6cfd5bbf87_salesforce-logo.png) Salesforce LlamaRank -------------------- Salesforce Research's proprietary fine-tuned rerank model with 8K context, outperforming Cohere Rerank for superior document retrieval. TRY THIS MODEL](https://docs.together.ai/docs/together-and-llamarank)

[Chat ![Image 208](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41cfe25bc0196783b63d6_garage-bAInd-logo.webp) Platypus2 Instruct ------------------ An instruction fine-tuned LLaMA-2 (70B) model by merging Platypus2 (70B) by garage-bAInd and LLaMA-2 Instruct v2 (70B) by upstage. TRY THIS MODEL](https://api.together.xyz/playground/chat/garage-bAInd/Platypus2-70B-instruct)

[Language ![Image 209](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41b23243f3397b0035786_wizard-lm-logo.webp) WizardLM v1.0 (70B) ------------------- This model achieves a substantial and comprehensive improvement on coding, mathematical reasoning and open-domain conversation capacities. TRY THIS MODEL](https://api.together.xyz/playground/language/WizardLM/WizardLM-70B-V1.0)

[Chat ![Image 210](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41d1fd146c3b0b9c76453_gryphe-logo.webp) MythoMax-L2 ----------- MythoLogic-L2 and Huginn merge using a highly experimental tensor type merge technique. The main difference with MythoMix is that I allowed more of Huginn to intermingle with the single tensors located at the front and end of a model TRY THIS MODEL](https://api.together.xyz/playground/chat/Gryphe/MythoMax-L2-13b)

[Chat ![Image 211](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen-Chat --------- 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B-Chat is a large-model-based AI assistant, which is trained with alignment techniques. TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/Qwen-7B-Chat)

[Language ![Image 212](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a073403f9e2b7806f05_qwen-logo.webp) Qwen ---- 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Aibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/Qwen-7B)

[Image ![Image 213](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66fe62e5a2afa1d4ebe94cae_bfl-logo.webp) FLUX.1 \[pro\] -------------- First generation premium image generation model by Black Forest Labs. TRY THIS MODEL](https://api.together.xyz/playground/image/black-forest-labs/FLUX.1-pro)

[Chat ![Image 214](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) RedPajama-INCITE Chat --------------------- Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-7B-v1 base model. TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/RedPajama-INCITE-7B-Chat)

[Embeddings ![Image 215](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a6683b94e540f016596_bert-logo.webp) BERT ---- Pretrained model on English language using a masked language modeling (MLM) objective. The embedding dimension is 768, and the number of model parameters is 110M. This model is uncased: it does not make a difference between english and English. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Language ![Image 216](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) RedPajama-INCITE ---------------- Base model that aims to replicate the LLaMA recipe as closely as possible (blog post). TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/RedPajama-INCITE-7B-Base)

[Embeddings ![Image 217](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41a6683b94e540f016596_bert-logo.webp) Sentence-BERT ------------- This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 500K (query, answer) pairs from the MS MARCO dataset. Its embedding dimension is 768 with 512 max context length. TRY THIS MODEL](https://docs.together.ai/docs/embeddings-rest)

[Language ![Image 218](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) GPT-JT-Moderation ----------------- This model can be used to moderate other chatbot models. Built using GPT-JT model fine-tuned on Ontocord.ai's OIG-moderation dataset v0.1. TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/GPT-JT-Moderation-6B)

[Language ![Image 219](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) GPT-JT ------ Fork of GPT-J instruction tuned to excel at few-shot prompts (blog post). TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/GPT-JT-6B-v1)

[Chat ![Image 220](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41adb42ea07d72bdde931_together-logo.png) GPT-NeoXT-Chat-Base ------------------- Chat model fine-tuned from EleutherAI’s GPT-NeoX with over 40 million instructions on carbon reduced compute. TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/GPT-NeoXT-Chat-Base-20B)

[Language ![Image 221](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) LLaMA ----- An auto-regressive language model, based on the transformer architecture. The model comes in different sizes: 7B, 13B, 33B and 65B parameters. TRY THIS MODEL](https://api.together.xyz/playground/language/huggyllama/llama-65b)

[Chat ![Image 222](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41e0ec3e6fc13160be7fa_tii-logo.webp) Falcon Instruct --------------- Falcon-40B-Instruct is a causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/falcon-40b-instruct)

[Language ![Image 223](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41e0ec3e6fc13160be7fa_tii-logo.webp) Falcon ------ Falcon-40B is a causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. TRY THIS MODEL](https://api.together.xyz/playground/language/togethercomputer/falcon-40b)

[![Image 224](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/66f41918314a4184b51788ed_meta-logo.png) Llama 3.1 70B ------------- The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes, that outperform many available open source and closed chat models on common industry benchmarks. TRY THIS MODEL](https://api.together.ai/playground/chat/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo)

[Chat ![Image 225](https://cdn.prod.website-files.com/plugins/Basic/assets/placeholder.60f9b1840c.svg) Pythia-Chat-Base ---------------- Chat model based on EleutherAI’s Pythia-7B model, and is fine-tuned with data focusing on dialog-style interactions. TRY THIS MODEL](https://api.together.xyz/playground/chat/togethercomputer/Pythia-Chat-Base-7B-v0.16)

End-to-end platform for the full generative AI lifecycle
--------------------------------------------------------

Leverage pre-trained models, fine-tune them for your needs, or build custom models from scratch. Whatever your generative AI needs, Together AI offers a seamless continuum of AI compute solutions to support your entire journey.

*   ### Inference
    
    The fastest way to launch AI models:
    
    *   ✔ Serverless or dedicated endpoints
        
    *   ✔ Deploy in enterprise VPC
        
    *   ✔ SOC 2 and HIPAA compliant
        
    
    *   ![Image 226](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd79a03b8efdd806dc5_meta.svg)
    *   ![Image 227](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd7c6645b16e7eeb3e3_black-forest-labs.png)
    *   ![Image 228](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd79a03b8efdd806dc1_mistral.svg)
    *   ![Image 229](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd7255e3e05c65acc3f_google.svg)
    *   ![Image 230](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd7cc4cd070af5c3a4a_stability.svg)
    *   ![Image 231](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673ccdd7e0d92ddd427631e7_qwen.svg)
*   ### Fine-Tuning
    
    Tailored customization for your tasks
    
    *   ✔ Complete model ownership
        
    *   ✔ Fully tune or adapt models
        
    *   ✔ Easy-to-use APIs
        
    
    *   Full Fine-Tuning
        
    *   LoRA Fine-Tuning
        
*   ### GPU Clusters
    
    Full control for massive AI workloads
    
    *   ✔ Accelerate large model training
        
    *   ✔ GB200, H200, and H100 GPUs
        
    *   ✔ Pricing from $1.75 / hour
        
    
    *   ![Image 232](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673cd0ccfe9bba9187f470c7_slurm.png)
    *   ![Image 233](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/673cd0cca8dfba8de14c2c95_kubernates.png)

Run  
models

Train   
Models

Speed, cost, and accuracy. Pick all three.
------------------------------------------

### SPEED RELATIVE TO VLLM

4**x** FASTER

### LLAMA-3 8B AT FULL PRECISION

400 TOKENS/SEC

### COST RELATIVE TO GPT-4o

11**x** lower cost

Why Together Inference
----------------------

Powered by the Together Inference Engine, combining research-driven innovation with deployment flexibility.

*   ### accelerated by cutting edge research
    
    **Transformer-optimized kernels:** our researchers' custom [FP8 inference kernels](https://www.together.ai/blog/nvidia-h200-and-h100-gpu-cluster-performance-together-kernel-collection), 75%+ faster than base PyTorch
    
    ‍
    
    **Quality-preserving quantization:** accelerating inference while [maintaining accuracy](https://www.together.ai/blog/together-inference-engine-2) with advances such as [QTIP](https://www.together.ai/blog/even-better-even-faster-quantized-llms-with-qtip)
    
    ‍
    
    **Speculative decoding:** faster throughput, powered by [novel](https://www.together.ai/blog/speculative-decoding-for-high-throughput-long-context-inference) [algorithms](https://www.together.ai/blog/specexec) and draft models trained on [RedPajama](https://arxiv.org/html/2411.12372v1) dataset
    
*   ### Flexibility to choose a model that fits your needs
    
    **Turbo:** ​​Best performance without losing accuracy
    
    ‍
    
    **Reference:** Full precision, available for 100% accuracy
    
    ‍
    
    **Lite:** Optimized for fast performance at the lowest cost
    
*   ### Available via Dedicated instances and serverless API
    
    **Dedicated instances**: fast, consistent performance, without rate limits, on your own single-tenant NVIDIA GPUs
    
    ‍
    
    **Serverless API**: quickly switch from closed LLMs to models like Llama, using our OpenAI compatible APIs
    

Control your IP.  
‍Own your AI.
--------------------------------

Fine-tune open-source models like Llama on your data and run them on Together Cloud or in a hyperscaler VPC. With no vendor lock-in, your AI remains fully under your control.

```shell
together files upload acme_corp_customer_support.jsonl
  
{
  "filename" : "acme_corp_customer_support.json",
  "id": "file-aab9997e-bca8-4b7e-a720-e820e682a10a",
  "object": "file"
}
  
  
together finetune create --training-file file-aab9997-bca8-4b7e-a720-e820e682a10a
--model together compute/RedPajama-INCITE-7B-Chat
```

```bash
together finetune create --training-file $FILE_ID 
--model $MODEL_NAME 
--wandb-api-key $WANDB_API_KEY 
--n-epochs 10 
--n-checkpoints 5 
--batch-size 8 
--learning-rate 0.0003
{
    "training_file": "file-aab9997-bca8-4b7e-a720-e820e682a10a",
    "model_output_name": "username/togethercomputer/llama-2-13b-chat",
    "model_output_path": "s3://together/finetune/63e2b89da6382c4d75d5ef22/username/togethercomputer/llama-2-13b-chat",
    "Suffix": "Llama-2-13b 1",
    "model": "togethercomputer/llama-2-13b-chat",
    "n_epochs": 4,
    "batch_size": 128,
    "learning_rate": 1e-06,
    "checkpoint_steps": 2,
    "created_at": 1687982945,
    "updated_at": 1687982945,
    "status": "pending",
    "id": "ft-5bf8990b-841d-4d63-a8a3-5248d73e045f",
    "epochs_completed": 3,
    "events": [
        {
            "object": "fine-tune-event",
            "created_at": 1687982945,
            "message": "Fine tune request created",
            "type": "JOB_PENDING",
        }
    ],
    "queue_depth": 0,
    "wandb_project_name": "Llama-2-13b Fine-tuned 1"
}
```

*   [Start simple Begin fine-tuning with a single command](https://www.together.ai/#)
*   [go deep Control hyperparameters like learning rate, batch size, and epochs to optimize model quality.](https://www.together.ai/#)

[Fine-tuning API](https://www.together.ai/products#fine-tuning)

Forge the AI frontier. Train on expert-built clusters.
------------------------------------------------------

Built by AI researchers for AI innovators, Together GPU Clusters are powered by NVIDIA GB200, H200, and H100 GPUs, along with the Together Kernel Collection — delivering up to 24% faster training operations.

*   ### Top-Tier NVIDIA GPUs
    
    NVIDIA's latest GPUs, like GB200, H200, and H100, for peak AI performance, supporting both training and inference.
    
*   ### Accelerated Software Stack
    
    The Together Kernel Collection includes custom CUDA kernels, reducing training times and costs with superior throughput.
    
*   ### High-Speed Interconnects
    
    InfiniBand and NVLink ensure fast communication between GPUs, eliminating bottlenecks and enabling rapid processing of large datasets.
    
*   ### Highly Scalable & Reliable
    
    Deploy 16 to 1000+ GPUs across global locations, with 99.9% uptime SLA.
    
*   ### Expert AI Advisory Services
    
    Together AI’s expert team offers consulting for custom model development and scalable training best practices.
    
*   ### Robust Management Tools
    
    Slurm and Kubernetes orchestrate dynamic AI workloads, optimizing training and inference seamlessly.
    

[Together GPU Clusters](https://www.together.ai/gpu-clusters)

Training-ready clusters – H100, H200, or A100
---------------------------------------------

[Reserve your cluster today](https://www.together.ai/forms/gpu-cluster-requests?utm_source=website&utm_medium=internal&utm_campaign=home-page)

THE AI ACCELERATION CLOUD
-------------------------

BUILT ON LEADING AI RESEARCH.
-----------------------------

![Image 234: Sphere
](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/650091497a540a18ebadbb53_sphere.svg)

Innovations
-----------

Our research team is behind breakthrough AI models, datasets, and optimizations.

[See all research](https://www.together.ai/research)

[### Cocktail SGD With Cocktail SGD, we’ve addressed a key hindrance to training generative AI models in a distributed environment: networking overhead. Cocktail SGD is a set of optimizations that reduces network overhead by up to 117x. Read more](https://www.together.ai/blog/cocktailsgd)

[### FlashAttention-3 FlashAttention-3 achieves up to 75% GPU utilization on H100s, making AI models up to 2x faster and enabling efficient processing of longer text inputs. It allows for faster training and inference of LLMs, supports lower precision operations for improved efficiency. Read more](https://www.together.ai/blog/flashattention-3)

[### RedPajama Our RedPajama project enables leading generative AI models to be available as fully open-source. The RedPajama models have been downloaded millions of times, and the RedPajama dataset has been used to create over 500 leading models. Read more](https://www.together.ai/blog/redpajama-data-v2)

[### Sub-quadratic model architectures In close collaboration with Hazy Research, we’re working on the next core architecture for generative AI models that will provide even faster performance with longer context. Our research published in this area includes Striped Hyena, Monarch Mixer, and FlashConv. Read more](https://www.together.ai/blog/stripedhyena-7b)

Customer Stories
----------------

See how we support leading teams around the world. Our customers are creating innovative generative AI applications, faster.

![Image 235](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/653bf70f91e3154115d3df0b_pikalabs.png)

### Pika creates the next gen text-to-video models on Together GPU Clusters

![Image 236](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/656671bcb5eedcfea92b1d5a_Pika_Avatar.png)

[](https://www.together.ai/solutions#pika-labs)

![Image 237](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/654109a9abf6ff146d4c27c1_nexusflow-white-300.png)

### Nexusflow uses Together GPU Clusters to build cybersecurity models

![Image 238](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/65458ff56464c3cb6a12622b_Screenshot%202023-11-03%20at%205.27.27%E2%80%AFPM.png)

[](https://www.together.ai/solutions#nexus-flow)

![Image 239](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/654141dfbfd06a9a1b6bf179_arcee-white-300.png)

### Arcee builds domain adaptive language models with Together Custom Models

![Image 240](https://cdn.prod.website-files.com/650c3b59079d92475f37b68f/6543d9c2fa14007cbaf19814_dalm.jpeg)

[](https://www.together.ai/solutions#arcee)

[Start building yours here →](http://api.together.ai/)

1.  Testing conducted by Together AI in November 2023 using Llama-2-70B running on Together Inference, TGI, vLLM, Anyscale, Perplexity, and Open AI. Mosaic ML comparison based on published numbers in [Mosaic ML blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices). Detailed results and methodology published [here](http://www.together.ai/blog/together-inference-engine-v1).
2.  Testing conducted by Together AI in November 2023 using Llama-2-70B running on Together Inference, TGI, vLLM, Anyscale, Perplexity, and Open AI. Mosaic ML comparison based on published numbers in [Mosaic ML blog](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices). Detailed results and methodology published [here](http://www.together.ai/blog/together-inference-engine-v1).
3.  Testing conducted by Together AI in November 2023 using Llama-2-70B running on Together Inference. Detailed results and methodology published [here](http://www.together.ai/blog/together-inference-engine-v1).
4.  Based on published pricing November 8th, 2023, comparing [Open AI GPT-3.5-Turbo](https://openai.com/pricing) to [Llama-2-13B on Together Inference](https://together.ai/pricing) using Serverless Endpoints. Assumes equal number of input and output tokens.
5.  Compared to a standard attention implementation in PyTorch, FlashAttention-2 can be up to 9x faster. [Source](https://hazyresearch.stanford.edu/blog/2023-07-17-flash2#:~:text=Compared%20to%20a%20standard%20attention%20implementation%20in%20PyTorch%2C%20FlashAttention%2D2%20can%20be%20up%20to%209x%20faster.).
6.  Testing methodology and results published in [this research paper.](https://together.ai/blog/cocktailsgd)
7.  Based on published pricing November 8th, 2023, comparing [AWS Capacity Blocks](https://aws.amazon.com/ec2/capacityblocks/pricing/) and [AWS p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/) instances to Together GPU Clusters configured with an equal number of H100 SXM5 GPUs on our 3200 Gbps Infiniband networking configuration.

Subscribe to newsletter
-----------------------

Thank you! Your submission has been received!

Oops! Something went wrong while submitting the form.

*   [Products](https://www.together.ai/products)
*   [Solutions](https://www.together.ai/solutions)
*   [Research](https://www.together.ai/research)
*   [Blog](https://www.together.ai/blog)
*   [About](https://www.together.ai/about)
*   [Pricing](https://www.together.ai/pricing)
*   [Contact](https://www.together.ai/contact)
*   [Status](http://status.together.ai/)

*   [](https://discord.gg/9Rk6sSeWEG)
*   [](http://twitter.com/togethercompute)
*   [](https://www.linkedin.com/company/togethercomputer)

© 2024 San Francisco, CA 94114

*   [Privacy policy](https://www.together.ai/privacy)
*   [Terms of service](https://www.together.ai/terms-of-service)

![Image 241](https://cdn.prod.website-files.com/64f6f2c0e3f4c5a91c1e823a/6500732503885fd3e7e06d70_logo-dark.svg)