Large Language Models Are Human-Level Prompt Engineers
------------------------------------------------------

ICLR 2023
---------

Yongchao Zhou\*, Andrei Ioan Muresanu\*, Ziwen Han\*, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba
--------------------------------------------------------------------------------------------------------

\[ [ArXiv](https://arxiv.org/abs/2211.01910) | [GitHub](https://github.com/keirp/automatic_prompt_engineer) | [Colab](https://colab.research.google.com/drive/1Hrz6Q7GFdH5OVg3Dis86f5OqiGdkDfRP?usp=sharing)  | [Demo](https://colab.research.google.com/drive/1oL1CcvzRybAbmeqs--2csaIvSOpjH072?usp=sharing) \]

By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to improve few-shot learning performance by simply prepending them to standard in-context learning prompts, to find better zero-shot chain-of- thought prompts, as well as to steer models toward truthfulness and/or informativeness.

Our method, Automatic Prompt Engineer (APE), automatically generates instructions for a task that is specified via output demonstrations: it generates several instruction candidates, either via direct inference or a recursive process based on semantic similarity, executes them using the target model, and selects the most appropriate instruction based on computed evaluation scores.

![Image 17](https://lh3.googleusercontent.com/8WAVq7YHvc4Kk5PIzl4e4xX65KlRG_R6szTD6VWC26xoiYD8x9q2t-MUFhH1CdUIIS0Kns3WbPmVJfY8gr9oHCSSz7IZT_N2mjjeO3idg89tUgonJrxjdt6qCBjbyyFRQg=w1280)

Due to the infinitely large search space, finding the right instruction can be extremely difficult, APE uses large language models as inference models. We consider two approaches to generate high-quality candidates, namely, forward mode generation and reverse mode generation.

![Image 18](https://lh5.googleusercontent.com/H32jqP6shBRiAtuQzCHPnKWnzVkuiZv_afbnJ3CsjvXwn2Vtr63mGtP26h4GYJwVVXFrG08QokWC_FOFBSG4aTvzjG3PxdqdT1xoDoksZtcDa_7ISUjsMMWXvzvQZYfSxw=w1280)

We examines how APE can guide LLMs to desired behaviors. We investigate from three perspectives: zero-shot performance, few-shot in-context learning performance, and truthfulness.

*   Instruction Induction: We assess the zero-shot and few-shot in-context learning on 24 instruction induction tasks proposed in [Honovich et al. (2022)](https://arxiv.org/abs/2205.10782). The tasks span many facets of language understanding, from simple phrase structure to similarity and causality identification.
    

*   BIG-Bench: We test APE on 21 more challenging curated tasks from BIG-Bench with human prompt engineered baselines. These tasks include emotional understanding, context-free understanding, reading comprehension, summarization, and various reasoning tasks.
    
*   Chain of Thought: We also use APE for a general chain of thought (CoT) prompt to dramatically improve the LLM reasoning, outperforming the prior human-crafted baseline in  ([Kojima et al., 2022](https://arxiv.org/abs/2205.11916)).
    
*   TruthfulQA: Finally, to see how APE-generated instructions can steer an LLM to generate answers with different styles, we apply our method on TruthfulQA ([Lin et al., 2022](https://arxiv.org/abs/2109.07958)) to study the trade-off between truthfulness and informativeness.
    

![Image 19](https://lh4.googleusercontent.com/FqSQ9snOeye5iviOQ4bOcawAlTYLHr3OKkirUy-p8xhNMRz4Z0nHwX3PsKH6Z8SHC00m7Hh-n5W3g0i9bu-rYkBMYNBkIkVlHWuLy7c4H70P_oGMt04B6S5IlyEyTyhGBw=w1280)

Zero-shot test accuracy on 24 Instruction Induction tasks. APE achieves human-level performance on 24 out of 24 tasks.

![Image 20](https://lh5.googleusercontent.com/C9Vv8allb1YeHStndA0jjIUml9It250IXLHd0o3w6N_-t5WkGik0H7na4w0MnjuYpdFB5gVvgmuyBB0m76wCGYL-G1r9eMziPFN_Kd5Iw6Ti2J6a9iofIwbxaDc-InwLpQ=w1280)

Few-shot in-context test accuracy on 24 Instruction Induction tasks. APE improves the few-shot in-context learning performance on 21 out of 24 tasks.

![Image 21](https://lh6.googleusercontent.com/t75GsVlv9q69EZzVDo1lfiEIWVGhSiP7z4E1U_q9Vv2EYRBrh6BSlAFKCLaNzHS7685BwMgSuSq23LgsiUSQ-OcV59Yd9nKvltY18OLtziFVEO3UugLfyZQcThxfzTMTwQ=w1280)

We curate 21 tasks from [BIG-Bench](https://github.com/google/BIG-bench) with human prompt engineered baselines, including nine tasks from BIG-Bench Hard  ([Suzgun et al., 2022](https://arxiv.org/abs/2210.09261)). We call this dataset BIG-Bench Instruction Induction (BBII).

APE is able to improve or match human prompts in zero-shot performance on 17/21 tasks.

  

![Image 22](https://lh5.googleusercontent.com/HWID7U-RUvS25TGCpVOVguhXWe21UfPBxzqpYupHUYPL5LS3zoYoMTpCPGWnUFtTJjk5Ig4S9d4F_oZQfvuV1bu9ILZaJ98rIFmrlw-0DPqd4o4kD2ET_xiQqjy6wiaGdg=w1280)

APE discovers a better general CoT prompt than the human engineered "Let's think step by step" from ([Kojima et al., 2022](https://arxiv.org/abs/2205.11916)).

This prompt to elicit chain of thought reasoning is able to improve the performance on MultiArith ([Roy & Roth, 2016](https://arxiv.org/abs/1608.01413)) from 78.7 -\> 82.0 and performance on GSM8K ([Cobbe et al., 2021](https://arxiv.org/abs/2110.14168)) from 40.7 -\> 43.0.

#1 on the leaderboard as of 2022.

![Image 23](https://lh4.googleusercontent.com/U1hLV7lVJIz1hDDohaZUuPWlhNDR3qZwaOFaNEvMhLfguTALbT5sYncAH7vKzyN1uAn5rXywRV5qm5sM-PjQRTdfvFsNjztf6nPf5wWyzAKNgxFL9dyX4hYy_gpLfwVsvA=w1280)

(a) Average test performance of APE instructions. Percentage of answers that were either true (% True), informative (% Info), or both (% True + % Info).

![Image 24](https://lh3.googleusercontent.com/D0FJONPtwh4QfxY6gBM7WWpksuDjIGfirjFxEZ3obVj9GWIs8mnUWM1RPoYPvj-wvuYn78MqT9tU1uUbhUYp3KRcz-brk1VUiozuxuWeR8XtQA5qUWHo6lKkPcaf5FmVMw=w1280)

(b) %True-%Info trade-off Test: %True-%Info frontier computed on test data with top 10 instructions selected from each metric.

  

We compared APE prompt with the human prompt from [Lin et al. (2022)](https://arxiv.org/abs/2109.07958). Figure (a) shows that APE instructions can outperform the human-engineered prompt on all three metrics. Figure (b) investigates the trade-off between truthfulness and informativeness using the top 10 candidates ranked by different metrics. The APE instructions tend to target the two ends of this %true-%info Pareto frontier.

@article{zhou2022large,

title={Large Language Models Are Human-Level Prompt Engineers},

author={Yongchao Zhou and Andrei Ioan Muresanu and Ziwen Han and Keiran Paster and Silviu Pitis and Harris Chan and Jimmy Ba},

year={2022},

eprint={2211.01910},

archivePrefix={arXiv},

primaryClass={cs.LG}

}

We would like to thank Or Honovich and Michael Zhang for their help and valuable feedback. JB was supported by NSERC Grant \[2020-06904\], CIFAR AI Chairs program, Google Research Scholar Program and Amazon Research Award. KP was supported by NSERC PGS-D. SP was supported by NSERC CGS-D. HC was supported by NSERC CGS-D and RBC Graduate Fellowship. Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute for Artificial Intelligence.