---
title: AI Alignment Forum
description: A community blog devoted to technical AI alignment research
url: https://www.alignmentforum.org/
timestamp: 2025-01-20T15:59:04.286Z
domain: www.alignmentforum.org
path: root
---

# AI Alignment Forum


A community blog devoted to technical AI alignment research


## Content

AI Alignment Forum
===============

[AI ALIGNMENT FORUM](https://www.alignmentforum.org/)

[AF](https://www.alignmentforum.org/)


------------------------------------------------------------------------------------------------

Login

[Home](https://www.alignmentforum.org/)[Library](https://www.alignmentforum.org/library)[Questions](https://www.alignmentforum.org/questions)[All Posts](https://www.alignmentforum.org/allPosts)

[About](https://www.alignmentforum.org/about)

[Home](https://www.alignmentforum.org/ "Latest posts, comments and curated content.")[Library](https://www.alignmentforum.org/library "Curated collections of the AI Alignment Forum's best writing.")[Questions](https://www.alignmentforum.org/questions)[All Posts](https://www.alignmentforum.org/allPosts "See all posts, filtered and sorted however you like.")

[Coherence arguments do not entail goal-directed behavior](https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior)

[Best of LessWrong 2018](https://www.alignmentforum.org/bestoflesswrong?year=2018&category=aisafety)

Rohin Shah argues that many common arguments for AI risk (about the perils of powerful expected utility maximizers) are actually arguments about goal-directed behavior or explicit reward maximization, which are not actually implied by coherence arguments. An AI system could be an expected utility maximizer without being goal-directed or an explicit reward maximizer.

by [Rohin Shah](https://www.alignmentforum.org/users/rohinmshah)

![Image 3](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/splashArtImagePrompta%20maze%2C%20with%20one%20path%20leading%20to%20a%20gold%20trophy%20and%20another%20leading%20to%20a%20simple%20exit/twtww7hos5fdx6xkoftw)

21DanielFilan

I think that strictly speaking this post (or at least the main thrust) is true, and proven in the first section. The title is arguably less true: I think of 'coherence arguments' as including things like 'it's not possible for you to agree to give me a limitless number of dollars in return for nothing', which does imply some degree of 'goal-direction'. I think the post is important, because it constrains the types of valid arguments that can be given for 'freaking out about goal-directedness', for lack of a better term. In my mind, it provokes various follow-up questions: 1. What arguments would imply 'goal-directed' behaviour? 2. With what probability will a random utility maximiser be 'goal-directed'? 3. How often should I think of a system as a utility maximiser in resources, perhaps with a slowly-changing utility function? 4. How 'goal-directed' are humans likely to make systems, given that we are making them in order to accomplish certain tasks that don't look like random utility functions? 5. Is there some kind of 'basin of goal-directedness' that systems fall in if they're even a little goal-directed, causing them to behave poorly? Off the top of my head, I'm not familiar with compelling responses from the 'freak out about goal-directedness' camp on points 1 through 5, even though as a member of that camp I think that such responses exist. Responses from outside this camp include Rohin's post 'Will humans build goal-directed agents?'. Another response is Brangus' comment post, although I find its theory of goal-directedness uncompelling. I think that it's notable that Brangus' post was released soon after this was announced as a contender for Best of LW 2018. I think that if this post were added to the Best of LW 2018 Collection, the 'freak out' camp might produce more of these responses and move the dialogue forward. As such, I think it should be added, both because of the clear argumentation and because of the response it is likely to provoke.

17Vanessa Kosoy

In this essay, Rohin sets out to debunk what ey perceive as a prevalent but erroneous idea in the AI alignment community, namely: "VNM and similar theorems imply goal-directed behavior". This is placed in the context of Rohin's thesis that solving AI alignment is best achieved by designing AI which is not goal-directed. The main argument is: "coherence arguments" imply expected utility maximization, but expected utility maximization does not imply goal-directed behavior. Instead, it is a vacuous constraint, since any agent policy can be regarded as maximizing the expectation of some utility function. I have mixed feelings about this essay. On the one hand, the core argument that VNM and similar theorems do not imply goal-directed behavior is true. To the extent that some people believed the opposite, correcting this mistake is important. On the other hand, (i) I don't think the claim Rohin is debunking is the claim Eliezer had in mind in those sources Rohin cites (ii) I don't think that the conclusions Rohin draws or at least implies are the right conclusions. The actual claim that Eliezer was making (or at least my interpretation of it) is, coherence arguments imply that if we assume an agent is goal-directed then it must be an expected utility maximizer, and therefore EU maximization is the correct mathematical model to apply to such agents. Why do we care about goal-directed agents in the first place? The reason is, on the one hand goal-directed agents are the main source of AI risk, and on the other hand, goal-directed agents are also the most straightforward approach to solving AI risk. Indeed, if we could design powerful agents with the goals we want, these agents would protect us from unaligned AIs and solve all other problems as well (or at least solve them better than we can solve them ourselves). Conversely, if we want to protect ourselves from unaligned AIs, we need to generate very sophisticated long-term plans of action in the physical world, possibl

[### The 2023 Review Final Voting ============](https://www.alignmentforum.org/reviewVoting)
============================================================================================

[What is this?](https://www.alignmentforum.org/posts/pudQtkre7f9GLmb2b/the-2023-lesswrong-review-the-basic-ask)

Nomination Voting

Dec 16th

Discussion

Jan 16th

Final Voting

Feb 6th

23[Before smart AI, there will be many mediocre or specialized AIs](https://www.alignmentforum.org/posts/5sWNnbHRkExfLaS49/before-smart-ai-there-will-be-many-mediocre-or-specialized)

[Lukas Finnveden](https://www.alignmentforum.org/users/lukas-finnveden)

2y

4

52[Davidad's Bold Plan for Alignment: An In-Depth Explanation](https://www.alignmentforum.org/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation)

[Charbel-Raphael Segerie](https://www.alignmentforum.org/users/charbel-raphael), [Gabin](https://www.alignmentforum.org/users/gabin-kolly)

2y

5

52[Reducing sycophancy and improving honesty via activation steering](https://www.alignmentforum.org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation)

[Nina Panickssery](https://www.alignmentforum.org/users/nina-panickssery)

1y

4

[Load More](https://www.alignmentforum.org/#)

AI Alignment Posts
==================

50[Welcome & FAQ!](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq)

[Ruben Bloom](https://www.alignmentforum.org/users/ruby), [Oliver Habryka](https://www.alignmentforum.org/users/habryka4)

3y

9

1[GPT-2 constructs a bag-of-tokens to represent context](https://www.alignmentforum.org/posts/QoFxZNG7bXishtxsv/gpt-2-constructs-a-bag-of-tokens-to-represent-context)

[Alex Gibson](https://www.alignmentforum.org/users/alex-gibson), [Jason Gross](https://www.alignmentforum.org/users/jason-gross)

3h

0

6[AXRP Episode 38.5 - Adri√† Garriga-Alonso on Detecting AI Scheming](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)

[DanielFilan](https://www.alignmentforum.org/users/danielfilan)

15h

0

39[Thoughts on the conservative assumptions in AI control](https://www.alignmentforum.org/posts/rHyPtvfnvWeMv7Lkb/thoughts-on-the-conservative-assumptions-in-ai-control)

[Buck Shlegeris](https://www.alignmentforum.org/users/buck)

3d

0

23[Timaeus is hiring researchers & engineers](https://www.alignmentforum.org/posts/g8e4pz7aHGCpahFR4/timaeus-is-hiring-researchers-and-engineers)

[Jesse Hoogland](https://www.alignmentforum.org/users/jesse-hoogland), [Stan van Wingerden](https://www.alignmentforum.org/users/stan-van-wingerden)

3d

0

29[Numberwang: LLMs Doing Autonomous Research, and a Call for Input](https://www.alignmentforum.org/posts/RSqfcyAW9ZkveGQ5u/numberwang-llms-doing-autonomous-research-and-a-call-for-1)

[Egg Syntax](https://www.alignmentforum.org/users/eggsyntax), [ncase](https://www.alignmentforum.org/users/ncase)

4d

1

31[Gaming TruthfulQA: Simple Heuristics Exposed Dataset Weaknesses](https://www.alignmentforum.org/posts/57k6xNcWtAtsSTcor/gaming-truthfulqa-simple-heuristics-exposed-dataset)

[](https://turntrout.com/original-truthfulqa-weaknesses)

[Alex Turner](https://www.alignmentforum.org/users/turntrout)

5d

[](https://turntrout.com/original-truthfulqa-weaknesses)

0

38[New, improved multiple-choice TruthfulQA](https://www.alignmentforum.org/posts/Bunfwz6JsNd44kgLT/new-improved-multiple-choice-truthfulqa)

[Owain Evans](https://www.alignmentforum.org/users/owain_evans), [James Chua](https://www.alignmentforum.org/users/james-chua), [Steph Lin](https://www.alignmentforum.org/users/steph-lin)

5d

0

34[Inference-Time-Compute: More Faithful? A Research Note](https://www.alignmentforum.org/posts/C8HAa2mf5kcBrpjkX/inference-time-compute-more-faithful-a-research-note)

[James Chua](https://www.alignmentforum.org/users/james-chua), [Owain Evans](https://www.alignmentforum.org/users/owain_evans)

5d

0

44[Building AI Research Fleets](https://www.alignmentforum.org/posts/WJ7y8S9WdKRvrzJmR/building-ai-research-fleets)

[Ben Goldhaber](https://www.alignmentforum.org/users/bgold), [Jesse Hoogland](https://www.alignmentforum.org/users/jesse-hoogland)

8d

0

[Load More](https://www.alignmentforum.org/#)

Popular Comments


====================

[Load More](https://www.alignmentforum.org/#)

Recent Discussion
=================

[Thane Ruthenis's Shortform](https://www.alignmentforum.org/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform)

[Thane Ruthenis](https://www.alignmentforum.org/users/thane-ruthenis)

4mo

[](https://www.alignmentforum.org/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=tTDfQXWsxf6TXHAAt)[Thane Ruthenis](https://www.alignmentforum.org/users/thane-ruthenis)18m10

Alright, so I've been following the latest OpenAI Twitter freakout, and here's some urgent information about the latest closed-doors developments that I've managed to piece together:

*   Following OpenAI Twitter freakouts is a colossal, utterly pointless waste of your time and you shouldn't do it ever.
*   If you saw [this comment of Gwern's](https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety?commentId=MPNF8uSsi9mvZLxqz) going around and were incredibly alarmed, you should probably undo the associated update regarding AI timelines (at least partially, see below).
*   OpenAI may be running some galaxy-brained psyops nowadays.

Here's the sequence of even... (read more)

Reply

2Logan Riggs Smith2d

You're right! Thanks For Mice, up to 77%¬† For human cells, up to 9% ¬†(if I'm understanding this part correctly). ¬† So seems like you can do wildly different depending on the setting (mice, humans, bovine, etc), and I don't know what the Retro folks were doing, but does make their result less impressive.¬†

2Tsvi Benson-Tilsen2d

(Still impressive and interesting of course, just not literally SOTA.)

2Logan Riggs Smith2d

Thinking through it more, Sox2-17 (they changed 17 amino acids from Sox2 gene) was your linked paper's result, and Retro's was a modified version of factors Sox AND KLF. Would be cool if these two results are complementary.

[AXRP Episode 38.5 - Adri√† Garriga-Alonso on Detecting AI Scheming](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)

6

[DanielFilan](https://www.alignmentforum.org/users/danielfilan)

15h

[YouTube link](https://youtu.be/3A7CK4-1OFo)

Suppose we‚Äôre worried about AIs engaging in long-term plans that they don‚Äôt tell us about. If we were to peek inside their brains, what should we look for to check whether this was happening? In this episode Adri√† Garriga-Alonso talks about his work trying to answer this question.

Topics we discuss:

*   [The Alignment Workshop](https://www.alignmentforum.org/#alignment-workshop)
*   [How to detect scheming AIs](https://www.alignmentforum.org/#how-to-detect-scheming-ais)
*   [Sokoban-solving networks taking time to think](https://www.alignmentforum.org/#sokoban-solving-nets-taking-time-to-think)
*   [Model organisms of long-term planning](https://www.alignmentforum.org/#model-organisms-of-long-term-planning)
*   [How and why to study planning in networks](https://www.alignmentforum.org/#how-and-why-study-planning-networks)

**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I‚Äôve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we‚Äôre discussing, as usual, are in the description. A transcript is, as usual, available...

[(Continue Reading ‚Äì 4487 more words)](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)

[Charlie Steiner's Shortform](https://www.alignmentforum.org/posts/QnzWgCkW9MPMp69A2/charlie-steiner-s-shortform)

[Charlie Steiner](https://www.alignmentforum.org/users/charlie-steiner)

4y

26Charlie Steiner4d

Could someone who thinks capabilities benchmarks are safety work explain the basic idea to me? It's not all that valuable for my personal work to know how good models are at ML tasks. Is it supposed to be valuable to legislators writing regulation? To SWAT teams calculating when to bust down the datacenter door and turn the power off? I'm not clear. But it sure seems valuable to someone building an AI to do ML research, to have a benchmark that will tell you where you can improve. But clearly other people think differently than me.

[Oliver Habryka](https://www.alignmentforum.org/users/habryka4)3d713

I think the core argument is "if you want to slow down, or somehow impose restrictions on AI research and deployment, you need some way of defining thresholds. Also, most policymaker's cruxes appear to be that AI will not be a big deal, but if they thought it was going to be a big deal they would totally want to regulate it much more. Therefore, having policy proposals that can use future eval results as a triggering mechanism is politically more feasible, and also, epistemically helpful since it allows people who do think it will be a big deal to establish a track record".

I find these arguments reasonably compelling, FWIW.

Reply

28elifland3d

Not representative of motivations for all people for all types of evals, but https://www.openphilanthropy.org/rfp-llm-benchmarks/, https://www.lesswrong.com/posts/7qGxm2mgafEbtYHBf/survey-on-the-acceleration-risks-of-our-new-rfps-to-study, https://docs.google.com/document/d/1UwiHYIxgDFnl\_ydeuUq0gYOqvzdbNiDpjZ39FEgUAuQ/edit, and some posts in https://www.lesswrong.com/tag/ai-evaluations seem relevant.

[What is the most impressive game LLMs can play well?](https://www.alignmentforum.org/posts/3c5tx5WjZ5Yvniq6Y/what-is-the-most-impressive-game-llms-can-play-well)

9

[Cole Wyeth](https://www.alignmentforum.org/users/cole-wyeth)

12d

_Epistemic status: This is an off-the-cuff question._

~5 years ago there was a lot of exciting progress on game playing through reinforcement learning (RL). Now we have basically switched paradigms, pretraining massive LLMs on ~the internet and then apparently doing some really trivial unsophisticated RL on top of that - this is successful and highly popular because interacting with LLMs is pretty awesome (at least if you haven't done it before) and they "feel" a lot more like A.G.I. Probably there's somewhat more commercial use as well via code completion (and some would say many other tasks, personally not really convinced - generative image/video models will certainly be profitable though). There's also a sense in which they are clearly more general - e.g. one RL algorithm may learn...

[(See More ‚Äì 38 more words)](https://www.alignmentforum.org/posts/3c5tx5WjZ5Yvniq6Y/what-is-the-most-impressive-game-llms-can-play-well)

2Vanessa Kosoy4d

Relevant link

3Vanessa Kosoy4d

Apparently someone let LLMs play against the random policy and for most of them, most games end in a draw. Seems like o1-preview is the best of those tested, managing to win 47% of the time.

4gwern4d

Given the other reports, like OA's own benchmarking (as well as the extremely large dataset of chess games they mention training on), I am skeptical of this claim, and wonder if this has the same issue as other 'random chess game' tests, where the 'random' part is not neutral but screws up the implied persona.

[Vanessa Kosoy](https://www.alignmentforum.org/users/vanessa-kosoy)3d20

Do you mean that seeing the opponent make dumb moves makes the AI infer that its own moves are also supposed to be dumb, or something else?

Reply

[Load More](https://www.alignmentforum.org/#)

## Metadata

```json
{
  "title": "AI Alignment Forum",
  "description": "A community blog devoted to technical AI alignment research",
  "url": "https://www.alignmentforum.org/",
  "content": "AI Alignment Forum\n===============\n\n[AI ALIGNMENT FORUM](https://www.alignmentforum.org/)\n\n[AF](https://www.alignmentforum.org/)\n\n\n------------------------------------------------------------------------------------------------\n\nLogin\n\n[Home](https://www.alignmentforum.org/)[Library](https://www.alignmentforum.org/library)[Questions](https://www.alignmentforum.org/questions)[All Posts](https://www.alignmentforum.org/allPosts)\n\n[About](https://www.alignmentforum.org/about)\n\n[Home](https://www.alignmentforum.org/ \"Latest posts, comments and curated content.\")[Library](https://www.alignmentforum.org/library \"Curated collections of the AI Alignment Forum's best writing.\")[Questions](https://www.alignmentforum.org/questions)[All Posts](https://www.alignmentforum.org/allPosts \"See all posts, filtered and sorted however you like.\")\n\n[Coherence arguments do not entail goal-directed behavior](https://www.alignmentforum.org/posts/NxF5G6CJiof6cemTw/coherence-arguments-do-not-entail-goal-directed-behavior)\n\n[Best of LessWrong 2018](https://www.alignmentforum.org/bestoflesswrong?year=2018&category=aisafety)\n\nRohin Shah argues that many common arguments for AI risk (about the perils of powerful expected utility maximizers) are actually arguments about goal-directed behavior or explicit reward maximization, which are not actually implied by coherence arguments. An AI system could be an expected utility maximizer without being goal-directed or an explicit reward maximizer.\n\nby [Rohin Shah](https://www.alignmentforum.org/users/rohinmshah)\n\n![Image 3](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/splashArtImagePrompta%20maze%2C%20with%20one%20path%20leading%20to%20a%20gold%20trophy%20and%20another%20leading%20to%20a%20simple%20exit/twtww7hos5fdx6xkoftw)\n\n21DanielFilan\n\nI think that strictly speaking this post (or at least the main thrust) is true, and proven in the first section. The title is arguably less true: I think of 'coherence arguments' as including things like 'it's not possible for you to agree to give me a limitless number of dollars in return for nothing', which does imply some degree of 'goal-direction'. I think the post is important, because it constrains the types of valid arguments that can be given for 'freaking out about goal-directedness', for lack of a better term. In my mind, it provokes various follow-up questions: 1. What arguments would imply 'goal-directed' behaviour? 2. With what probability will a random utility maximiser be 'goal-directed'? 3. How often should I think of a system as a utility maximiser in resources, perhaps with a slowly-changing utility function? 4. How 'goal-directed' are humans likely to make systems, given that we are making them in order to accomplish certain tasks that don't look like random utility functions? 5. Is there some kind of 'basin of goal-directedness' that systems fall in if they're even a little goal-directed, causing them to behave poorly? Off the top of my head, I'm not familiar with compelling responses from the 'freak out about goal-directedness' camp on points 1 through 5, even though as a member of that camp I think that such responses exist. Responses from outside this camp include Rohin's post 'Will humans build goal-directed agents?'. Another response is Brangus' comment post, although I find its theory of goal-directedness uncompelling. I think that it's notable that Brangus' post was released soon after this was announced as a contender for Best of LW 2018. I think that if this post were added to the Best of LW 2018 Collection, the 'freak out' camp might produce more of these responses and move the dialogue forward. As such, I think it should be added, both because of the clear argumentation and because of the response it is likely to provoke.\n\n17Vanessa Kosoy\n\nIn this essay, Rohin sets out to debunk what ey perceive as a prevalent but erroneous idea in the AI alignment community, namely: \"VNM and similar theorems imply goal-directed behavior\". This is placed in the context of Rohin's thesis that solving AI alignment is best achieved by designing AI which is not goal-directed. The main argument is: \"coherence arguments\" imply expected utility maximization, but expected utility maximization does not imply goal-directed behavior. Instead, it is a vacuous constraint, since any agent policy can be regarded as maximizing the expectation of some utility function. I have mixed feelings about this essay. On the one hand, the core argument that VNM and similar theorems do not imply goal-directed behavior is true. To the extent that some people believed the opposite, correcting this mistake is important. On the other hand, (i) I don't think the claim Rohin is debunking is the claim Eliezer had in mind in those sources Rohin cites (ii) I don't think that the conclusions Rohin draws or at least implies are the right conclusions. The actual claim that Eliezer was making (or at least my interpretation of it) is, coherence arguments imply that if we assume an agent is goal-directed then it must be an expected utility maximizer, and therefore EU maximization is the correct mathematical model to apply to such agents. Why do we care about goal-directed agents in the first place? The reason is, on the one hand goal-directed agents are the main source of AI risk, and on the other hand, goal-directed agents are also the most straightforward approach to solving AI risk. Indeed, if we could design powerful agents with the goals we want, these agents would protect us from unaligned AIs and solve all other problems as well (or at least solve them better than we can solve them ourselves). Conversely, if we want to protect ourselves from unaligned AIs, we need to generate very sophisticated long-term plans of action in the physical world, possibl\n\n[### The 2023 Review Final Voting ============](https://www.alignmentforum.org/reviewVoting)\n============================================================================================\n\n[What is this?](https://www.alignmentforum.org/posts/pudQtkre7f9GLmb2b/the-2023-lesswrong-review-the-basic-ask)\n\nNomination Voting\n\nDec 16th\n\nDiscussion\n\nJan 16th\n\nFinal Voting\n\nFeb 6th\n\n23[Before smart AI, there will be many mediocre or specialized AIs](https://www.alignmentforum.org/posts/5sWNnbHRkExfLaS49/before-smart-ai-there-will-be-many-mediocre-or-specialized)\n\n[Lukas Finnveden](https://www.alignmentforum.org/users/lukas-finnveden)\n\n2y\n\n4\n\n52[Davidad's Bold Plan for Alignment: An In-Depth Explanation](https://www.alignmentforum.org/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation)\n\n[Charbel-Raphael Segerie](https://www.alignmentforum.org/users/charbel-raphael), [Gabin](https://www.alignmentforum.org/users/gabin-kolly)\n\n2y\n\n5\n\n52[Reducing sycophancy and improving honesty via activation steering](https://www.alignmentforum.org/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation)\n\n[Nina Panickssery](https://www.alignmentforum.org/users/nina-panickssery)\n\n1y\n\n4\n\n[Load More](https://www.alignmentforum.org/#)\n\nAI Alignment Posts\n==================\n\n50[Welcome & FAQ!](https://www.alignmentforum.org/posts/Yp2vYb4zHXEeoTkJc/welcome-and-faq)\n\n[Ruben Bloom](https://www.alignmentforum.org/users/ruby), [Oliver Habryka](https://www.alignmentforum.org/users/habryka4)\n\n3y\n\n9\n\n1[GPT-2 constructs a bag-of-tokens to represent context](https://www.alignmentforum.org/posts/QoFxZNG7bXishtxsv/gpt-2-constructs-a-bag-of-tokens-to-represent-context)\n\n[Alex Gibson](https://www.alignmentforum.org/users/alex-gibson), [Jason Gross](https://www.alignmentforum.org/users/jason-gross)\n\n3h\n\n0\n\n6[AXRP Episode 38.5 - Adri√† Garriga-Alonso on Detecting AI Scheming](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)\n\n[DanielFilan](https://www.alignmentforum.org/users/danielfilan)\n\n15h\n\n0\n\n39[Thoughts on the conservative assumptions in AI control](https://www.alignmentforum.org/posts/rHyPtvfnvWeMv7Lkb/thoughts-on-the-conservative-assumptions-in-ai-control)\n\n[Buck Shlegeris](https://www.alignmentforum.org/users/buck)\n\n3d\n\n0\n\n23[Timaeus is hiring researchers & engineers](https://www.alignmentforum.org/posts/g8e4pz7aHGCpahFR4/timaeus-is-hiring-researchers-and-engineers)\n\n[Jesse Hoogland](https://www.alignmentforum.org/users/jesse-hoogland), [Stan van Wingerden](https://www.alignmentforum.org/users/stan-van-wingerden)\n\n3d\n\n0\n\n29[Numberwang: LLMs Doing Autonomous Research, and a Call for Input](https://www.alignmentforum.org/posts/RSqfcyAW9ZkveGQ5u/numberwang-llms-doing-autonomous-research-and-a-call-for-1)\n\n[Egg Syntax](https://www.alignmentforum.org/users/eggsyntax), [ncase](https://www.alignmentforum.org/users/ncase)\n\n4d\n\n1\n\n31[Gaming TruthfulQA: Simple Heuristics Exposed Dataset Weaknesses](https://www.alignmentforum.org/posts/57k6xNcWtAtsSTcor/gaming-truthfulqa-simple-heuristics-exposed-dataset)\n\n[](https://turntrout.com/original-truthfulqa-weaknesses)\n\n[Alex Turner](https://www.alignmentforum.org/users/turntrout)\n\n5d\n\n[](https://turntrout.com/original-truthfulqa-weaknesses)\n\n0\n\n38[New, improved multiple-choice TruthfulQA](https://www.alignmentforum.org/posts/Bunfwz6JsNd44kgLT/new-improved-multiple-choice-truthfulqa)\n\n[Owain Evans](https://www.alignmentforum.org/users/owain_evans), [James Chua](https://www.alignmentforum.org/users/james-chua), [Steph Lin](https://www.alignmentforum.org/users/steph-lin)\n\n5d\n\n0\n\n34[Inference-Time-Compute: More Faithful? A Research Note](https://www.alignmentforum.org/posts/C8HAa2mf5kcBrpjkX/inference-time-compute-more-faithful-a-research-note)\n\n[James Chua](https://www.alignmentforum.org/users/james-chua), [Owain Evans](https://www.alignmentforum.org/users/owain_evans)\n\n5d\n\n0\n\n44[Building AI Research Fleets](https://www.alignmentforum.org/posts/WJ7y8S9WdKRvrzJmR/building-ai-research-fleets)\n\n[Ben Goldhaber](https://www.alignmentforum.org/users/bgold), [Jesse Hoogland](https://www.alignmentforum.org/users/jesse-hoogland)\n\n8d\n\n0\n\n[Load More](https://www.alignmentforum.org/#)\n\nPopular Comments\n\n\n====================\n\n[Load More](https://www.alignmentforum.org/#)\n\nRecent Discussion\n=================\n\n[Thane Ruthenis's Shortform](https://www.alignmentforum.org/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform)\n\n[Thane Ruthenis](https://www.alignmentforum.org/users/thane-ruthenis)\n\n4mo\n\n[](https://www.alignmentforum.org/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=tTDfQXWsxf6TXHAAt)[Thane Ruthenis](https://www.alignmentforum.org/users/thane-ruthenis)18m10\n\nAlright, so I've been following the latest OpenAI Twitter freakout, and here's some urgent information about the latest closed-doors developments that I've managed to piece together:\n\n*   Following OpenAI Twitter freakouts is a colossal, utterly pointless waste of your time and you shouldn't do it ever.\n*   If you saw [this comment of Gwern's](https://www.lesswrong.com/posts/HiTjDZyWdLEGCDzqu/implications-of-the-inference-scaling-paradigm-for-ai-safety?commentId=MPNF8uSsi9mvZLxqz) going around and were incredibly alarmed, you should probably undo the associated update regarding AI timelines (at least partially, see below).\n*   OpenAI may be running some galaxy-brained psyops nowadays.\n\nHere's the sequence of even... (read more)\n\nReply\n\n2Logan Riggs Smith2d\n\nYou're right! Thanks For Mice, up to 77%¬† For human cells, up to 9% ¬†(if I'm understanding this part correctly). ¬† So seems like you can do wildly different depending on the setting (mice, humans, bovine, etc), and I don't know what the Retro folks were doing, but does make their result less impressive.¬†\n\n2Tsvi Benson-Tilsen2d\n\n(Still impressive and interesting of course, just not literally SOTA.)\n\n2Logan Riggs Smith2d\n\nThinking through it more, Sox2-17 (they changed 17 amino acids from Sox2 gene) was your linked paper's result, and Retro's was a modified version of factors Sox AND KLF. Would be cool if these two results are complementary.\n\n[AXRP Episode 38.5 - Adri√† Garriga-Alonso on Detecting AI Scheming](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)\n\n6\n\n[DanielFilan](https://www.alignmentforum.org/users/danielfilan)\n\n15h\n\n[YouTube link](https://youtu.be/3A7CK4-1OFo)\n\nSuppose we‚Äôre worried about AIs engaging in long-term plans that they don‚Äôt tell us about. If we were to peek inside their brains, what should we look for to check whether this was happening? In this episode Adri√† Garriga-Alonso talks about his work trying to answer this question.\n\nTopics we discuss:\n\n*   [The Alignment Workshop](https://www.alignmentforum.org/#alignment-workshop)\n*   [How to detect scheming AIs](https://www.alignmentforum.org/#how-to-detect-scheming-ais)\n*   [Sokoban-solving networks taking time to think](https://www.alignmentforum.org/#sokoban-solving-nets-taking-time-to-think)\n*   [Model organisms of long-term planning](https://www.alignmentforum.org/#model-organisms-of-long-term-planning)\n*   [How and why to study planning in networks](https://www.alignmentforum.org/#how-and-why-study-planning-networks)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I‚Äôve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we‚Äôre discussing, as usual, are in the description. A transcript is, as usual, available...\n\n[(Continue Reading ‚Äì 4487 more words)](https://www.alignmentforum.org/posts/MpLmcLBiEbpzv2awg/axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai)\n\n[Charlie Steiner's Shortform](https://www.alignmentforum.org/posts/QnzWgCkW9MPMp69A2/charlie-steiner-s-shortform)\n\n[Charlie Steiner](https://www.alignmentforum.org/users/charlie-steiner)\n\n4y\n\n26Charlie Steiner4d\n\nCould someone who thinks capabilities benchmarks are safety work explain the basic idea to me? It's not all that valuable for my personal work to know how good models are at ML tasks. Is it supposed to be valuable to legislators writing regulation? To SWAT teams calculating when to bust down the datacenter door and turn the power off? I'm not clear. But it sure seems valuable to someone building an AI to do ML research, to have a benchmark that will tell you where you can improve. But clearly other people think differently than me.\n\n[Oliver Habryka](https://www.alignmentforum.org/users/habryka4)3d713\n\nI think the core argument is \"if you want to slow down, or somehow impose restrictions on AI research and deployment, you need some way of defining thresholds. Also, most policymaker's cruxes appear to be that AI will not be a big deal, but if they thought it was going to be a big deal they would totally want to regulate it much more. Therefore, having policy proposals that can use future eval results as a triggering mechanism is politically more feasible, and also, epistemically helpful since it allows people who do think it will be a big deal to establish a track record\".\n\nI find these arguments reasonably compelling, FWIW.\n\nReply\n\n28elifland3d\n\nNot representative of motivations for all people for all types of evals, but https://www.openphilanthropy.org/rfp-llm-benchmarks/, https://www.lesswrong.com/posts/7qGxm2mgafEbtYHBf/survey-on-the-acceleration-risks-of-our-new-rfps-to-study, https://docs.google.com/document/d/1UwiHYIxgDFnl\\_ydeuUq0gYOqvzdbNiDpjZ39FEgUAuQ/edit, and some posts in https://www.lesswrong.com/tag/ai-evaluations seem relevant.\n\n[What is the most impressive game LLMs can play well?](https://www.alignmentforum.org/posts/3c5tx5WjZ5Yvniq6Y/what-is-the-most-impressive-game-llms-can-play-well)\n\n9\n\n[Cole Wyeth](https://www.alignmentforum.org/users/cole-wyeth)\n\n12d\n\n_Epistemic status: This is an off-the-cuff question._\n\n~5 years ago there was a lot of exciting progress on game playing through reinforcement learning (RL). Now we have basically switched paradigms, pretraining massive LLMs on ~the internet and then apparently doing some really trivial unsophisticated RL on top of that - this is successful and highly popular because interacting with LLMs is pretty awesome (at least if you haven't done it before) and they \"feel\" a lot more like A.G.I. Probably there's somewhat more commercial use as well via code completion (and some would say many other tasks, personally not really convinced - generative image/video models will certainly be profitable though). There's also a sense in which they are clearly more general - e.g. one RL algorithm may learn...\n\n[(See More ‚Äì 38 more words)](https://www.alignmentforum.org/posts/3c5tx5WjZ5Yvniq6Y/what-is-the-most-impressive-game-llms-can-play-well)\n\n2Vanessa Kosoy4d\n\nRelevant link\n\n3Vanessa Kosoy4d\n\nApparently someone let LLMs play against the random policy and for most of them, most games end in a draw. Seems like o1-preview is the best of those tested, managing to win 47% of the time.\n\n4gwern4d\n\nGiven the other reports, like OA's own benchmarking (as well as the extremely large dataset of chess games they mention training on), I am skeptical of this claim, and wonder if this has the same issue as other 'random chess game' tests, where the 'random' part is not neutral but screws up the implied persona.\n\n[Vanessa Kosoy](https://www.alignmentforum.org/users/vanessa-kosoy)3d20\n\nDo you mean that seeing the opponent make dumb moves makes the AI infer that its own moves are also supposed to be dumb, or something else?\n\nReply\n\n[Load More](https://www.alignmentforum.org/#)",
  "usage": {
    "tokens": 4470
  }
}
```
