ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
: Repository-level Coding using LLMs and Planning
Ramakrishna Bairi
rbairi@microsoft.com
Microsoft ResearchIndia
Atharv Sonwane
t-asonwane@microsoft.com
Microsoft ResearchIndia
Aditya Kanade
kanadeaditya@microsoft.com
Microsoft ResearchIndia
Vageesh D C
vachand@microsoft.com
Microsoft ResearchIndia
Arun Iyer
ariy@microsoft.com
Microsoft ResearchIndia
Suresh Parthasarathy
supartha@microsoft.com
Microsoft ResearchIndia
Sriram Rajamani
sriram@microsoft.com
Microsoft ResearchIndia
B. Ashok
bash@microsoft.com
Microsoft ResearchIndia
Shashank Shet
t-sshet@microsoft.com
Microsoft ResearchIndia
(2018; 2024)
Abstract.

Software engineering activities such as package migration, fixing errors reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks.

Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic framework, called 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â to solve it. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â synthesizes a multi-step chain of edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm.

We evaluate the effectiveness of 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2â€“97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 has better match with the ground truth compared to baselines. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 is able to get 5/6 repositories to pass the validity checks (e.g., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as 
ï¿½ï¿½ï¿½ï¿½ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
) cannot get any of the repositories to pass them. We will release our data and evaluation scripts at https://aka.ms/CodePlan.

Automated coding, repositories, LLMs, static analysis, plan, chain of edits
â€ copyright: acmcopyright
â€ journalyear: 2018
â€ doi: XXXXXXX.XXXXXXX
â€ conference: Make sure to enter the correct conference title from your rights confirmation emai; June 03â€“05, 2018; Woodstock, NY
â€ price: 15.00
â€ isbn: 978-1-4503-XXXX-X/18/06
â€ copyright: acmcopyright
â€ journalyear: 2024
â€ doi: XXXXXXX.XXXXXXX
â€ price: 15.00
â€ isbn: 978-1-4503-XXXX-X/18/06
â€ ccs: Computing methodologiesÂ Planning under uncertainty
â€ ccs: Software and its engineeringÂ Software maintenance tools
â€ ccs: Software and its engineeringÂ Software evolution
â€ ccs: Software and its engineeringÂ Automatic programming
1.Introduction

We use a Complex Numbers library that had the following edit -

+ class Complex {
+   float real;
+   float imag;
+   dict<string, string> metadata;
+ }
- tuple<float, float> create_complex(float a, float b)
+ Complex create_complex(float a, float b, dict metadata)

Modify the code repository in accordance with this change.

Figure 1.Task instruction to migrate a code repository due to an API change in the Complex Numbers library.
Figure 2.Overview of 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
.

The remarkable generative abilities of Large Language Models (LLMs)Â (Brown etÂ al., 2020; Chen etÂ al., 2021; Chowdhery etÂ al., 2022; Fried etÂ al., 2022; OpenAI, 2023; Touvron etÂ al., 2023) have opened new ways to automate coding tasks. Tools built on LLMs, such as Amazon Code WhispererÂ (Amazon Web Services, Inc., 2023), GitHub CopilotÂ (Github, Inc., 2023) and ReplitÂ (Replit, Inc., 2023), are now widely used to complete code

given a natural language intent and context of surrounding code, and also to perform code edits based on natural language instructionsÂ (Wilson-Thomas, [n.â€‰d.]). Such edits are typically done for small regions of code such as completing or editing the current line, or the body of the entire method.

While these tools help with the â€inner loopâ€ of software engineering where the developer is coding in the editor and editing a small region of code, there are several tasks in the â€outer loopâ€ of software engineering that involve the entire code repository. For example, if our code repository uses a library 
ğ¿
, and the API of library 
ğ¿
 changes from version 
ğ‘£
ğ‘›
 to version 
ğ‘£
ğ‘›
+
1
, we need to migrate our code repository to correctly invoke the revised version. Such a migration task involves making edits not only to all the regions of repository that make calls to the relevant APIs in library 
ğ¿
, but also to regions of the repository (across file boundaries) having transitive syntactic and semantic dependencies on the updated code.

tuple<tuple<float, float>, dict> func(float a, float b) {
  string timestamp = GetTimestamp(DateTime.Now);
   var c = (create_complex(a,b), new Dictionary<string, string>()"time", timestamp);
  return c;
}
 	
Complex func(float a, float b) {
  String timestamp = GetTimestamp(DataTime.Now);
  dict_metadata = new Dictionary<string, string>(){"time", timestamp};
  Complex c = create_complex(a, b, metadata);
  return c;
}

(a) Create.cs - Original	(b) Create.cs - Modified (seed edit)

void process(float a, float b, float k) {
  var c = func(a, b);
  Console.WriteLine(c[0][0], c[0][1]);
  float norm = compute_norm(c[0][0], c[0][1]);
  Console.WriteLine(norm * k);
}
 	
void process(float a, float b, float k) {
  Complex c = func(a, b);
  Console.WriteLine(c.real, c.imag);
  float norm = compute_norm(c.real, c.imag);
  Console.WriteLine(norm * k);
}

(c) Process.cs - Original	(d) Process.cs - Modified (derived edit)
Figure 3.Relevant code snippets from our repository.

This is illustrated in FigureÂ 2, which shows a change in the API for a Complex Numbers library. Our task is to migrate our code repository in accordance with this change. The left side of FigureÂ 3 shows relevant parts of our code repository that use the Complex Numbers library. Specifically, the file Create.cs has the method func, which invokes the create_complex method from the library, and Process.cs has the method process which invokes func.

We can pass the task description from FigureÂ 2 and the body of func to an LLM to generate the revised code for func as shown in the right side of FigureÂ 3. As seen, the LLM has correctly edited the invocation to the create_complex API so that it returns an object of type Complex instead of a tuple of two floating point values. Note that this edit has resulted in a change to the signature of the method func â€“ it now returns an object of type Complex. This necessitates changes to callers of method func such as the process method in file Process.cs, shown in the left-bottom of FigureÂ 3. Without a suitable change to the body of the process method, our code does not build! A suitable change to the process method which gets the repository to a consistent state, so that it builds without errors, is shown in the bottom-right of FigureÂ 3.

Problem Formulation. The migration task above is representative of a family of tasks that involve editing an entire code repository for various purposes such as fixing error reports from static analysis or testing, fixing a buggy coding pattern, refactoring, or adding type annotations or other specifications. Each of these tasks involves a set of seed specifications such as the one shown in FigureÂ 2, which are starting points for the code editing task. These seed specifications typically trigger other editing requirements on code, and such requirements need to be propagated across dependencies in the code repository to perform other edits across the repository to complete the coding task. Typically, such propagation of edits across dependencies is done manually.

Our goal is to construct a repository-level coding system, which automatically generates derived specifications for edits such as one required for the process method in FigureÂ 3, in order to get the repository to a valid state. Here, validity is defined with respect to an oracle, which can be instantiated to various ways of enforcing repository-level correctness conditions such as building without errors, passing static analysis, passing a type system or a set of tests, or passing a verification tool. We define an LLM-driven repository-level coding task as follows:

LLM-driven Repository-level Coding Task
Given a start state of a repository 
ğ‘…
ğ‘ 
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘Ÿ
â€‹
ğ‘¡
, a set of seed edit specifications 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
, an oracle 
Î˜
 such that 
Î˜
â€‹
(
ğ‘…
ğ‘ 
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘Ÿ
â€‹
ğ‘¡
)
=
ğ–³ğ—‹ğ—ğ–¾
, and an LLM 
ğ¿
, the goal of an LLM-driven repository-level coding task is to reach a repository state 
ğ‘…
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘Ÿ
â€‹
ğ‘”
â€‹
ğ‘’
â€‹
ğ‘¡
=
ğ¸
â€‹
ğ‘¥
â€‹
ğ‘’
â€‹
ğ‘
â€‹
ğ‘¢
â€‹
ğ‘¡
â€‹
ğ‘’
â€‹
ğ¸
â€‹
ğ‘‘
â€‹
ğ‘–
â€‹
ğ‘¡
â€‹
ğ‘ 
â€‹
(
ğ¿
,
ğ‘…
ğ‘ 
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘Ÿ
â€‹
ğ‘¡
,
ğ‘ƒ
)
 where 
ğ‘ƒ
 is a chain of edit specifications from 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
âˆª
Î”
ğ‘‘
â€‹
ğ‘’
â€‹
ğ‘Ÿ
â€‹
ğ‘–
â€‹
ğ‘£
â€‹
ğ‘’
â€‹
ğ‘‘
 where 
Î”
ğ‘‘
â€‹
ğ‘’
â€‹
ğ‘Ÿ
â€‹
ğ‘–
â€‹
ğ‘£
â€‹
ğ‘’
â€‹
ğ‘‘
 is a set of derived edit specifications so that 
Î˜
â€‹
(
ğ‘…
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘Ÿ
â€‹
ğ‘”
â€‹
ğ‘’
â€‹
ğ‘¡
)
=
ğ–³ğ—‹ğ—ğ–¾
.

Proposed Solution. In this paper, we propose a method to compute derived specifications by framing (LLM-driven) repository-level coding as a planning problem. Automated planningÂ (Ghallab etÂ al., 2004; Russell, 2010) aims to solve multi-step problems, where each step executes one action among many alternatives towards reaching a target state. It is used in a wide range of areas such as motion planningÂ (LaÂ Valle, 2011), autonomous drivingÂ (GonzÃ¡lez etÂ al., 2015), roboticsÂ (Karpas and Magazzeni, 2020) and theorem provingÂ (Bundy, 1988).

We present a task-agnostic framework, called 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
, which synthesizes a multi-step plan to solve the repository-level coding task. As shown in FigureÂ 2, the input to 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â is a repository, a task with seed specifications expressed through a natural language instruction or a set of initial code edits, a correctness oracle and an LLM. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â constructs a plan graph where each node in the graph identifies a code edit obligation that the LLM needs to discharge and an edge indicates that the target node needs to be discharged consequent to the source node. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
Â monitors the code edits and adaptively extends the plan graph. The edits 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
 follow from the task description, whereas the edits 
Î”
ğ‘‘
â€‹
ğ‘’
â€‹
ğ‘Ÿ
â€‹
ğ‘–
â€‹
ğ‘£
â€‹
ğ‘’
â€‹
ğ‘‘
 are identified and contextualized based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm. The merge block merges the code generated by the LLM into the repository. Once all the steps in a plan are completed, the repository is analyzed by the oracle. The task is completed if the oracle validates the repository. If it finds errors, the error reports are used as seed specifications for the next round of plan generation and execution.

Consider again, the example API migration task specified in FigureÂ 2 on code in FigureÂ 3. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 performs the edit of the method func using the instruction in FigureÂ 2 as a seed specification. By analyzing the code change between FigureÂ 3(a)â€“(b), it classifies the change as an escaping change as it affects signature of method func. The change may-impact analysis identifies that the caller(s) of func may be affected and hence, the adaptive planning algorithm uses caller-callee dependencies to infer a derived specification to edit the method process, which invokes func. Both the seed and derived changes are executed by creating suitable prompts for an LLM and the resulting code repository passes the oracle, i.e., builds without errors. Note that this is a simple example with only one-hop change propagation. In practice, the derived changes can themselves necessitate other changes transitively and 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 handles such cases.

A simpler alternative to our planning is to use the oracle to infer derived specifications. For example, the build system can find the error in the process method after the seed change is made in FigureÂ 3. This has important limitations. First, not all changes induce build errors even though they result in behavioral changes, e.g., changing the return value from True to False without changing the return type. Second, the build system is agnostic to cause-effect relationship when code breaks. For example, if the signature of an overriding method is changed as per the seed specification then a similar change is needed in the corresponding virtual method. However, the build system (when run on the intermediate, inconsistent snapshot of the repository) blames the overriding method for not conforming to the virtual method. NaÃ¯vely trying to fix the build error would end up reverting the seed change. The static analysis and planning components of 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 overcome these limitations. We experimentally compare 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 against a baseline that uses a build system to iteratively identify breaking changes and uses an LLM to fix them. Our quantitative and qualitative results show that 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 is superior to this kind of oracle-guided repair technique.

Contributions. To the best of our knowledge, the problem of monitoring the effects of code edits made by an LLM to a repository and systematically planning a chain of inter-dependent edits has not been identified and solved before.

In the space of repository-level coding tasks, two types of contexts have been found to be useful for prompting LLMs: (1) spatial context to provide cross-file information to the model using static analysisÂ (Pashakhanloo etÂ al., 2022; Shrivastava etÂ al., 2022; Ding etÂ al., 2022; Wei etÂ al., 2023b; Pei etÂ al., 2023b; Agrawal etÂ al., 2023; Shrivastava etÂ al., 2023; Liu etÂ al., 2023) or retrievalÂ (Xu etÂ al., 2021; Zhang etÂ al., 2023b), and (2) temporal context to condition the predictions on the history of edits to the repositoryÂ (Brody etÂ al., 2020; Reid and Neubig, 2022; Gupta etÂ al., 2023; Wei etÂ al., 2023a). Since 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 monitors the code changes and maintains a repository-wide dependency graph, we provide both these forms of contexts in a unified framework. The existing techniques assume that the next edit location is provided by the developer and do not account for the effect of an edit on the dependent code. In contrast, by inferring the impact of each change, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 propagates the changes to dependent code, paving a way to automate repository-level coding tasks through chain of edits.

In summary, we make the following contributions in this paper:

(1) 

We are the first to formalize the problem of automating repository-level coding tasks using LLMs, which requires analyzing the effects of code changes and propagating them across the repository. There are currently no systematic and scalable solutions to this problem.

(2) 

We frame repository-level coding as a planning problem and design a task-agnostic framework, called 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
, based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 synthesizes a multi-step chain of edits (plan) to be actuated by an LLM.

(3) 

We experiment with two repository-level coding tasks using the gpt-4-32k model: package migration for C# repositories and temporal code edits for Python repositories. We compare against baselines that use the oracles (a build system for C# and a static type checker for Python) for identifying derived edit specifications (in contrast to planning used in 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
). We use the same contextualization method as 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 in the baselines.

(4) 

Our results show that 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 has better match with the ground truth compared to baselines. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 is able to get 5/6 repositories to pass the validity checks, whereas the baselines cannot get any of the repositories to pass them. Except for the 2 proprietary repositories, we will release our data and evaluation scripts at https://aka.ms/CodePlan.

2.Design

In this section, we first give an overview of the 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 algorithm for automating repository-level coding tasks (SectionÂ 2.1). We then present the static analysis (SectionÂ 2.2) and the adaptive planning and plan execution (SectionÂ 2.3) components of 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
.

2.1.The 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 Algorithm
1/* Inputs: R is the source code of a repository, Delta_seeds is a set of seed edit specifications, Theta is an oracle and L is an LLM. */
3CodePlan(R, Delta_seeds, Theta, L):
4  let mutable G: PlanGraph = null in  
5  let mutable D: DependencyGraph = ConstructDependencyGraph(R) in 
6    while Delta_seeds is not empty 
7      IntializePlanGraph(G, Delta_seeds) 
8      AdaptivePlanAndExecute(R, D, G) 
9      Delta_seeds = Theta(R) 
11InitializePlanGraph(G, Delta_seeds): 
12  for each 
âŸ¨
B, I
âŸ©
 in Delta_seeds
13    AddRoot(G, 
âŸ¨
B, I, Pending
âŸ©
) 
15AdaptivePlanAndExecute(R, D, G): 
16  while G has Nodes with Pending status
17    let 
âŸ¨
B, I, Pending
âŸ©
 = GetNextPending(G) in
18    // First step: extract fragment of code
19    let Fragmemt = ExtractCodeFragment(B, R, I) in 
20    // Second step: gather context of the edit
21    let Context = GatherContext(B, R, D) in  
22    // Third step: use the LLM to get edited code fragment
23    let Prompt = MakePrompt(Fragment, I, Context) in 
24    let NewFragment = InvokeLLM(L, Prompt) in 
25    // Fourth step: merge the updated code fragment into R
26    let R = Merge(NewFragment, B, R) in 
27    let Labels = ClassifyChanges(Fragment, NewFragment) in 
28    let Dâ€™ = UpdateDependencyGraphâ€‹(D, Labels, Fragment, NewFragment, B) in 
29    // Fifth step: adaptively plan and propogate the effect of the edit on dependant code
30    let BlockRelationPairs â€‹â€‹â€‹=â€‹â€‹ GetAffectedBlocks(Labels, B, D, Dâ€™) in 
31      MarkCompleted(B, G) 
32      for each 
âŸ¨
Bâ€™, rel
âŸ©
 in BlockRelationPairs
33        let N = GetNode(B) in
34        let M = SelectOrAddNode(Bâ€™, Nil, Pending) in 
35          AddEdge(G, M, N, rel) 
36    D := Dâ€™
38GatherContext(B, R, D): 
39  let SC = GetSpatialContext(B, R) in
40  let TC = GetTemporalContext(G, B) in
41    (SC, TC) 
AlgorithmÂ 1 The 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 algorithm to automate repository-level coding tasks. The data structures and functions in Cyan and Orchid are explained in SectionÂ 2.2â€“Â 2.3 respectively.

The 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 algorithm (AlgorithmÂ 1) takes four inputs: (1) the source code of a repository 
ğ‘…
, (2) a set of seed edit specifications for the task in hand, 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
, (3) an oracle, 
Î˜
, and (4) an LLM, 
ğ¿
.

The core data structure maintained by the algorithm is a plan graph 
ğº
, a directed acyclic graph with multiple root nodes (lineÂ 4). Each node in the plan graph is a tuple 
âŸ¨
ğµ
,
ğ¼
,
ğ‘†
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘¡
â€‹
ğ‘¢
â€‹
ğ‘ 
âŸ©
, where 
ğµ
 is a block of code (that is, a sequence of code locations) in the repository 
ğ‘…
, 
ğ¼
 is an edit instruction (along the lines of the example shown in FigureÂ 2),

and 
ğ‘†
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘¡
â€‹
ğ‘¢
â€‹
ğ‘ 
 is either 
ğ‘
â€‹
ğ‘’
â€‹
ğ‘›
â€‹
ğ‘‘
â€‹
ğ‘–
â€‹
ğ‘›
â€‹
ğ‘”
 or 
ğ‘
â€‹
ğ‘œ
â€‹
ğ‘š
â€‹
ğ‘
â€‹
ğ‘™
â€‹
ğ‘’
â€‹
ğ‘¡
â€‹
ğ‘’
â€‹
ğ‘‘
.

The 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 algorithm also maintains a dependency graph 
ğ·
 (lineÂ 5). FigureÂ 4 illustrates the dependency graph structure. We will discuss it in details in SectionÂ 2.2.1. For now, it suffices to know that the dependency graph 
ğ·
 represents the syntactic and semantic dependency relations between code blocks in the repository 
ğ‘…
.

The loop at linesÂ 6â€“9 is executed until 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
 is non-empty. LineÂ 7 calls the InitializePlanGraph function (linesÂ 11â€“13) that adds all the changes in 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
 as root nodes of the plan graph. Each edit specification comprises of a code block 
ğµ
 and an edit instruction 
ğ¼
.

The status is set to pending for the root nodes (lineÂ 13). The function AdaptivePlanAndExecute is called at lineÂ 8 which executes the plan, updates the dependency graph with each code change and extends the plan as necessary. Once the plan graph is completely executed, the oracle 
Î˜
 is run on the repository. It returns error locations and diagnostic messages which form 
Î”
ğ‘ 
â€‹
ğ‘’
â€‹
ğ‘’
â€‹
ğ‘‘
â€‹
ğ‘ 
 for the next round. If the repository passes the oracleâ€™s checks then it returns an empty set and the 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 algorithm terminates.

We now discuss AdaptivePlanAndExecute, which is the main work horse. It iteratively picks each pending node and processes it. Processing a pending node with an edit specification for a block 
ğµ
 with edit instruction 
ğ¼
 involves the following five steps:

(1) 

The first step (lineÂ 19) is to extract the fragment of code to edit. Simply extracting code of the block 
ğµ
 loses information about relationship of 
ğµ
 with the surrounding code. Keeping the entire file on the other hand takes up prompt space and is often unnecessary. We found the surrounding context is most helpful when a block belongs to a class. For such blocks, we sketch the enclosing class. That is, in addition to the code of block 
ğµ
, we also keep declarations of the enclosing class and its members. As we discuss later, this sketched representation also helps us merge the LLMâ€™s output into a source code file more easily.

(2) 

The second step (lineÂ 21) is to gather the context of the edit. The context of the edit (lineÂ 38â€“41) consists of (a) spatial context, which contains related code such as methods called from the block 
ğµ
, and (b) temporal context, which contains the previous edits that caused the need to edit the block 
ğµ
. The temporal context is formed by edits along the paths from the root nodes of the plan graph to 
ğµ
.

(3) 

The third step (linesÂ 23â€“24) constructs a prompt for the edit using the fragment extracted in the first step, the instruction 
ğ¼
 from the edit specification and the context extracted in the second step, and invokes the LLM using the prompt to get the edited code fragment.

(4) 

The fourth step (linesÂ 26â€“28) merges the edited code back into the repository. Since the code is updated, many dependency relationships such as caller-callee, class hierarchy, etc. may need to change, and hence, this step also updates the dependency graph 
ğ·
.

(5) 

The fifth and final step (linesÂ 30â€“35) does adaptive planning to propagate the effects of the current edit on dependant code blocks. This involves classifying the change in the edited block, and depending on the type of change, picking the right dependencies in the dependency graph to traverse and locate affected blocks. For instance, if the edit of a method 
ğ‘š
 in the current block 
ğµ
 involves update to the signature of the method, then all callers of 
ğ‘š
 get affected (the scenario in FigureÂ 3). For each affected block 
ğµ
â€²
 and the dependency relation rel connecting 
ğµ
 to 
ğµ
â€²
 in the dependency graph, we get a pair 
âŸ¨
ğµ
â€²
,
rel
âŸ©
. If a node exists for 
ğµ
â€²
 in the plan graph and it is pending, then we add an edge from 
ğµ
 to 
ğµ
â€²
 labeled with rel to the plan graph. Otherwise, the edge is added to a newly created node for 
ğµ
â€²
 (lineÂ 34). The block 
ğµ
 is marked as completed (lineÂ 31).

2.2.Static Analysis Components
Figure 4.Illustration of the dependency graph annotated with relations as the edge labels.

We now turn our attention to the static analysis components used in 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
. We will cover all the data structures and functions in Cyan background from AlgorithmÂ 1.

2.2.1.Incremental Dependency Analysis

An LLM can be provided a code fragment and an instruction to edit it in a prompt. While the LLM may perform the desired edit accurately, analyzing the impact of the edit on the rest of the repository is outside the scope of the LLM call. We believe static analysis is well-suited to do this and propose an incremental dependency analysis for the same.

DependencyGraph. Dependency analysisÂ (Aho etÂ al., 2007) is used for tracking syntactic and semantic relations between code elements. In our case, we are interested in relations between import statements, methods, classes, field declarations and statements (excluding those that operate only on variables defined locally within the enclosing method). Formally, a dependency graph D 
=
(
ğ‘
,
ğ¸
)
 where 
ğ‘
 is a set of nodes representing the code blocks mentioned above and 
ğ¸
 is a set of labeled edges where the edge label gives the relation between the source and target nodes of the edge. FigureÂ 4 illustrates all the relations we track as labeled edges. The relations include (1) syntactic relations (ParentOf and ChildOf, Construct and ConstructedBy) between a block 
ğ‘
 and the block 
ğ‘
 that encloses 
ğ‘
 syntactically; a special case being a constructor and its enclosing class related by Construct and ConstructedBy, (2) import relations (Imports and ImportedBy) between an import statement and statements that use the imported modules, (3) inheritance relations (BaseClassOf and DerivedClassOf) between a class and its superclass, (4) method override relations (Overrides and OverridenBy) between an overriding method and the overriden method, (5) method invocation relations (Calls and CalledBy) between a statement and the method it calls, (6) object instantiation relations (Instantiates and InstantiatedBy) between a statement and the constructor of the object it creates, and (7) field use relations (Uses and UsedBy) between a statement and the declaration of a field it uses.

ConstructDependencyGraph. The dependency relations are derived across the source code spread over the repository through static analysis. We represent the source code of a repository as a forest of abstract syntax trees (ASTs) and add the dependency edges between AST sub-trees. A file-local analysis derives the syntactic and import relations. All other relations require an inter-class, inter-procedural analysis that can span file boundaries. In particular, we use the class hierarchy analysisÂ (Dean etÂ al., 1995) for deriving the semantic relations.

Atomic Change
 	
Label
	
Dependency Graph Update
	
Change May-Impact Analysis

Modification Changes

Body of method M
 	
MMB
	
Recompute the edges incident on the statements in the method body.
	
If an escaping object is modified then Rel(D, M, CalledBy) else Nil.


Signature of method M
 	
MMS
	
Recompute the edges incident on the method.
	
Rel(D, M, CalledBy), Rel(D, M, Overrides), Rel(D, M, OverriddenBy), Rel(
D
â€²
, M, Overrides), Rel(
D
â€²
, M, OverriddenBy)


Field F in class C
 	
MF
	
Recompute the edges incident on the field.
	
Rel(D, F, UsedBy), Rel(D, C, ConstructedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Declaration of class C
 	
MC
	
Recompute the edges incident on the class.
	
Rel(D, C, InstantiatedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf), Rel(
D
â€²
, C, BaseClassOf), Rel(
D
â€²
, C, DerivedClassOf)


Signature of constructor of class C
 	
MCC
	
No change.
	
Rel(D, C, InstantiatedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Import/Using statement I
 	
MI
	
Recompute the edges incident on the import statement.
	
Rel(D, I, ImportedBy)

Addition Changes

Method M in class C
 	
AM
	
Add new node and edges by analyzing the method. If C.M overrides a base class method B.M then redirect the Calls/CalledBy edges from B.M to C.M if the receiver object is of type C.
	
Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf), Rel(
D
â€²
, M, CalledBy)


Field F in class C
 	
AF
	
Add new node and edges by analyzing the field declaration.
	
Rel(D, C, ConstructedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Declaration of class C
 	
AC
	
Add new node and edges by analyzing the class declaration.
	
Nil


Constructor of class C
 	
ACC
	
Add new node and edges by analyzing the constructor.
	
Rel(D, C, InstantiatedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Import/Using statement I
 	
AI
	
Add new node and edges by analyzing the import statement.
	
Nil

Deletion Changes

Method M in class C
 	
DM
	
Remove the node for M and edges incident on M. If C.M overrides a base class method B.M then redirect the Calls/CalledBy edges from C.M to B.M if the receiver object is of type C.
	
Rel(D, M, CalledBy), Rel(D, M, Overrides), Rel(D, M, OverriddenBy)


Field F in class C
 	
DF
	
Remove the node of the field and edges incident on it.
	
Rel(D, F, UsedBy), Rel(D, C, ConstructedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Declaration of class C
 	
DC
	
Remove the node of the class and edges incident on it.
	
Rel(D, C, InstantiatedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Constructor of class C
 	
DCC
	
Remove the edges incident on the class due to object instatiations using the constructor.
	
Rel(D, C, InstantiatedBy), Rel(D, C, BaseClassOf), Rel(D, C, DerivedClassOf)


Import/Using statement I
 	
DI
	
Remove the node of the import statement and edges incident on it.
	
Rel(D, I, ImportedBy)
Table 1.Rules for updating the dependency graph and for change may-impact analysis for atomic changes. We refer to the dependency graphs before and after the updates by D and 
D
â€²
 respectively.

ClassifyChanges. As discussed in SectionÂ 2.1, in the fourth step, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 merges the code generated by the LLM into the repository. By pattern-matching the code before and after, we classify the code changes. TableÂ 1 (the first and second columns) gives the types of atomic changes and their labels. Broadly, the changes are organized as modification, addition and deletion changes, and further by which construct is changed. We distinguish between method body and method signature changes. Similarly, we distinguish between changes to a class declaration, to its constructor or to its fields. The changes to import statements or the statements that use imports are also identified. These are atomic changes. An LLM can make multiple simultaneous edits in the given code fragment, resulting in multiple atomic changes, all of which are identified by the ClassifyChanges function.

UpdateDependencyGraph. As code generated by the LLM is merged, the dependency relations associated with the code at the change site are re-analyzed. TableÂ 1 (the third column) gives the rules to update the dependency graph D to 
D
â€²
 based on the labels inferred by ClassifyChanges. For modification changes, we recompute the relations of the changed code except for constructors. A constructor is related to its enclosing class by a syntactic relation which does not have to be recomputed. For addition changes, new nodes and edges are created for the added code. Edges corresponding to syntactic relations are created in a straightforward manner. If a change simultaneously adds an element (an import, a method, a field or a class) and its uses, we create a node for the added element before analyzing the statements that use it. Addition of a method needs special handling as shown in the table: if an overriding method C.M is added then the Calls/CalledBy edges incident on the matching overriden method B.M are redirected to C.M if the call is issued on a receiver object of type C. The deletion of an overriding method requires an analogous treatment as stated in TableÂ 1. All other deletion changes require removing nodes and edges as stated in the table.

2.2.2.Change May-Impact Analysis

In the fifth step, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 identifies the code blocks that may have been impacted by the code change by the LLM. Let Rel(D, B, rel) be the set of blocks that are connected to a block B via relation rel in the dependency graph D. Let D and 
D
â€²
 be the dependency graph before and after the updates in TableÂ 1.

GetAffectedBlocks. The last column in TableÂ 1 tells us how to identify blocks affected by a code change for each type of change. When the body of a method M is edited, we perform escape analysisÂ (Choi etÂ al., 1999; Blanchet, 2003) to identify if any object accessible in the callers of M (an escaping object) has been affected by the change. If yes, the callers of M (identified through Rel(D, M, CalledBy)) are identified as affected blocks. Otherwise, the change is localized to the method and there are no affected blocks. If the signature of a method is edited, the callers and methods related to it through method-override relation in the inheritance hierarchy are affected. The signature change itself can affect the Overrides and OverridenBy relations, e.g., addition or deletion of the @Override access modifier. Therefore, the blocks related by these relations in the updated dependency graph 
D
â€²
 are also considered as affected as shown in TableÂ 1 (the row with MMS label). When a field F of a class C is modified, the statements that use F, the constructors of C and sub/super-classes of C are affected. When a class is modified, the methods that instantiate it and its sub/super-classes as per D and 
D
â€²
 are affected. A modification to a constructor has a similar rule except that such a change does not change inheritance relations and hence, only D is required. When an import statement I is modified, the statements that use the imported module are affected.

The addition and deletion changes are less complex than the modification changes, and their rules are designed along the same lines as discussed above. In the interest of space, we do not explain each of them step-by-step. We assume that there is no use of a newly added class or an import in the code. Therefore, adding them does not result in any affected blocks. In our experiments, we have found the rules in TableÂ 1 to be adequate. However, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 can be easily configured to accommodate variations of the rules in TableÂ 1 if necessary.

2.3.Adaptive Planning and Plan Execution

We now discuss the data structures and functions from AlgorithmÂ 1 in the Orchid background.

2.3.1.Adaptive Planning

Having identified the affected blocks (using GetAffectedBlocks), 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 creates change obligations that need to be discharged using an LLM to make the dependent code consistent with the change. As discussed in SectionÂ 2.1, this is an iterative process.

PlanGraph. A plan graph P 
=
(
ğ‘‚
,
ğ¶
)
 is a directed acyclic graph with a set of obligations 
ğ‘‚
, each of which is a triple 
âŸ¨
ğµ
,
ğ¼
,
ğ‘ 
â€‹
ğ‘¡
â€‹
ğ‘
â€‹
ğ‘¡
â€‹
ğ‘¢
â€‹
ğ‘ 
âŸ©
 where B is a block, I is an instruction and status is either pending or completed. An edge in 
ğ¶
 records the cause, the dependency relation between the blocks in the source and target obligations. In other words, the edge label identifies which Rel clause in a change may-impact rule in TableÂ 1 results in creation of the target obligation.

ExtractCodeFragment. As discussed in the first step in SectionÂ 2.1, simply extracting code for a block B is sub-optimal as it loses context. The ExtractCodeFragment function takes the whole class the code block belongs to, keeps the complete code for B and retains only declarations of the class and other class members. We found this to be useful because the names and types of the class and other members provide additional context to the LLM. Often times the LLM needs to make multiple simultaneous changes. For example, in some of our case studies, the LLM has to add a field declaration, take an argument to a constructor and use it within the constructor to initialize the field. Providing the sketch of the surrounding code as a code fragment to the LLM allows the LLM to make these changes at the right places. The code fragment extraction logic is implemented by traversing the AST and â€foldingâ€ away the subtrees (e.g., method bodies) that are sketched. As stated in SectionÂ 1, this sketched representation also allows us to place the LLM generated code back into the AST without ambiguity, even when there are multiple simultaneous changes.

GetSpatialContext. Spatial context in CodePlan refers to the arrangement and relationships of code blocks within a codebase, helping understand how classes, functions, variables, and modules are structured and interact. Itâ€™s crucial for making accurate code changes. CodePlan utilizes the dependency graph to extract spatial context, representing code as nodes and their relationships as edges. This graph enables CodePlan to navigate codebases, identify relevant code blocks, and maintain awareness of their spatial context. As a result, when generating code edits, the dependency graph empowers CodePlan to make context-aware code modifications that are consistent with the codeâ€™s spatial organization, enhancing the accuracy and reliability of its code editing capabilities.

GetTemporalContext. The plan graph records all change obligations and their inter-dependences. Extracting temporal context is accomplished by linearizing all paths from the root nodes of the plan graph to the target node. Each change is a pair of the code fragments before and after the change. The temporal context also states the â€causesâ€ (recorded as edge labels) that connect the target node with its predecessor nodes. For example, if a node A is connected to B with a CalledBy edge, then the temporal context for B is the before/after fragments for A and a statement that says that â€B calls Aâ€, which helps the LLM understand the cause-effect relation between the latest temporal change (change to A) and the current obligation (to make a change to B).

2.3.2.Plan Execution

ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 iteratively selects a pending node in the plan graph and invokes an LLM to discharge the change obligation.

MakePrompt. Having extracted the code fragment to be edited along with the relevant spatial and temporal context, we construct a prompt to pass to the LLM with the structure given below. We open with the task specific instructions 
p
1
 followed by listing the edits made in the repository so far 
p
2
 that are relevant to the fragment being edited. The next section 
p
3
 notes how each of the fragments present in 
p
2
 are related to the fragment to be edited. This is followed by the spatial context 
p
4
 and the fragment to the edited 
p
5
.

Prompt Template
p
1
Task Instructions: Your task is to 
â€¦


p
2
Earlier Code Changes (Temporal Context): These are edits that have been made in the code-base previously -

Edit 1:
Â Â Â Â Before: <<code_before>>
Â Â Â Â After: <<code_after>>

â‹¯


p
3
Causes for Change: The change is required due to -

<<code_to_be_edited>> is related to <<code_changed_earlier>> by <<cause>>
â‹¯


p
4
Related Code (Spatial Context): The following code maybe related -

<<related_code_block-1>>
â‹¯


p
5
Code to be Changed Next: The existing code is given below -

<<code_to_be_edited>>

Edit the â€Code to be Changed Nextâ€ and produce â€Changed Codeâ€ below. Edit the â€Code to be Changed Nextâ€ according to the â€Task Instructionsâ€ to make it consistent with the â€Earlier Code Changesâ€, â€Causes for Changeâ€ and â€Related Codeâ€. If no changes are needed, output â€No changes.â€

Oracle and Plan Iterations. Once all the nodes in the plan graph are marked as completed and no new nodes are added, an iteration of repository-level code edits is completed. As shown in FigureÂ 2, the oracle is invoked on the repository. If the oracle flags any errors (e.g., build errors), the error locations and diagnostic messages are added as seed changes for the next iteration and the adaptive planning resumes once again. If the oracle does not flag any errors, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 terminates.

3.Implementation

In this section, we provide a detailed overview of the implementation components that constitute the core of our method.

Dependency Graph Construction. At the core of the CodePlan methodology lies the Dependency Graph, which is instrumental in representing the intricate relationships between code blocks. To build this Dependency Graph from a code repository, we adopt a systematic approach. Initially, we parse all the code files within the repository, utilizing the tree-sitter libraryÂ (Brunsfeld etÂ al., 2023) to generate an AST-like structure. This structured representation simplifies the identification of various fundamental code blocks within the codebase. For instance, Figure 5 exemplifies an AST structure for a C# code snippet produced by tree-sitter. Code blocks are identified at different levels, including Classes, Methods, import statements, and non-class expressions. For instance, in FigureÂ 5, the subtree rooted at the class_declaration node corresponds to the SyncSubscriberTest class.

Figure 5.AST structure for a C# code snippet produced by tree-sitter.

Relation Identification in C#. In the context of C# repositories, the establishment of edges within the Dependency Graph involves the careful tracing of relationships within the AST. We have devised custom logic for each type of relationship outlined in Figure 4, encompassing vital connections such as caller-callee, overrides-overridden, base class-derived class, and others. To illustrate, for the Caller/Callee relationship, we search for invocation_expression nodes within the AST. Subsequently, we process the sub-tree beneath these nodes to resolve essential details such as the target class and the invoked methodâ€™s name. Armed with this information, we create Calls/CalledBy relation links between the code block initiating the method call and the corresponding method block within the target class. While we have implemented custom logic for these relations, itâ€™s important to note that alternative dependency analysis tools for C# such as Language Servers for C# (LSP)Â (Omn, [n.â€‰d.]), CodeQLÂ (Cod, [n.â€‰d.]), or similar solutions can also be integrated into our system, owing to its inherent flexibility.

Relation Identification in Python. For Python repositories, we use JediÂ (Jed, [n.â€‰d.]) - a static analysis tool which discovers references and declarations of symbols throughout the codebase. These capabilities are harnessed to identify edges in the Dependency Graph for relationships such caller-callee, overrides-overridden, and base class-derived class.

Integration of GPT-4 for Code Edits. CodePlan leverages the remarkable capabilities of GPT-4Â (OpenAI, 2023) to perform code edits effectively. During the construction of input data for the edit model, we meticulously provide temporal context, spatial context, and the actual code to be edited in the form of code snippets. These code snippets represent classes or methods that contain the edit site and are meticulously structured in a sketched representation, as stated in SectionÂ 2.1. This sketched representation ensures that the model is enriched with a substantial context for each edit site, significantly enhancing the quality and accuracy of the edits it generates.

Language Extensibility. While our current implementation proficiently supports C# and Python repositories, extending support to repositories in other programming languages is a straightforward endeavor. It primarily entails creating a dependency graph with the relations identified in Figure 4 and incorporating it into the CodePlan framework, thereby allowing for seamless adaptation to a diverse array of programming languages.

4.Related Work

LLMs for Coding Tasks. A multitude of LLMsÂ (Ahmad etÂ al., 2021; Wang etÂ al., 2021; Wang and Komatsuzaki, 2021; Brown etÂ al., 2020; Austin etÂ al., 2021; Chen etÂ al., 2021; Black etÂ al., 2022; Xu etÂ al., 2022; Chowdhery etÂ al., 2022; Fried etÂ al., 2022; OpenAI, 2023; Touvron etÂ al., 2023) have been trained on large-scale corpora of source code and natural language text. These have been used to accomplish a variety of coding tasks. A few examples of their use include program synthesisÂ (Li etÂ al., 2022; Nijkamp etÂ al., 2023), program repairÂ (Xia etÂ al., 2023; Jin etÂ al., 2023; Ahmed and Devanbu, 2023), vulnerability patchingÂ (Pearce etÂ al., 2022), inferring program invariantsÂ (Pei etÂ al., 2023a), test generationÂ (SchÃ¤fer etÂ al., 2023) and multi-task evaluationÂ (Tian etÂ al., 2023). However, these investigations are performed on curated examples that are extracted from their repositories and are meant to be accomplished with independent invocations of the LLM. We consider a different class of tasks posed at the scale of code repositories, where an LLM is called multiple times on different examples which are inter-dependent. We monitor the results of each LLM invocation within the repository-wide context to identify future code change obligations to get the repository to a consistent state, e.g., where the repository is free of build or runtime errors.

Automated Planning. Automated planningÂ (Ghallab etÂ al., 2004; Russell, 2010) is a well-studied topic in AI. Online planningÂ (Russell, 2010) is used when the effect of actions is not known and the state-space cannot be enumerated a priori. It requires monitoring the actions and plan extension. In our case, the edit actions are carried out by an LLM whose results cannot be predicted before-hand and the state-space is unbounded. As a consequence, our adaptive planning is an online algorithm where we monitor the actions and extend the plan through static analysis. In orthogonal directions, (Jiang etÂ al., 2023) uses an LLM to derive a plan given a natural language intent before generating code to solve complex coding problems and (Zhang etÂ al., 2023a) performs lookahead planning (tree search) to guide token-level decoding of code LMs. Planning in our work is based on analyzing dependency relations and changes to them as an LLM makes changes to a code repository.

Analysis of Code Changes. Static analysis is used for ensuring software quality. It is expensive to recompute the analysis results every time the code undergoes changes. The field of incremental program analysis offers techniques to recompute only the analysis results impacted by the change. Specialized algorithms have been developed for dataflow analysisÂ (Ryder, 1983; Arzt and Bodden, 2014), pointer analysisÂ (Yur etÂ al., 1999), symbolic executionÂ (Person etÂ al., 2011), bug detectionÂ (McPeak etÂ al., 2013) and type analysisÂ (Busi etÂ al., 2019). Program differencingÂ (Apiwattanapong etÂ al., 2004; Lahiri etÂ al., 2012; Kim etÂ al., 2012) and change impact analysisÂ (Arnold and Bohner, 1996; Jashki etÂ al., 2008) determine the differences in two program versions and the effect of a change on the rest of the program. The impact of changes has been studied for regression testingÂ (Ren etÂ al., 2004), analyzing refactoringsÂ (Dig etÂ al., 2006) and assisting in code reviewÂ (Alves etÂ al., 2014; Ge etÂ al., 2017). We analyze the code generated by an LLM and incrementally update the syntactic (e.g., parent-child) and dependency (e.g., caller-callee) relations. We further analyze the likely impact of those changes on related code blocks and create change obligations to be discharged by the LLM.

Spatial and Temporal Contextualization. As discussed in the Introduction, LLMs benefit from relevant context derived from other files in the repository and from past edits. We provide both these pieces of information to the LLM by tracking the code changes and dependency relations.

Learning Edit Patterns. Many approaches have been developed to learn edit patterns from past edits or commits in the form of rewrite rulesÂ (deÂ Sousa etÂ al., 2021), bug fixesÂ (Andersen and Lawall, 2010; Bader etÂ al., 2019), type changesÂ (Ketkar etÂ al., 2022), API migrationsÂ (Lamothe etÂ al., 2020; Xu etÂ al., 2019) and neural representations of editsÂ (Yin etÂ al., 2019). Approaches such as (Meng etÂ al., 2011) and (Meng etÂ al., 2013) synthesize context-aware edit scripts from user-provided examples and apply them in new contexts. Other approaches observe the user actions in an IDE to automate repetitive editsÂ (Miltner etÂ al., 2019) and temporally-related edit sequencesÂ (Zhang etÂ al., 2022). We do not aim to learn edit patterns and we do not assume similarities between edits. Our focus is to identify effects of code changes made by an LLM and to guide the LLM towards additional changes that become necessary.

5.Conclusions and Future Work

In this paper, we introduced 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
, a novel framework designed to tackle the challenges of repository-level coding tasks, which involve pervasive code modifications across large and inter-dependent codebases. 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 leverages incremental dependency analysis, change may-impact analysis, and adaptive planning to orchestrate multi-step edits guided by Large Language Models. We evaluated 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 on diverse code repositories with varying complexities and sizes, including both internal proprietary repositories and public GitHub repositories in C# and Python for migration and temporal edit tasks. Our results demonstrated that 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 outperforms baseline methods, achieving better alignment with the ground truth. In conclusion, 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 presents a promising approach to automating complex repository-level coding tasks, offering both productivity benefits and accuracy improvements. Its success in addressing these challenges opens up new possibilities for efficient and reliable software engineering practices.

While 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 has shown significant promise, there are several avenues for future research and enhancements. First, we aim to expand its applicability to a broader range of programming languages and code artifacts, including configuration files, metadata, and external dependencies, to provide a more holistic solution for repository-level editing. Additionally, we plan to explore further customization of 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
â€™s change may-impact analysis. This could involve incorporating task-specific impact analysis rules, either through rule-based methods or more advanced machine learning techniques, to fine-tune its editing decisions for specific coding tasks. Furthermore, we will address the challenge of handling dynamic dependencies, such as data flow dependencies, complex dynamic dispatching (via virtual functions and dynamic castings), algorithmic dependencies (e.g., when input lists are expected to be sorted), and various execution dependencies (such as multi-threading and distributed processing), to make 
ğ–¢ğ—ˆğ–½ğ–¾ğ–¯ğ—…ğ–ºğ—‡
 even more versatile in addressing a wider range of software engineering tasks.

References
(1)	
aud ([n.â€‰d.])	[n.â€‰d.].audiocraft.https://github.com/facebookresearch/audiocraft.
Cod ([n.â€‰d.])	[n.â€‰d.].CodeQL.https://github.com/github/codeql.
JAR ([n.â€‰d.])	[n.â€‰d.].JARVIS.https://github.com/microsoft/JARVIS.
Jed ([n.â€‰d.])	[n.â€‰d.].Jedi.https://github.com/davidhalter/jedi.
Omn ([n.â€‰d.])	[n.â€‰d.].OmniSharp.https://github.com/OmniSharp/csharp-language-server-protocol.
pyr ([n.â€‰d.])	[n.â€‰d.].Pyright.https://github.com/microsoft/pyright.
rep ([n.â€‰d.])	[n.â€‰d.].Reactive Streams TCK.https://github.com/reactive-streams/reactive-streams-dotnet/tree/master/src/tck.
whi ([n.â€‰d.])	[n.â€‰d.].whisper.https://github.com/openai/whisper.
Agrawal etÂ al. (2023)	LakshyaÂ A Agrawal, Aditya Kanade, Navin Goyal, ShuvenduÂ K. Lahiri, and SriramÂ K. Rajamani. 2023.Guiding Language Models of Code with Global Context using Monitors.arXiv:2306.10763Â [cs.CL]
Ahmad etÂ al. (2021)	WasiÂ Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2021.Unified Pre-training for Program Understanding and Generation.arXiv:2103.06333Â [cs.CL]
Ahmed and Devanbu (2023)	Toufique Ahmed and Premkumar Devanbu. 2023.Better patching using LLM prompting, via Self-Consistency.arXiv:2306.00108Â [cs.SE]
Aho etÂ al. (2007)	AlfredÂ V Aho, Ravi Sethi, JeffreyÂ D Ullman, etÂ al. 2007.Compilers: principles, techniques, and tools. Vol.Â 2.Addison-wesley Reading.
Alves etÂ al. (2014)	Everton L.Â G. Alves, Myoungkyu Song, and Miryung Kim. 2014.RefDistiller: A Refactoring Aware Code Review Tool for Inspecting Manual Refactoring Edits. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (Hong Kong, China) (FSE 2014). Association for Computing Machinery, New York, NY, USA, 751â€“754.https://doi.org/10.1145/2635868.2661674
Amazon Web Services, Inc. (2023)	Amazon Web Services, Inc. 2023.Amazon Code Whisperer - AI Code Generator.https://aws.amazon.com/codewhisperer/Accessed: July 25, 2023.
Andersen and Lawall (2010)	Jesper Andersen and JuliaÂ L Lawall. 2010.Generic patch inference.Automated software engineering 17 (2010), 119â€“148.
Apiwattanapong etÂ al. (2004)	Taweesup Apiwattanapong, Alessandro Orso, and MaryÂ Jean Harrold. 2004.A differencing algorithm for object-oriented programs. In Proceedings. 19th International Conference on Automated Software Engineering, 2004. IEEE, 2â€“13.
Arnold and Bohner (1996)	RS Arnold and SA Bohner. 1996.An introduction to software change impact analysis.Software Change Impact Analysis (1996), 1â€“26.
Arzt and Bodden (2014)	Steven Arzt and Eric Bodden. 2014.Reviser: efficiently updating IDE-/IFDS-based data-flow analyses in response to incremental program changes. In Proceedings of the 36th International Conference on Software Engineering. 288â€“298.
Austin etÂ al. (2021)	Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021.Program Synthesis with Large Language Models.http://arxiv.org/abs/2108.07732arXiv:2108.07732 [cs].
Bader etÂ al. (2019)	Johannes Bader, Andrew Scott, Michael Pradel, and Satish Chandra. 2019.Getafix: Learning to Fix Bugs Automatically.Proc. ACM Program. Lang. 3, OOPSLA, Article 159 (Oct. 2019), 27Â pages.https://doi.org/10.1145/3360585
Black etÂ al. (2022)	Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, and others. 2022.Gpt-neox-20b: An open-source autoregressive language model.arXiv preprint arXiv:2204.06745 (2022).
Blanchet (2003)	Bruno Blanchet. 2003.Escape analysis for JavaTM: Theory and practice.ACM Transactions on Programming Languages and Systems (TOPLAS) 25, 6 (2003), 713â€“775.
Brody etÂ al. (2020)	Shaked Brody, Uri Alon, and Eran Yahav. 2020.A structural model for contextual code changes.4, OOPSLA (Nov. 2020).https://doi.org/10.1145/3428283Publisher Copyright: Â© 2020 Owner/Author..
Brown etÂ al. (2020)	Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al. 2020.Language models are few-shot learners.Advances in neural information processing systems 33 (2020), 1877â€“1901.
Brunsfeld etÂ al. (2023)	Max Brunsfeld, Andrew Hlynskyi, Patrick Thomson, Josh Vera, Phil Turnbull, Timothy Clem, Douglas Creager, Andrew Helwer, Rob Rix, Hendrik VanÂ Antwerpen, Michael Davis, , Ika, Tuáº¥n-Anh Nguyá»…n, Stafford Brunk, Niranjan Hasabnis, Bfredl, Mingkai Dong, Matt Massicotte, Jonathan Arnett, Vladimir Panteleev, Steven Kalt, Kolja Lampe, Alex Pinkus, Mark Schmitz, Matthew Krupcale, Narpfel, Santos Gallegos, Vicent MartÃ­, and , Edgar. 2023.tree-sitter/tree-sitter: v0.20.8.https://doi.org/10.5281/ZENODO.4619183
Bundy (1988)	Alan Bundy. 1988.The use of explicit plans to guide inductive proofs. In 9th International Conference on Automated Deduction: Argonne, Illinois, USA, May 23â€“26, 1988 Proceedings 9. Springer, 111â€“120.
Busi etÂ al. (2019)	Matteo Busi, Pierpaolo Degano, and Letterio Galletta. 2019.Using standard typing algorithms incrementally. In NASA Formal Methods: 11th International Symposium, NFM 2019, Houston, TX, USA, May 7â€“9, 2019, Proceedings 11. Springer, 106â€“122.
Chen etÂ al. (2021)	Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde deÂ Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, and others. 2021.Evaluating large language models trained on code.arXiv preprint arXiv:2107.03374 (2021).
Choi etÂ al. (1999)	Jong-Deok Choi, Manish Gupta, Mauricio Serrano, VugranamÂ C Sreedhar, and Sam Midkiff. 1999.Escape analysis for Java.Acm Sigplan Notices 34, 10 (1999), 1â€“19.
Chowdhery etÂ al. (2022)	Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, HyungÂ Won Chung, Charles Sutton, Sebastian Gehrmann, etÂ al. 2022.Palm: Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311 (2022).
deÂ Sousa etÂ al. (2021)	ReudismamÂ Rolim de Sousa, Gustavo Soares, Rohit Gheyi, Titus Barik, and Loris Dâ€™Antoni. 2021.Learning Quick Fixes from Code Repositories. In SBES â€™21: 35th Brazilian Symposium on Software Engineering, Joinville, Santa Catarina, Brazil, 27 September 2021 - 1 October 2021, CristianoÂ D. Vasconcellos, KarinaÂ Girardi Roggia, Vanessa Collere, and Paulo Bousfield (Eds.). ACM, 74â€“83.https://doi.org/10.1145/3474624.3474650
Dean etÂ al. (1995)	Jeffrey Dean, David Grove, and Craig Chambers. 1995.Optimization of object-oriented programs using static class hierarchy analysis. In ECOOPâ€™95â€”Object-Oriented Programming, 9th European Conference, Ã…arhus, Denmark, August 7â€“11, 1995 9. Springer, 77â€“101.
Dig etÂ al. (2006)	Danny Dig, Can Comertoglu, Darko Marinov, and Ralph Johnson. 2006.Automated detection of refactorings in evolving components. In ECOOP 2006â€“Object-Oriented Programming: 20th European Conference, Nantes, France, July 3-7, 2006. Proceedings 20. Springer, 404â€“428.
Ding etÂ al. (2022)	Yangruibo Ding, Zijian Wang, WasiÂ Uddin Ahmad, MuraliÂ Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. 2022.CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context.http://arxiv.org/abs/2212.10007arXiv:2212.10007 [cs].
Fried etÂ al. (2022)	Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.Incoder: A generative model for code infilling and synthesis.arXiv preprint arXiv:2204.05999 (2022).
Ge etÂ al. (2017)	Xi Ge, Saurabh Sarkar, Jim Witschey, and Emerson Murphy-Hill. 2017.Refactoring-aware code review. In 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). IEEE, 71â€“79.
Ghallab etÂ al. (2004)	Malik Ghallab, Dana Nau, and Paolo Traverso. 2004.Automated Planning: theory and practice.Elsevier.
Github, Inc. (2023)	Github, Inc. 2023.GitHub Copilot: Your AI pair programmer.https://github.com/features/copilotAccessed: July 25, 2023.
GonzÃ¡lez etÂ al. (2015)	David GonzÃ¡lez, JoshuÃ© PÃ©rez, Vicente MilanÃ©s, and Fawzi Nashashibi. 2015.A review of motion planning techniques for automated vehicles.IEEE Transactions on intelligent transportation systems 17, 4 (2015), 1135â€“1145.
Gupta etÂ al. (2023)	Priyanshu Gupta, Avishree Khare, Yasharth Bajpai, Saikat Chakraborty, Sumit Gulwani, Aditya Kanade, Arjun Radhakrishna, Gustavo Soares, and Ashish Tiwari. 2023.GrACE: Generation using Associated Code Edits.arXiv:2305.14129Â [cs.SE]
Jashki etÂ al. (2008)	Mohammad-Amin Jashki, Reza Zafarani, and Ebrahim Bagheri. 2008.Towards a more efficient static software change impact analysis method. In Proceedings of the 8th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering. 84â€“90.
Jiang etÂ al. (2023)	Xue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. 2023.Self-planning Code Generation with Large Language Model.arXiv:2303.06689Â [cs.SE]
Jin etÂ al. (2023)	Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023.InferFix: End-to-End Program Repair with LLMs.arXiv preprint arXiv:2303.07263 (2023).
Karpas and Magazzeni (2020)	Erez Karpas and Daniele Magazzeni. 2020.Automated planning for robotics.Annual Review of Control, Robotics, and Autonomous Systems 3 (2020), 417â€“439.
Ketkar etÂ al. (2022)	Ameya Ketkar, Oleg Smirnov, Nikolaos Tsantalis, Danny Dig, and Timofey Bryksin. 2022.Inferring and applying type changes. In Proceedings of the 44th International Conference on Software Engineering. 1206â€“1218.
Kim etÂ al. (2012)	Miryung Kim, David Notkin, Dan Grossman, and Gary Wilson. 2012.Identifying and summarizing systematic code changes via rule inference.IEEE Transactions on Software Engineering 39, 1 (2012), 45â€“62.
LaÂ Valle (2011)	StevenÂ M LaÂ Valle. 2011.Motion planning.IEEE Robotics & Automation Magazine 18, 2 (2011), 108â€“118.
Lahiri etÂ al. (2012)	ShuvenduÂ K Lahiri, Chris Hawblitzel, Ming Kawaguchi, and Henrique RebÃªlo. 2012.Symdiff: A language-agnostic semantic diff tool for imperative programs. In Computer Aided Verification: 24th International Conference, CAV 2012, Berkeley, CA, USA, July 7-13, 2012 Proceedings 24. Springer, 712â€“717.
Lamothe etÂ al. (2020)	Maxime Lamothe, Weiyi Shang, and Tse-HsunÂ Peter Chen. 2020.A3: Assisting android api migrations using code examples.IEEE Transactions on Software Engineering 48, 2 (2020), 417â€“431.
Li etÂ al. (2022)	Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, RÃ©mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, AgustinÂ Dal Lago, Thomas Hubert, Peter Choy, Cyprien deÂ Masson dâ€™Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, DanielÂ J. Mankowitz, EsmeÂ Sutherland Robson, Pushmeet Kohli, NandoÂ de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022.Competition-level code generation with AlphaCode.Science 378, 6624 (2022), 1092â€“1097.https://doi.org/10.1126/science.abq1158_eprint: https://www.science.org/doi/pdf/10.1126/science.abq1158.
Liu etÂ al. (2023)	Tianyang Liu, Canwen Xu, and Julian McAuley. 2023.RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems.arXiv:2306.03091Â [cs.CL]
McPeak etÂ al. (2013)	Scott McPeak, Charles-Henri Gros, and MuraliÂ Krishna Ramanathan. 2013.Scalable and incremental software bug detection. In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering. 554â€“564.
Meng etÂ al. (2011)	Na Meng, Miryung Kim, and KathrynÂ S McKinley. 2011.Sydit: Creating and applying a program transformation from an example. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering. 440â€“443.
Meng etÂ al. (2013)	Na Meng, Miryung Kim, and KathrynÂ S McKinley. 2013.LASE: locating and applying systematic edits by learning from examples. In 2013 35th International Conference on Software Engineering (ICSE). IEEE, 502â€“511.
Miltner etÂ al. (2019)	Anders Miltner, Sumit Gulwani, Vu Le, Alan Leung, Arjun Radhakrishna, Gustavo Soares, Ashish Tiwari, and Abhishek Udupa. 2019.On the fly synthesis of edit suggestions.Proceedings of the ACM on Programming Languages 3, OOPSLA (2019), 1â€“29.
Nijkamp etÂ al. (2023)	Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023.CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh International Conference on Learning Representations.https://openreview.net/forum?id=iaYcJKpY2B_
OpenAI (2023)	OpenAI. 2023.GPT-4 Technical Report.arXiv:2303.08774Â [cs.CL]
Papineni etÂ al. (2002)	Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311â€“318.
Pashakhanloo etÂ al. (2022)	Pardis Pashakhanloo, Aaditya Naik, Yuepeng Wang, Hanjun Dai, Petros Maniatis, and Mayur Naik. 2022.Codetrek: Flexible modeling of code using an extensible relational representation.(2022).
Pearce etÂ al. (2022)	Hammond Pearce, Benjamin Tan, Baleegh Ahmad, Ramesh Karri, and Brendan Dolan-Gavitt. 2022.Examining Zero-Shot Vulnerability Repair with Large Language Models. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 1â€“18.
Pei etÂ al. (2023b)	Hengzhi Pei, Jinman Zhao, Leonard Lausen, Sheng Zha, and George Karypis. 2023b.Better context makes better code language models: A case study on function call argument completion. In AAAI.https://www.amazon.science/publications/better-context-makes-better-code-language-models-a-case-study-on-function-call-argument-completion
Pei etÂ al. (2023a)	Kexin Pei, David Bieber, Kensen Shi, Charles Sutton, and Pengcheng Yin. 2023a.Can Large Language Models Reason about Program Invariants?(2023).
Person etÂ al. (2011)	Suzette Person, Guowei Yang, Neha Rungta, and Sarfraz Khurshid. 2011.Directed incremental symbolic execution.Acm Sigplan Notices 46, 6 (2011), 504â€“515.
Reid and Neubig (2022)	Machel Reid and Graham Neubig. 2022.Learning to Model Editing Processes.https://doi.org/10.48550/ARXIV.2205.12374
Ren etÂ al. (2004)	Xiaoxia Ren, Fenil Shah, Frank Tip, BarbaraÂ G Ryder, and Ophelia Chesley. 2004.Chianti: a tool for change impact analysis of java programs. In Proceedings of the 19th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications. 432â€“448.
Replit, Inc. (2023)	Replit, Inc. 2023.Replit.https://replit.com/Accessed: July 25, 2023.
Russell (2010)	StuartÂ J Russell. 2010.Artificial intelligence a modern approach.Pearson Education, Inc.
Ryder (1983)	BarbaraÂ G Ryder. 1983.Incremental data flow analysis. In Proceedings of the 10th ACM SIGACT-SIGPLAN symposium on Principles of programming languages. 167â€“176.
SchÃ¤fer etÂ al. (2023)	Max SchÃ¤fer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. 2023.Adaptive test generation using a large language model.arXiv preprint arXiv:2302.06527 (2023).
Shrivastava etÂ al. (2023)	Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. 2023.RepoFusion: Training Code Models to Understand Your Repository.arXiv:2306.10998Â [cs.LG]
Shrivastava etÂ al. (2022)	Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. 2022.Repository-level prompt generation for large language models of code.arXiv preprint arXiv:2206.12839 (2022).
Tian etÂ al. (2023)	Haoye Tian, Weiqi Lu, TszÂ On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein, and TegawendÃ©Â F. BissyandÃ©. 2023.Is ChatGPT the Ultimate Programming Assistant â€“ How far is it?arXiv:2304.11938Â [cs.SE]
Touvron etÂ al. (2023)	Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, CristianÂ Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023.Llama 2: Open Foundation and Fine-Tuned Chat Models.arXiv:2307.09288Â [cs.CL]
Wang and Komatsuzaki (2021)	Ben Wang and Aran Komatsuzaki. 2021.GPT-J-6B: A 6 billion parameter autoregressive language model.
Wang etÂ al. (2021)	Yue Wang, Weishi Wang, ShafiqÂ R. Joty, and Steven C.Â H. Hoi. 2021.CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation.ArXiv abs/2109.00859 (2021).
Wei etÂ al. (2023a)	Jiayi Wei, Greg Durrett, and Isil Dillig. 2023a.Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing.arXiv:2305.18584Â [cs.SE]
Wei etÂ al. (2023b)	Jiayi Wei, Greg Durrett, and Isil Dillig. 2023b.TypeT5: Seq2seq Type Inference using Static Analysis.arXiv:2303.09564Â [cs.SE]
Wilson-Thomas ([n.â€‰d.])	Mark Wilson-Thomas. [n.â€‰d.].GitHub Copilot chat for Visual Studio 2022.https://devblogs.microsoft.com/visualstudio/github-copilot-chat-for-visual-studio-2022/Accessed: July 25, 2023.
Xia etÂ al. (2023)	ChunqiuÂ Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023.Automated program repair in the era of large pre-trained language models. In Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery.
Xu etÂ al. (2022)	FrankÂ F. Xu, Uri Alon, Graham Neubig, and VincentÂ Josua Hellendoorn. 2022.A Systematic Evaluation of Large Language Models of Code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming (MAPS 2022). Association for Computing Machinery, New York, NY, USA, 1â€“10.https://doi.org/10.1145/3520312.3534862event-place: San Diego, CA, USA.
Xu etÂ al. (2021)	FrankÂ F Xu, Junxian He, Graham Neubig, and VincentÂ J Hellendoorn. 2021.Capturing structural locality in non-parametric language models.arXiv preprint arXiv:2110.02870 (2021).
Xu etÂ al. (2019)	Shengzhe Xu, Ziqi Dong, and Na Meng. 2019.Meditor: inference and application of API migration edits. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC). IEEE, 335â€“346.
Yin etÂ al. (2019)	Pengcheng Yin, Graham Neubig, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. 2019.Learning to Represent Edits. In ICLR 2019.https://www.microsoft.com/en-us/research/publication/learning-to-represent-edits/arXiv:1810.13337 [cs.LG].
Yur etÂ al. (1999)	Jyh-shiarn Yur, BarbaraÂ G Ryder, and WilliamÂ A Landi. 1999.An incremental flow-and context-sensitive pointer aliasing analysis. In Proceedings of the 21st International conference on Software Engineering. 442â€“451.
Zhang etÂ al. (2023b)	Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023b.RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation.arXiv preprint arXiv:2303.12570 (2023).
Zhang etÂ al. (2023a)	Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, JoshuaÂ B. Tenenbaum, and Chuang Gan. 2023a.Planning with Large Language Models for Code Generation.arXiv:2303.05510Â [cs.LG]
Zhang etÂ al. (2022)	Yuhao Zhang, Yasharth Bajpai, Priyanshu Gupta, Ameya Ketkar, Miltiadis Allamanis, Titus Barik, Sumit Gulwani, Arjun Radhakrishna, Mohammad Raza, Gustavo Soares, and Ashish Tiwari. 2022.Overwatch: Learning patterns in code edit sequences.Proc. ACM Program. Lang. 6, OOPSLA2 (2022), 395â€“423.https://doi.org/10.1145/3563302
â—„  Feeling
lucky? Conversion
report Report
an issue ViewÂ original
onÂ arXivâ–º
Copyright Privacy Policy Generated on Wed Feb 28 05:01:14 2024 by LaTeXML