---
title: Introduction | ell  documentation
description: ell is a lightweight prompt engineering library treating prompts as functions. It provides tools for versioning, monitoring, and visualization of language model programs.
url: https://docs.ell.so/index.html
timestamp: 2025-01-20T16:15:38.796Z
domain: docs.ell.so
path: index.html
---

# Introduction | ell  documentation


ell is a lightweight prompt engineering library treating prompts as functions. It provides tools for versioning, monitoring, and visualization of language model programs.


## Content

ell: The Language Model Programming Library[¶](https://docs.ell.so/index.html#ell-the-language-model-programming-library "Link to this heading")
------------------------------------------------------------------------------------------------------------------------------------------------

[![Image 27: Install](https://img.shields.io/badge/get_started-blue)](https://docs.ell.so/installation)[![Image 28: Discord](https://dcbadge.limes.pink/api/server/vWntgU52Xb?style=flat)](https://discord.gg/vWntgU52Xb)[![Image 29: X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/wgussml)](https://twitter.com/wgussml)[![Image 30: Jobs Board](https://img.shields.io/badge/jobs-board-green)](https://jobs.ell.so/)

`ell` is a lightweight prompt engineering library treating prompts as functions. After years of building and using language models at OpenAI and in the startup ecosystem, `ell` was designed from the following principles:

Prompts are programs, not strings[¶](https://docs.ell.so/index.html#prompts-are-programs-not-strings "Link to this heading")
----------------------------------------------------------------------------------------------------------------------------

```
import ell
@ell.simple(model="gpt-4o-mini")
def hello(world: str):
"""You are a helpful assistant""" # System prompt
    name = world.capitalize()
    return f"Say hello to {name}!" # User prompt
hello("sam altman") # just a str, "Hello Sam Altman! ..."
```

[![Image 31: ell demonstration](https://docs.ell.so/_images/gif1.webp)](https://docs.ell.so/_images/gif1.webp)Prompts aren’t just strings; they are all the code that leads to strings being sent to a language model. In ell, we think of one particular way of using a language model as a discrete subroutine called a **language model program** (LMP).

LMPs are fully encapsulated functions that produce either a string prompt or a list of messages to be sent to various multimodal language models. This encapsulation creates a clean interface for users, who only need to be aware of the required data specified to the LMP.

Prompt engineering is an optimization process[¶](https://docs.ell.so/index.html#prompt-engineering-is-an-optimization-process "Link to this heading")
-----------------------------------------------------------------------------------------------------------------------------------------------------

The process of prompt engineering involves many iterations, similar to the optimization processes in machine learning. Because LMPs are just functions, `ell` provides rich tooling for this process.

[![Image 32: ell demonstration](https://docs.ell.so/_images/versions_small.webp)](https://docs.ell.so/_images/versions_small.webp)`ell` provides **automatic versioning and serialization of prompts** through static and dynamic analysis and `gpt-4o-mini` **autogenerated commit messages** directly to a _local store_. This process is similar to checkpointing in a machine learning training loop, but it doesn’t require any special IDE or editor - it’s all done with regular Python code.

```
 import ell
 ell.init(store='./logdir')  # Versions your LMPs and their calls
 # ... define your lmps
 hello("strawberry") # the source code of the LMP the call is saved to the store
```

Tools for monitoring, versioning, and visualization[¶](https://docs.ell.so/index.html#tools-for-monitoring-versioning-and-visualization "Link to this heading")
---------------------------------------------------------------------------------------------------------------------------------------------------------------

[![Image 33: ell demonstration](https://docs.ell.so/_images/ell_studio_better.webp)](https://docs.ell.so/_images/ell_studio_better.webp)

```
ell-studio--storage./logdir
```

Prompt engineering goes from a dark art to a science with the right tools. **Ell Studio is a local, open source tool for prompt version control, monitoring, visualization**. With Ell Studio you can empiricize your prompt optimziation process over time and catch regressions before its too late.

Test-time compute is important[¶](https://docs.ell.so/index.html#test-time-compute-is-important "Link to this heading")
-----------------------------------------------------------------------------------------------------------------------

Going from a demo to something that actually works, often means prompt engineering solutions that involve multiple calls to a language model. By forcing a functional decomposition of the problem, `ell` makes it **easy to implement test-time compute leveraged techniques in a readable and modular way.**

[![Image 34: ell demonstration](https://docs.ell.so/_images/compositionality.webp)](https://docs.ell.so/_images/compositionality.webp)

```
import ell
from typing import List
@ell.simple(model="gpt-4o-mini", temperature=1.0, n=10)
def write_ten_drafts(idea : str):
"""You are an adept story writer. The story should only be 3 paragraphs"""
   return f"Write a story about {idea}."
@ell.simple(model="gpt-4o", temperature=0.1)
def choose_the_best_draft(drafts : List[str]):
"""You are an expert fiction editor."""
   return f"Choose the best draft from the following list: {'\n'.join(drafts)}."
drafts = write_ten_drafts(idea)
best_draft = choose_the_best_draft(drafts) # Best of 10 sampling.
```

Every call to a language model is valuable[¶](https://docs.ell.so/index.html#every-call-to-a-language-model-is-valuable "Link to this heading")
-----------------------------------------------------------------------------------------------------------------------------------------------

Every call to a language model is worth its weight in credits. In practice, LLM invocations are used for fine tuning, distillation, k-shot prompting, reinforcement learning from human feedback, and more. A good prompt engineering system should capture these as first class concepts.

[![Image 35: ell demonstration](https://docs.ell.so/_images/invocations.webp)](https://docs.ell.so/_images/invocations.webp)In addition to storing the source code of every LMP, `ell` optionally saves every call to a language model locally. This allows you to generate invocaiton datasets, compare LMP outputs by version, and generally do more with the full spectrum of prompt engineering artifacts.

Complexity when you need it, simplicity when you don’t[¶](https://docs.ell.so/index.html#complexity-when-you-need-it-simplicity-when-you-don-t "Link to this heading")
----------------------------------------------------------------------------------------------------------------------------------------------------------------------

Using language models is **just passing strings around, except when it’s not.**

```
import ell
@ell.tool()
def scrape_website(url : str):
   return requests.get(url).text
@ell.complex(model="gpt-5-omni", tools=[scrape_website])
def get_news_story(topic : str):
   return [
      ell.system("""Use the web to find a news story about the topic"""),
      ell.user(f"Find a news story about {topic}.")
   ]
message_response = get_news_story("stock market")
if message_response.tool_calls:
   for tool_call in message_response.tool_calls:
      #...
if message_response.text:
   print(message_response.text)
if message_response.audio:
   # message_response.play_audio() supprot for multimodal outputs will work as soon as the LLM supports it
   pass
```

Using `@ell.simple` causes LMPs to yield **simple string outputs.** But when more complex or multimodal output is needed, `@ell.complex` can be used to yield `Message` objects responses from language mdoels.

Multimodality should be first class[¶](https://docs.ell.so/index.html#multimodality-should-be-first-class "Link to this heading")
---------------------------------------------------------------------------------------------------------------------------------

LLMs can process and generate various types of content, including text, images, audio, and video. Prompt engineering with these data types should be as easy as it is with text.

```
from PIL import Image
import ell
@ell.simple(model="gpt-4o", temperature=0.1)
def describe_activity(image: Image.Image):
   return [
      ell.system("You are VisionGPT. Answer <5 words all lower case."),
      ell.user(["Describe what the person in the image is doing:", image])
   ]
# Capture an image from the webcam
describe_activity(capture_webcam_image()) # "they are holding a book"
```

[![Image 36: ell demonstration](https://docs.ell.so/_images/multimodal_compressed.webp)](https://docs.ell.so/_images/multimodal_compressed.webp)`ell` supports rich type coercion for multimodal inputs and outputs. You can use PIL images, audio, and other multimodal inputs inline in `Message` objects returned by LMPs.

Prompt engineering libraries shouldn’t interfere with your workflow[¶](https://docs.ell.so/index.html#prompt-engineering-libraries-shouldn-t-interfere-with-your-workflow "Link to this heading")
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

`ell` is designed to be a lightweight and unobtrusive library. It doesn’t require you to change your coding style or use special editors.

[![Image 37: ell demonstration](https://docs.ell.so/_images/useitanywhere_compressed.webp)](https://docs.ell.so/_images/useitanywhere_compressed.webp)You can continue to use regular Python code in your IDE to define and modify your prompts, while leveraging ell’s features to visualize and analyze your prompts. Migrate from langchain to `ell` one function at a time.

* * *

To get started with `ell`, see the [Getting Started](https://docs.ell.so/getting_started.html) section, or go onto [Installation](https://docs.ell.so/installation.html) and get ell installed.

## Metadata

```json
{
  "title": "Introduction | ell  documentation",
  "description": "ell is a lightweight prompt engineering library treating prompts as functions. It provides tools for versioning, monitoring, and visualization of language model programs.",
  "url": "https://docs.ell.so/index.html",
  "content": "ell: The Language Model Programming Library[¶](https://docs.ell.so/index.html#ell-the-language-model-programming-library \"Link to this heading\")\n------------------------------------------------------------------------------------------------------------------------------------------------\n\n[![Image 27: Install](https://img.shields.io/badge/get_started-blue)](https://docs.ell.so/installation)[![Image 28: Discord](https://dcbadge.limes.pink/api/server/vWntgU52Xb?style=flat)](https://discord.gg/vWntgU52Xb)[![Image 29: X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/wgussml)](https://twitter.com/wgussml)[![Image 30: Jobs Board](https://img.shields.io/badge/jobs-board-green)](https://jobs.ell.so/)\n\n`ell` is a lightweight prompt engineering library treating prompts as functions. After years of building and using language models at OpenAI and in the startup ecosystem, `ell` was designed from the following principles:\n\nPrompts are programs, not strings[¶](https://docs.ell.so/index.html#prompts-are-programs-not-strings \"Link to this heading\")\n----------------------------------------------------------------------------------------------------------------------------\n\n```\nimport ell\n@ell.simple(model=\"gpt-4o-mini\")\ndef hello(world: str):\n\"\"\"You are a helpful assistant\"\"\" # System prompt\n    name = world.capitalize()\n    return f\"Say hello to {name}!\" # User prompt\nhello(\"sam altman\") # just a str, \"Hello Sam Altman! ...\"\n```\n\n[![Image 31: ell demonstration](https://docs.ell.so/_images/gif1.webp)](https://docs.ell.so/_images/gif1.webp)Prompts aren’t just strings; they are all the code that leads to strings being sent to a language model. In ell, we think of one particular way of using a language model as a discrete subroutine called a **language model program** (LMP).\n\nLMPs are fully encapsulated functions that produce either a string prompt or a list of messages to be sent to various multimodal language models. This encapsulation creates a clean interface for users, who only need to be aware of the required data specified to the LMP.\n\nPrompt engineering is an optimization process[¶](https://docs.ell.so/index.html#prompt-engineering-is-an-optimization-process \"Link to this heading\")\n-----------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe process of prompt engineering involves many iterations, similar to the optimization processes in machine learning. Because LMPs are just functions, `ell` provides rich tooling for this process.\n\n[![Image 32: ell demonstration](https://docs.ell.so/_images/versions_small.webp)](https://docs.ell.so/_images/versions_small.webp)`ell` provides **automatic versioning and serialization of prompts** through static and dynamic analysis and `gpt-4o-mini` **autogenerated commit messages** directly to a _local store_. This process is similar to checkpointing in a machine learning training loop, but it doesn’t require any special IDE or editor - it’s all done with regular Python code.\n\n```\n import ell\n ell.init(store='./logdir')  # Versions your LMPs and their calls\n # ... define your lmps\n hello(\"strawberry\") # the source code of the LMP the call is saved to the store\n```\n\nTools for monitoring, versioning, and visualization[¶](https://docs.ell.so/index.html#tools-for-monitoring-versioning-and-visualization \"Link to this heading\")\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[![Image 33: ell demonstration](https://docs.ell.so/_images/ell_studio_better.webp)](https://docs.ell.so/_images/ell_studio_better.webp)\n\n```\nell-studio--storage./logdir\n```\n\nPrompt engineering goes from a dark art to a science with the right tools. **Ell Studio is a local, open source tool for prompt version control, monitoring, visualization**. With Ell Studio you can empiricize your prompt optimziation process over time and catch regressions before its too late.\n\nTest-time compute is important[¶](https://docs.ell.so/index.html#test-time-compute-is-important \"Link to this heading\")\n-----------------------------------------------------------------------------------------------------------------------\n\nGoing from a demo to something that actually works, often means prompt engineering solutions that involve multiple calls to a language model. By forcing a functional decomposition of the problem, `ell` makes it **easy to implement test-time compute leveraged techniques in a readable and modular way.**\n\n[![Image 34: ell demonstration](https://docs.ell.so/_images/compositionality.webp)](https://docs.ell.so/_images/compositionality.webp)\n\n```\nimport ell\nfrom typing import List\n@ell.simple(model=\"gpt-4o-mini\", temperature=1.0, n=10)\ndef write_ten_drafts(idea : str):\n\"\"\"You are an adept story writer. The story should only be 3 paragraphs\"\"\"\n   return f\"Write a story about {idea}.\"\n@ell.simple(model=\"gpt-4o\", temperature=0.1)\ndef choose_the_best_draft(drafts : List[str]):\n\"\"\"You are an expert fiction editor.\"\"\"\n   return f\"Choose the best draft from the following list: {'\\n'.join(drafts)}.\"\ndrafts = write_ten_drafts(idea)\nbest_draft = choose_the_best_draft(drafts) # Best of 10 sampling.\n```\n\nEvery call to a language model is valuable[¶](https://docs.ell.so/index.html#every-call-to-a-language-model-is-valuable \"Link to this heading\")\n-----------------------------------------------------------------------------------------------------------------------------------------------\n\nEvery call to a language model is worth its weight in credits. In practice, LLM invocations are used for fine tuning, distillation, k-shot prompting, reinforcement learning from human feedback, and more. A good prompt engineering system should capture these as first class concepts.\n\n[![Image 35: ell demonstration](https://docs.ell.so/_images/invocations.webp)](https://docs.ell.so/_images/invocations.webp)In addition to storing the source code of every LMP, `ell` optionally saves every call to a language model locally. This allows you to generate invocaiton datasets, compare LMP outputs by version, and generally do more with the full spectrum of prompt engineering artifacts.\n\nComplexity when you need it, simplicity when you don’t[¶](https://docs.ell.so/index.html#complexity-when-you-need-it-simplicity-when-you-don-t \"Link to this heading\")\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nUsing language models is **just passing strings around, except when it’s not.**\n\n```\nimport ell\n@ell.tool()\ndef scrape_website(url : str):\n   return requests.get(url).text\n@ell.complex(model=\"gpt-5-omni\", tools=[scrape_website])\ndef get_news_story(topic : str):\n   return [\n      ell.system(\"\"\"Use the web to find a news story about the topic\"\"\"),\n      ell.user(f\"Find a news story about {topic}.\")\n   ]\nmessage_response = get_news_story(\"stock market\")\nif message_response.tool_calls:\n   for tool_call in message_response.tool_calls:\n      #...\nif message_response.text:\n   print(message_response.text)\nif message_response.audio:\n   # message_response.play_audio() supprot for multimodal outputs will work as soon as the LLM supports it\n   pass\n```\n\nUsing `@ell.simple` causes LMPs to yield **simple string outputs.** But when more complex or multimodal output is needed, `@ell.complex` can be used to yield `Message` objects responses from language mdoels.\n\nMultimodality should be first class[¶](https://docs.ell.so/index.html#multimodality-should-be-first-class \"Link to this heading\")\n---------------------------------------------------------------------------------------------------------------------------------\n\nLLMs can process and generate various types of content, including text, images, audio, and video. Prompt engineering with these data types should be as easy as it is with text.\n\n```\nfrom PIL import Image\nimport ell\n@ell.simple(model=\"gpt-4o\", temperature=0.1)\ndef describe_activity(image: Image.Image):\n   return [\n      ell.system(\"You are VisionGPT. Answer <5 words all lower case.\"),\n      ell.user([\"Describe what the person in the image is doing:\", image])\n   ]\n# Capture an image from the webcam\ndescribe_activity(capture_webcam_image()) # \"they are holding a book\"\n```\n\n[![Image 36: ell demonstration](https://docs.ell.so/_images/multimodal_compressed.webp)](https://docs.ell.so/_images/multimodal_compressed.webp)`ell` supports rich type coercion for multimodal inputs and outputs. You can use PIL images, audio, and other multimodal inputs inline in `Message` objects returned by LMPs.\n\nPrompt engineering libraries shouldn’t interfere with your workflow[¶](https://docs.ell.so/index.html#prompt-engineering-libraries-shouldn-t-interfere-with-your-workflow \"Link to this heading\")\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n`ell` is designed to be a lightweight and unobtrusive library. It doesn’t require you to change your coding style or use special editors.\n\n[![Image 37: ell demonstration](https://docs.ell.so/_images/useitanywhere_compressed.webp)](https://docs.ell.so/_images/useitanywhere_compressed.webp)You can continue to use regular Python code in your IDE to define and modify your prompts, while leveraging ell’s features to visualize and analyze your prompts. Migrate from langchain to `ell` one function at a time.\n\n* * *\n\nTo get started with `ell`, see the [Getting Started](https://docs.ell.so/getting_started.html) section, or go onto [Installation](https://docs.ell.so/installation.html) and get ell installed.",
  "usage": {
    "tokens": 2095
  }
}
```
