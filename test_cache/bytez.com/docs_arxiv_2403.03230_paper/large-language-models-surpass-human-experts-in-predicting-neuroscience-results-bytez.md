---
title: Large language models surpass human experts in predicting neuroscience results | Bytez
description: This research explores whether large language models (LLMs), like advanced AI programs, can predict the outcomes of neuroscience experiments as well as or better than human experts. The study created a new test called BrainBench, where both LLMs and human experts had to choose the correct outcome from two versions of research abstracts. LLMs performed significantly better than human experts, achieving an average accuracy of 81.4% compared to the experts' 63.4%. The study suggests that LLMs can learn patterns from scientific literature to make predictions about research results, paving the way for AI to assist scientists in the future.
url: https://bytez.com/docs/arxiv/2403.03230/paper
timestamp: 2025-01-20T15:59:27.792Z
domain: bytez.com
path: docs_arxiv_2403.03230_paper
---

# Large language models surpass human experts in predicting neuroscience results | Bytez


This research explores whether large language models (LLMs), like advanced AI programs, can predict the outcomes of neuroscience experiments as well as or better than human experts. The study created a new test called BrainBench, where both LLMs and human experts had to choose the correct outcome from two versions of research abstracts. LLMs performed significantly better than human experts, achieving an average accuracy of 81.4% compared to the experts' 63.4%. The study suggests that LLMs can learn patterns from scientific literature to make predictions about research results, paving the way for AI to assist scientists in the future.


## Content

[b](https://bytez.com/)

[Discover](https://bytez.com/)

[Models](https://bytez.com/models)

[Search](https://bytez.com/search)

[About](https://bytez.com/about)

## Metadata

```json
{
  "title": "Large language models surpass human experts in predicting neuroscience results | Bytez",
  "description": "This research explores whether large language models (LLMs), like advanced AI programs, can predict the outcomes of neuroscience experiments as well as or better than human experts. The study created a new test called BrainBench, where both LLMs and human experts had to choose the correct outcome from two versions of research abstracts. LLMs performed significantly better than human experts, achieving an average accuracy of 81.4% compared to the experts' 63.4%. The study suggests that LLMs can learn patterns from scientific literature to make predictions about research results, paving the way for AI to assist scientists in the future.",
  "url": "https://bytez.com/docs/arxiv/2403.03230/paper",
  "content": "[b](https://bytez.com/)\n\n[Discover](https://bytez.com/)\n\n[Models](https://bytez.com/models)\n\n[Search](https://bytez.com/search)\n\n[About](https://bytez.com/about)",
  "usage": {
    "tokens": 49
  }
}
```
